
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Tema 1: Repàs de probabilitat &#8212; 104392 - Modelització i Inferència 2020.08.04 documentation</title>
    <link rel="stylesheet" href="../_static/sab-book.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Tema 2: Introducció a l’inferència estadística" href="0_2_Intro_stats.html" />
    <link rel="prev" title="1. Tema 0: Intro al curs" href="0_0_Intro_curs.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="0_2_Intro_stats.html" title="3. Tema 2: Introducció a l’inferència estadística"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="0_0_Intro_curs.html" title="1. Tema 0: Intro al curs"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">104392 - Modelització i Inferència 2020.08.04 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">2. </span>Tema 1: Repàs de probabilitat</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="tema-1-repas-de-probabilitat">
<h1><span class="section-number">2. </span>Tema 1: Repàs de probabilitat<a class="headerlink" href="#tema-1-repas-de-probabilitat" title="Permalink to this headline">¶</a></h1>
<div class="section" id="espais-i-mesures-de-probabilitat">
<h2><span class="section-number">2.1. </span>Espais i mesures de Probabilitat<a class="headerlink" href="#espais-i-mesures-de-probabilitat" title="Permalink to this headline">¶</a></h2>
<div class="section" id="espai-de-probabilitat">
<h3><span class="section-number">2.1.1. </span>Espai de Probabilitat<a class="headerlink" href="#espai-de-probabilitat" title="Permalink to this headline">¶</a></h3>
<p class="note">Durant el Tema 1 haurem d’anar una mica ràpid. És impossible fer un curs de probabilitat
en 2 setmanes, però per sort ja n’heu fet un!</p>
<p>Un <strong>espai de probabilitat</strong> és un model matemàtic del resultat d’un <strong>experiment aleatori</strong>.</p>
<p>Consisteix en un triplet <span class="math notranslate nohighlight">\(\left(\Omega, \mathcal{A}, P\right)\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span>: l’<strong>espai mostral</strong>, conjunt de resultats possibles d’un experiment</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A} \subseteq 2^{\Omega}\)</span>: el conjunt d’<strong>esdeveniments</strong>, una família de subconjunts d’<span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span>: una <strong>mesura de probabilitat</strong>, una funció <span class="math notranslate nohighlight">\(\mathcal{A} \rightarrow \left[0, 1\right]\)</span></p></li>
</ul>
<p><em>Recordatori</em>: <span class="math notranslate nohighlight">\(2^{\Omega}\)</span> és el conjunt de tots els sub-conjunts d’<span class="math notranslate nohighlight">\(\Omega\)</span>, incloent-hi <span class="math notranslate nohighlight">\(\emptyset\)</span> i <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
</div>
<div class="section" id="mesura-de-probabilitat">
<h3><span class="section-number">2.1.2. </span>Mesura de probabilitat<a class="headerlink" href="#mesura-de-probabilitat" title="Permalink to this headline">¶</a></h3>
<p>Una <strong>mesura de probabilitat</strong> <span class="math notranslate nohighlight">\(P: \mathcal{A} \rightarrow \left[0, 1\right]\)</span>
ha de satisfer els següents axiomes (de Kolmogorov):</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P\left(\Omega\right)=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\forall A\in\mathcal{A}\)</span>, <span class="math notranslate nohighlight">\(P\left(A\right)\geq 0\)</span></p></li>
<li><p>Per <span class="math notranslate nohighlight">\(A_1,A_2,A_3, \cdots \in \mathcal{A}\)</span> disjunts, <span class="math notranslate nohighlight">\(P\left(\cup_i A_i\right) = \sum_i P\left(A_i\right)\)</span></p></li>
</ol>
<p class="note">Fixeu-vos que tenim llibertat a l’hora de definir <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> pels esdeveniments que ens
interessen (sempre i quan formin una <span class="math notranslate nohighlight">\(\sigma\)</span>-àlgebra.)</p>
<p>Això és una construcció axiomàtica de Probabilitat, formalitzada per Andrey Kolmogorov.</p>
<p>Noteu que no hem associat cap interpretació al significat físic dels valors de <span class="math notranslate nohighlight">\(P\)</span>. Dues interpretacions típiques:</p>
<ul class="simple">
<li><p><strong>Frequentista</strong>: <span class="math notranslate nohighlight">\(P\left(A\right)\)</span> representa la frequència amb que observariem l’esdeveniment <cite>A</cite> si realitzéssim un gran nombre d’experiments</p></li>
<li><p><strong>Bayesiana</strong>: <span class="math notranslate nohighlight">\(P\left(A\right)\)</span> representa la nostra certesa sobre l’ocurrència de l’esdeveniment <cite>A</cite></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Les dues interpretacions no són completament ortogonals, però són l’orígen d’un munt de
discussions filosòfiques i a vegades dogmàtiques. Si us interessa el tema us recomano
<a class="reference external" href="https://projecteuclid.org/euclid.ba/1340370429">Objections to Bayesian statistics</a>.</p>
</div>
<p>Aquest no és un curs de probabilitat, per tant amagarem “detalls” important sota l’alfombra:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}\)</span> en realitat ha de ser una <span class="math notranslate nohighlight">\(\sigma\)</span>-àlgebra (conté <span class="math notranslate nohighlight">\(\emptyset\)</span>, tancat per unió contable i complement)</p></li>
<li><p>Per a conjunts <span class="math notranslate nohighlight">\(\Omega\)</span> contables, podem tirar milles considerant <span class="math notranslate nohighlight">\(\mathcal{A} = 2^{\Omega}\)</span></p></li>
<li><p>La cosa es complica quan <span class="math notranslate nohighlight">\(\Omega\)</span> no és discret (exemples: l’alçada d’una població, el nivell d’expressió d’un gen)</p></li>
</ul>
<p class="note"><strong>Recomano</strong> donar una ullada al [Casella &amp; Berger] o a una altra de les referències
bibliogràfiques per una intro no tècnica a les <span class="math notranslate nohighlight">\(\sigma\)</span>-àlgebres</p>
</div>
<div class="section" id="algunes-propietats-de-les-mesures-de-probabilitat">
<h3><span class="section-number">2.1.3. </span>Algunes propietats de les mesures de probabilitat<a class="headerlink" href="#algunes-propietats-de-les-mesures-de-probabilitat" title="Permalink to this headline">¶</a></h3>
<p class="note"><strong>Teorema [Casella &amp; Berger 1.2.8 i 1.2.9]</strong> Per una mesura de probabilitat <span class="math notranslate nohighlight">\(P\)</span> i
qualsevol esdeveniments <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span>, tenim:</p>
<ol class="note arabic simple">
<li><p><span class="math notranslate nohighlight">\(P\left(\emptyset\right)=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A\right) \leq 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A^c\right) = 1 - P\left(A\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(B \cap A^c\right) = P\left(B\right) - P\left(A \cap B\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A \cup B\right) = P\left(A\right) + P\left(B\right) - P\left(A \cap B\right)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A \subseteq B\)</span>, aleshores <span class="math notranslate nohighlight">\(P\left(A\right) \leq P\left(B\right)\)</span></p></li>
</ol>
<p><strong>Demostració</strong>: Punts (1), (2), (3), exercici :) (recomano començar pel 3er punt).
Punts (4)-(6) tot seguit.</p>
<p>Pel punt (4), només cal observar que <span class="math notranslate nohighlight">\(B = \left(B \cap A\right) \cup \left(B \cap A^c\right)\)</span> (exercici).
D’aquesta identitat i tenint en compte que <span class="math notranslate nohighlight">\(B \cap A\)</span> i <span class="math notranslate nohighlight">\(B \cap A^c\)</span> son disjunts,
s’en dedueix l’expressió usant el 3er axioma de Kolmogorov.</p>
<p>Pel punt (5), utilitzem la següent identitat <span class="math notranslate nohighlight">\(A \cup B = A \cup \left(B \cap A^c\right)\)</span> i apliquem el punt (4).</p>
<p>Finalment el punt (6) el demostrem observant que si <span class="math notranslate nohighlight">\(A \subseteq B\)</span> aleshores <span class="math notranslate nohighlight">\(A \cap B = A\)</span>
i que <span class="math notranslate nohighlight">\(0 \leq P\left(B \cap A^c\right) = P\left(B\right) - P\left(A\right)\)</span>.</p>
<p>Els següents són propietats interessants relatives a col.leccions de conjunts:</p>
<p class="note"><strong>Teorema [Casella &amp; Berger 1.2.11]</strong> Si <span class="math notranslate nohighlight">\(P\)</span> és una mesura de probabilitat:</p>
<ol class="note arabic simple">
<li><p>Per cualsevol partició <span class="math notranslate nohighlight">\(C_1, \cdots, C_N\)</span> d’ <span class="math notranslate nohighlight">\(\Omega\)</span>, <span class="math notranslate nohighlight">\(P\left(A\right) = \sum_i P\left(A \cap C_i \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A_1, A_2 \cdots, \in \mathcal{A}\)</span>, <span class="math notranslate nohighlight">\(P\left(\cup_i A_i\right) \leq \sum_i P\left(A_i \right)\)</span> (desigualtat de Boole)</p></li>
</ol>
<p><strong>Demostració</strong>: (1) tot seguit, (2) exercici.</p>
<p>Demostració punt (1): Recordem que una partició <span class="math notranslate nohighlight">\(C_1, \cdots, C_N\)</span> d’ <span class="math notranslate nohighlight">\(\Omega\)</span>
és una col.lecció de conjunts tal que <span class="math notranslate nohighlight">\(\cup_i C_i = \Omega\)</span> i <span class="math notranslate nohighlight">\(C_i \cap C_j = \emptyset, \forall i\neq j\)</span>.</p>
<p>Tenim doncs la següent cadena d’identitats:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A &amp;= A \cap \Omega \\
A &amp; = A \cap \cup_i C_i \\
A &amp; = \cup_i \left( A \cap C_i \right)\\
P\left(A\right) &amp; = P\left(\cup_i \left( A \cap C_i\right)\right)\end{split}\]</div>
<p>i com que <span class="math notranslate nohighlight">\(A \cap C_i\)</span> i <span class="math notranslate nohighlight">\(A \cap C_j\)</span> son disjunts, el resultat
s’obté considerant el 3er axioma de Kolmogorov.</p>
</div>
<div class="section" id="exemples-d-espais-de-probabilitat">
<h3><span class="section-number">2.1.4. </span>Exemples d’espais de probabilitat<a class="headerlink" href="#exemples-d-espais-de-probabilitat" title="Permalink to this headline">¶</a></h3>
<p><strong>Experiment 1</strong>: Modelar el resultat de llançar un dau de 6 cares</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega = \left\{1, 2, 3, 4, 5, 6\right\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A} = \left\{ \left\{1\right\}, \left\{2\right\}, \cdots, \left\{1, 2\right\}, \cdots, \emptyset, \Omega \right\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(x\right) = \frac{1}{6}, x \in \Omega\)</span></p></li>
</ul>
<p class="note"><strong>Exercici</strong>: Com definirieu <span class="math notranslate nohighlight">\(P\left(A\right)\)</span> per a qualsevol <span class="math notranslate nohighlight">\(A \in \mathcal{A}\)</span>?</p>
<ul class="build simple">
<li><p>Resposta: <span class="math notranslate nohighlight">\(P\left(A\right) = \sum_{x \in A} P\left(x\right)\)</span>. Podeu comprovar que aquesta construcció satisfà els axiomes.</p></li>
</ul>
<p><strong>Experiment 2</strong>: Escollir 100 persones i fer-els-hi una prova d’anticossos per SARS-COV-2</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega = \left\{+, -\right\}^{100}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A} = ?\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A\right) = ?\)</span></p></li>
</ul>
<p><strong>Experiment 3</strong>: Escollir aleatòriament un estudiant d’aquesta classe i mesurar-ne la seva alçada</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega = \left[0, \infty \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A} = ?\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A\right) = ?\)</span></p></li>
</ul>
</div>
<div class="section" id="questionari-de-repas">
<h3><span class="section-number">2.1.5. </span>Qüestionari de repàs<a class="headerlink" href="#questionari-de-repas" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>Un espai de probabilitat és el triplet d’un ______________, un ______________ i una _____________.</p></li>
<li><p>Quina dels següents assercions <strong>no</strong> és un axioma de Kolmogorov:</p></li>
</ol>
<ol class="loweralpha simple">
<li><p>Si <span class="math notranslate nohighlight">\(A \cap B = \emptyset\)</span>, <span class="math notranslate nohighlight">\(P\left(A \cup B \right) = P\left(A \right) + P\left( B \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A\right) \leq 1, \forall A \in \mathcal{A}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left(A\right) \geq 0, \forall A \in \mathcal{A}\)</span></p></li>
</ol>
<ol class="arabic simple" start="3">
<li><p>Quin és l’<span class="math notranslate nohighlight">\(\Omega\)</span> i l’<span class="math notranslate nohighlight">\(\mathcal{A}\)</span> del següent experiment: <em>mesurar la vida útil en dies dels ordinadors Macbook Pro.</em></p></li>
<li><p>Quin és l’<span class="math notranslate nohighlight">\(\Omega\)</span> i l’<span class="math notranslate nohighlight">\(\mathcal{A}\)</span> de l’experiment: <em>llençar un dau fins que treiem un 6.</em></p></li>
</ol>
</div>
</div>
<div class="section" id="independencia-i-probabilitat-condicional">
<h2><span class="section-number">2.2. </span>Independència i probabilitat condicional<a class="headerlink" href="#independencia-i-probabilitat-condicional" title="Permalink to this headline">¶</a></h2>
<div class="section" id="probabilitat-condicional">
<h3><span class="section-number">2.2.1. </span>Probabilitat condicional<a class="headerlink" href="#probabilitat-condicional" title="Permalink to this headline">¶</a></h3>
<p>Donats <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span>, amb <span class="math notranslate nohighlight">\(P\left(B\right) &gt; 0\)</span>,
<span class="math notranslate nohighlight">\(P\left(A|B\right) = \frac{P\left(A \cap B\right)}{P\left(B\right)}\)</span> (aquesta construcció satisfà els axiomes de Kolmogorov)</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/proba_condicional.png"><img alt="../_images/proba_condicional.png" src="../_images/proba_condicional.png" style="height: 300px;" /></a>
</div>
<p><span class="math notranslate nohighlight">\(P\left(\cdot|B\right)\)</span> és la restricció de <span class="math notranslate nohighlight">\(P\)</span> al subconjunt d’esdeveniments B. Alguns preguntes/petits exercicis interessants:</p>
<ol class="build arabic simple">
<li><p>Tindria sentit definir <span class="math notranslate nohighlight">\(P\left(A|B\right)\)</span> si <span class="math notranslate nohighlight">\(P\left(B\right) = 0\)</span>?</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A \cap B = \emptyset\)</span>, <span class="math notranslate nohighlight">\(P\left(A|B\right)\)</span>?</p></li>
<li><p>Com podem interpretar si <span class="math notranslate nohighlight">\(P\left(A|B\right) =P\left(A\right)\)</span>? Podeu donar un exemple “físic”?</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A \subseteq B\)</span>, quina relació hi ha entre <span class="math notranslate nohighlight">\(P\left(A|B\right)\)</span> i <span class="math notranslate nohighlight">\(P\left(A\right)\)</span>?</p></li>
</ol>
</div>
<div class="section" id="esdeveniments-independents">
<h3><span class="section-number">2.2.2. </span>Esdeveniments independents<a class="headerlink" href="#esdeveniments-independents" title="Permalink to this headline">¶</a></h3>
<p>Diem que <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span>, són independents si:</p>
<p><span class="math notranslate nohighlight">\(P\left(A \cap B\right) =P\left(A\right)P\left(B\right)\)</span></p>
<p>Això és equivalent a <span class="math notranslate nohighlight">\(P\left(A|B\right) =P\left(A\right)\)</span> si <span class="math notranslate nohighlight">\(P\left(B\right) &gt; 0\)</span>.</p>
<p>Algunes preguntes [Casella &amp; Berger Teorema 1.3.9] (mirem de respondre per intució primer i matemàticament després):</p>
<ol class="build arabic simple">
<li><p>Si <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span> son independents, què podem dir de <span class="math notranslate nohighlight">\(A, B^c\)</span>?</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span> son independents, què podem dir de <span class="math notranslate nohighlight">\(A^c, B^c\)</span>?</p></li>
</ol>
<p>Per exemple, l’independència conjunta no implica independència de parells:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/ex_independencia.png"><img alt="../_images/ex_independencia.png" src="../_images/ex_independencia.png" style="height: 300px;" /></a>
</div>
<p>Calculem <span class="math notranslate nohighlight">\(P\left(A \cap B \cap C\right)\)</span> i <span class="math notranslate nohighlight">\(P\left(B \cap C\right)\)</span>…</p>
<p><em>Nota:</em> l’independència de parells tampoc implica independència mútua (veure Problema)</p>
<p>Per resoldre aquests problemes, fa falta una definició molt més estricta
de la noció d’independència en conjunts d’esdeveniments:</p>
<p class="note"><strong>Definició</strong> <span class="math notranslate nohighlight">\(A_1, A_2 \cdots, \in \mathcal{A}\)</span> són mutualment independents si per cualsevol
subcol.lecció <span class="math notranslate nohighlight">\(A_{i_1}, A_{i_2} \cdots, \in \mathcal{A}\)</span>, tenim que <span class="math notranslate nohighlight">\(P\left(\cap_j A_{i_j}\right) = \Pi_j P\left(A_{i_j}\right)\)</span></p>
<p>(En aquest curs, quan parlem de mostres independents, estarem assumint independència mútua)</p>
</div>
</div>
<div class="section" id="variables-aleatories-i-funcions-de-distribucio">
<h2><span class="section-number">2.3. </span>Variables aleatòries i funcions de distribució<a class="headerlink" href="#variables-aleatories-i-funcions-de-distribucio" title="Permalink to this headline">¶</a></h2>
<div class="section" id="variable-aleatoria">
<h3><span class="section-number">2.3.1. </span>Variable aleatòria<a class="headerlink" href="#variable-aleatoria" title="Permalink to this headline">¶</a></h3>
<p class="note"><strong>Definició</strong> Una variable aleatòria (<em>v.a.</em> pels amics) és una funció <span class="math notranslate nohighlight">\(X : \Omega \to \mathcal{X} \subseteq \mathbb{R}\)</span>.</p>
<p>Podem doncs definir una funció de probabilitat [Casella &amp; Berger 1.4.2]:</p>
<p><span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = P\left(\left\{s\in \Omega: X\left(s\right) \in A \right\}\right)\)</span></p>
<p>que satisfà els axiomes de Kolmogorov. Aquesta definició es pot especialitzar
quan <span class="math notranslate nohighlight">\(\Omega, \mathcal{X}\)</span> són contables:</p>
<p><span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = \sum_{s\in \Omega: X\left(s\right) \in A } P\left(s\right)\)</span></p>
<p class="note">Enlloc de treballar amb <span class="math notranslate nohighlight">\(P_X\left(X \in A\right)\)</span>, en general caracteritzarem les v.a. a través de les seves funcions de distribució, de massa o de densitat.</p>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="../_images/v.a.png"><img alt="../_images/v.a.png" src="../_images/v.a.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-text">Diagrama explicatiu de la identitat <span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = P\left(\left\{s\in \Omega: X\left(s\right) \in A \right\}\right)\)</span>.
Podem caracteritzar l’esdeveniment <span class="math notranslate nohighlight">\(X \in A\)</span> relatiu a una v.a. <span class="math notranslate nohighlight">\(X\)</span> en funció de l’esdeveniment <span class="math notranslate nohighlight">\(\left\{s\in \Omega: X\left(s\right) \in A \right\}\)</span>
en l’espai mostral d’orígen. En aquest curs no ho tindrem en compte, però en realitat
no totes les funcions <span class="math notranslate nohighlight">\(X : \Omega \to \mathcal{X} \subseteq \mathbb{R}\)</span> són admissibles,
només les <a class="reference external" href="https://en.wikipedia.org/wiki/Measurable_function">mesurables</a>.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Per entendre un concepte, sempre va bé intentar reflexionar primer sobre
els casos més extremadament simples.</p>
<ul class="simple">
<li><p>Q:<em>Quina seria la v.a. més simple?</em></p></li>
<li><p>R: La v.a. constant, definida com <span class="math notranslate nohighlight">\(X : \Omega \to 0\)</span></p></li>
<li><p>Q: <em>I la 2a més simple?</em></p></li>
<li><p>R: La v.a. de Bernouilli, definida com <span class="math notranslate nohighlight">\(X : \Omega \to \left\{0, 1\right\}\)</span></p></li>
</ul>
<p>Aplicant la definició anterior, tenim que la v.a. de Bernouilli està completament
caracterizada per un sol paràmetre <span class="math notranslate nohighlight">\(p = P\left(\left\{s\in \Omega: X\left(s\right) = 1\right\}\right)\)</span></p>
<p>Revisitem l’<strong>Experiment 2</strong> anterior (escollim 100 persones i fem una prova d’anticossos per SARS-COV-2)</p>
<ul class="simple">
<li><p>Teniem que <span class="math notranslate nohighlight">\(\Omega = \left\{+, -\right\}^{100}\)</span></p></li>
<li><p>Definim v.a. <span class="math notranslate nohighlight">\(X : \left\{+, -\right\}^{100} \to \mbox{Nombre de +} \in \left[0, 100\right]\)</span></p></li>
</ul>
<p><strong>Exercici</strong>: Fent servir l’identitat <span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = \sum_{s\in \Omega: X\left(s\right) \in A } P\left(s\right)\)</span>, derivem <span class="math notranslate nohighlight">\(P_X\left(X=k\right)\)</span>.</p>
<p>Primer determinem el conjunt <span class="math notranslate nohighlight">\(\left\{s\in \Omega: X\left(s\right) \in A\right\}\)</span> sobre el qual haurem de sumar:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{s\in \Omega: X\left(s\right) \in A\right\} &amp;= \left\{s\in \Omega: X\left(s\right)= k\right\}\\
&amp;= \mbox{Totes les seqüencies amb exactament k +}\end{split}\]</div>
<p>Fixeu-vos que hi ha <span class="math notranslate nohighlight">\({n \choose k}\)</span> seqüencies amb <span class="math notranslate nohighlight">\(k\)</span> “+” d’entre <span class="math notranslate nohighlight">\(n=100\)</span> individus. Per altra banda,
si assumim que cada individu és + de manera independent, tenim que cada seqüència
succeeix amb probabilitat <span class="math notranslate nohighlight">\(p^k\left(1-p\right)^{n-k}\)</span>.</p>
<p>Per tant deduïm que <span class="math notranslate nohighlight">\(P_X\left(X=k\right) = {n \choose k}p^k\left(1-p\right)^{n-k}\)</span> (distribució binomial)</p>
<p class="note">Què passa si alguns individus són membres d’una mateixa família?</p>
<ol class="note arabic simple">
<li><p>Donat un espai mostral <span class="math notranslate nohighlight">\(\Omega\)</span>, quin seria el conjunt d’esdeveniments més “petit”?</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A \cap B = \emptyset\)</span>, vol dir que A, B són esdeveniments independents?</p></li>
<li><p>Quin és l’espai mostral d’una v.a. <span class="math notranslate nohighlight">\(X: \Omega \to \mathcal{X}\)</span>?</p></li>
<li><p>Quina és la probabilitat de la seqüència [+,-,+,-] si <span class="math notranslate nohighlight">\(P\left(+\right)=0.3\)</span> i cada esdeveniment +/- és mutualment independent?</p></li>
</ol>
<p>Revisitem l’<strong>Experiment 3</strong>. Escollim un estudiant d’aquesta classe i aquest cop mesurem la raó alçada/pes:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega = \left(0, \infty \right) \times \left(0, \infty \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Z: (x, y) \in \Omega \to \frac{x}{y} \in \left(0, \infty \right)\)</span></p></li>
<li><p>Com calculariem <span class="math notranslate nohighlight">\(P_Z\left(Z \in A\right)\)</span>? <em>Necessitarem de fer alguna suposició addicional sobre les v.a. X i Y</em></p></li>
</ul>
<p class="note">En la gran majoria de problemes haurem de fer una hipòtesi sobre el model aleatori de les observacions (hipòtesi que després haurem de validar comprovant la <em>bondat de l’ajust</em>)</p>
</div>
<div class="section" id="funcio-de-distribucio">
<h3><span class="section-number">2.3.2. </span>Funció de distribució<a class="headerlink" href="#funcio-de-distribucio" title="Permalink to this headline">¶</a></h3>
<p class="note"><strong>Definició</strong> La funció de distribució cumulativa (f.d.c.) d’una v.a. es defineix com <span class="math notranslate nohighlight">\(F\left(x\right) = P\left(X \leq x\right)\)</span>.</p>
<p>De fet qualsevol funció pot ser una f.d.c si compleix [Casella &amp; Berger Teorema 1.5.3]:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\lim_{x\to -\infty} F(x) = 0\)</span> i <span class="math notranslate nohighlight">\(\lim_{x\to \infty} F(x) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F(x)\)</span> és no-decreixent</p></li>
<li><p><span class="math notranslate nohighlight">\(F(x)\)</span> és contínua per la dreta (<span class="math notranslate nohighlight">\(\lim_{x\to x_0^+} F(x) = x_0\)</span>)</p></li>
</ol>
<p class="note">El més important es que la f.d.c caracteritza únicament una variable aleatòria: si <span class="math notranslate nohighlight">\(F_X = F_Y\)</span>, aleshores <span class="math notranslate nohighlight">\(X\)</span> i <span class="math notranslate nohighlight">\(Y\)</span> són idènticament distribuïdes [Casella &amp; Berger 1.5.8 i 1.5.10]</p>
</div>
<div class="section" id="funcio-de-massa-o-densitat-de-probabilitat">
<h3><span class="section-number">2.3.3. </span>Funció de massa o densitat de probabilitat<a class="headerlink" href="#funcio-de-massa-o-densitat-de-probabilitat" title="Permalink to this headline">¶</a></h3>
<p>A voltes ens serà més pràctic treballar amb un altre objecte, la funció de massa de probabilitat (f.m.p.) <span class="math notranslate nohighlight">\(p_X\)</span> o de densitat de probabilitat (f.d.p) <span class="math notranslate nohighlight">\(f_X\)</span>.</p>
<ul class="simple">
<li><p><strong>Cas discret</strong>: <span class="math notranslate nohighlight">\(p_X\left(k\right) = P_X\left(X=k\right)\)</span> (noteu que <span class="math notranslate nohighlight">\(F_X\left(x\right) = \sum_{k=-\infty}^{x}p_X\left(k\right))\)</span>)</p></li>
<li><p><strong>Cas “continu”</strong>: La funció <span class="math notranslate nohighlight">\(f_X\)</span> tal que <span class="math notranslate nohighlight">\(F_X\left(x\right) = \int_{-\infty}^x f_X\left(t\right)dt\)</span></p></li>
<li><p><strong>Cas “mixte”</strong>:  No les podrem caracteritzar amb una f.m.p o una f.d.p, però recordeu que existeixen v.a. que no són discretes ni contínues!</p></li>
</ul>
<p class="note">Aquí ens desviem una mica de la notació de [Casella &amp; Berger] al fer servir <span class="math notranslate nohighlight">\(p_X\)</span> enlloc de <span class="math notranslate nohighlight">\(f_X\)</span> per la f.m.p.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Estem ometent molts “detalls” tècnics importants… Hi ha variables contínues per les que <span class="math notranslate nohighlight">\(f_X\)</span> no existeix.</p>
</div>
<p>Tal i com hem fist per la f.d.c, tenim un resultat similar per la f.d.p o la f.m.p: <span class="math notranslate nohighlight">\(f_X\left(x\right)\)</span> (<span class="math notranslate nohighlight">\(p_X\left(k\right)\)</span>)
és una f.d.p (f.m.p) si i només si [Casella &amp; Berger 1.6.5]:</p>
<ol class="loweralpha simple">
<li><p><span class="math notranslate nohighlight">\(f_X\left(x\right) \geq 0, \forall x\)</span> (<span class="math notranslate nohighlight">\(p_X\left(k\right) \geq 0, \forall k\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\int_{\infty}^{-\infty} f_X\left(x\right)dx = 1\)</span> (<span class="math notranslate nohighlight">\(\sum_{\infty}^{\infty} p_X\left(k\right) = 1\)</span>)</p></li>
</ol>
<p>Per tant podem construir una f.d.p. a partir de qualsevol funció <span class="math notranslate nohighlight">\(h\left(x\right)\)</span> no-negativa, definint:</p>
<p><span class="math notranslate nohighlight">\(K = \int_{-\infty}^{\infty} h\left(x\right)dx\)</span> (també coneguda com <em>funció de partició</em>)</p>
<p>i <span class="math notranslate nohighlight">\(f_X\left(x\right) = \frac{h\left(x\right)}{K}\)</span>. Això es fa servir per exemple
en uns objectes anomentats <a class="reference external" href="https://en.wikipedia.org/wiki/Graphical_model">Models Gràfics Probabilístics</a>.</p>
</div>
<div class="section" id="exemple-funcio-de-distribucio-i-massa-d-una-v-a-geometrica">
<h3><span class="section-number">2.3.4. </span>Exemple: funció de distribució i massa d’una v.a. geomètrica<a class="headerlink" href="#exemple-funcio-de-distribucio-i-massa-d-una-v-a-geometrica" title="Permalink to this headline">¶</a></h3>
<p>Considerem la variable aleatòria corresponent a l’experiment de
llançar una moneda fins que surti cara.</p>
<ul class="build simple">
<li><p>L’espai mostral és: <span class="math notranslate nohighlight">\(\Omega = \left\{C, XC, XXC, \cdots \right\}\)</span></p></li>
<li><p>Definim la v.a. <span class="math notranslate nohighlight">\(X\)</span> com el nombre de creus que obtenim abans de la primera cara.</p></li>
</ul>
<p>Si suposem que:</p>
<ol class="arabic simple">
<li><p>Cada llançament és independent de l’altre (pregunta: podeu imaginar una situació en que no ho fos)</p></li>
<li><p>La probabilitat d’obtenir cara és <span class="math notranslate nohighlight">\(p\)</span></p></li>
</ol>
<p>Podem calcular <span class="math notranslate nohighlight">\(p_X\left(k\right)=?\)</span></p>
<p>La f.m.p és la distribució geomètrica:</p>
<p><span class="math notranslate nohighlight">\(p_X\left(k\right) = P\left(\mbox{X}\right)^{k-1}P\left(\mbox{C}\right) = \left(1-p\right)^{k-1}p\)</span></p>
<p>A partir de la qual podem calcular la f.d.c:</p>
<p><span class="math notranslate nohighlight">\(F_X\left(x\right) = \sum_{k=1}^x p_X\left(k\right) = \sum_{k=1}^x \left(1-p\right)^{k-1}p\)</span></p>
<p>utilitzant l’identitat <span class="math notranslate nohighlight">\(\sum_{k=1}^x \rho^{x-1}=\frac{1-\rho^x}{1-\rho}\)</span>, podem arribar a:</p>
<p><span class="math notranslate nohighlight">\(F_X\left(x\right) = 1 - \left(1-p\right)^x\)</span></p>
<p>Seria interessant que comprovéssiu que <span class="math notranslate nohighlight">\(F_X\left(x\right)\)</span> compleix les condicions per
ser una f.d.c.</p>
<p>Una v.a. <span class="math notranslate nohighlight">\(X\)</span> és <em>memoryless</em> si:</p>
<p><span class="math notranslate nohighlight">\(P\left(X &gt; m+n | X &gt; m\right) = P\left(X &gt; n \right)\)</span></p>
<p><em>Exercici:</em> Comprovem que aquesta propietat es verifica per la <span class="math notranslate nohighlight">\(p_X\left(k\right)\)</span> geomètrica.</p>
<ul class="simple">
<li><p>L’interpretació de la propietat és interessant, per exemple, en el contexte de la loteria: No haver guanyat després de jugar 10 cops no incrementa la probabilitat que guanyem en els següents 10 cops…</p></li>
<li><p>Aquesta propietat no és tant freqüent com podria semblar.</p></li>
<li><p>Aquesta f.m.p és interessant per modelar problemes de <em>temps de vida</em>, per exemple: fallada d’un component electrònic, on la probabilitat de que falli <strong>no canvia amb el temps</strong>.</p></li>
</ul>
</div>
<div class="section" id="altres-v-a-discretes">
<h3><span class="section-number">2.3.5. </span>Altres v.a. discretes<a class="headerlink" href="#altres-v-a-discretes" title="Permalink to this headline">¶</a></h3>
<p>A través dels exemples, fins ara ja hem vist 4 tipus de variables aleatòries discretes:</p>
<ul class="simple">
<li><p>Uniforme, <span class="math notranslate nohighlight">\(X \in \left\{0, \cdots, k-1\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = c \right)=\frac{1}{k}\)</span></p></li>
<li><p>Bernouilli, <span class="math notranslate nohighlight">\(X \in \left\{0, 1\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = 1; p \right)=p\)</span></p></li>
<li><p>Binomial, <span class="math notranslate nohighlight">\(X \in \left\{0, \cdots, n\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = k; p, n \right)={n\choose k}p^k\left(1-p\right)^{n-k}\)</span></p></li>
<li><p>Geomètrica, <span class="math notranslate nohighlight">\(X \in \left\{1, \cdots,\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = k; p \right)= p\left(1-p\right)^{k-1}\)</span></p></li>
</ul>
<p class="note">Exercici: podeu trobar un experiment “físic” que es correspongui a cada una de les v.a. anteriors?</p>
<p>Us recomano donar un cop d’ull pel vostre compte a dues distribucions famoses més,
la <strong>hipergeomètrica</strong> i la <strong>binomial negativa</strong>. Ara donarem una ullada a la de Poisson.</p>
<p>La distribució de Poisson es pot motivar físicament amb el següent exemple. Suposeu volem modelar # de clients que arriben en un interval T:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/poisson_motivation.png"><img alt="../_images/poisson_motivation.png" src="../_images/poisson_motivation.png" style="height: 300px;" /></a>
</div>
<ul class="simple">
<li><p>els intervals de temps <span class="math notranslate nohighlight">\(\delta t_i = \frac{T}{N}, N \gg 1\)</span>, aleshores <span class="math notranslate nohighlight">\(B_i\)</span> és aproximadament Bernouilli(p)</p></li>
<li><p>els esdeveniments <span class="math notranslate nohighlight">\(B_i\)</span> són independents</p></li>
</ul>
<p>Per tant <span class="math notranslate nohighlight">\(X\)</span> és aproximadament <span class="math notranslate nohighlight">\(\mbox{Binomial}\left(N, p\right)\)</span> on N és el nombre d’intervals en el periòde.</p>
<p>La distribució de Poisson apareix quan tenim que <span class="math notranslate nohighlight">\(p \to 0\)</span> i <span class="math notranslate nohighlight">\(N \to \infty\)</span>
mantenint el nombre mig d’arribades per interval de temps fixe, que anomenarem <span class="math notranslate nohighlight">\(\lambda = Np\)</span>.</p>
<p>La f.m.p de  <span class="math notranslate nohighlight">\(X\)</span> és aleshores, quan <span class="math notranslate nohighlight">\(n \to \infty\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P\left(X = k; \lambda \right) &amp; =\frac{n!}{k!\left(n-k\right)!}\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k} \\
                     &amp; =\frac{\lambda^k}{k!}\frac{n!}{\left(n-k\right)!}\frac{1}{n^k}\left(1 - \frac{\lambda}{n}\right)^n\left(1-\frac{\lambda}{n}\right)^{-k} \\
                     &amp; \to \frac{\lambda^k}{k!}e^{-\lambda}\end{split}\]</div>
<p><em>Exercici</em>: Justificar l’últim pas!</p>
</div>
<div class="section" id="incis-sobre-les-v-a-continues">
<h3><span class="section-number">2.3.6. </span>Incís sobre les v.a. contínues<a class="headerlink" href="#incis-sobre-les-v-a-continues" title="Permalink to this headline">¶</a></h3>
<p>Hem vist que una variable aleatòria contínua es caracteritza per una funció
de densitat de probabilitat <span class="math notranslate nohighlight">\(f_X\)</span> tal que:</p>
<p><span class="math notranslate nohighlight">\(F_X\left(x\right) = \int_{-\infty}^x f_X\left(t\right)dt\)</span></p>
<p>per tant tenim que</p>
<p><span class="math notranslate nohighlight">\(P\left(a &lt; X \leq b\right) = \int_{a}^b f_X\left(t\right)dt\)</span></p>
<p>Una conseqüència d’aquesta definició quan <span class="math notranslate nohighlight">\(b \to a\)</span> és el que pot semblar paradoxal:</p>
<p><span class="math notranslate nohighlight">\(P\left(X = x\right) = 0\)</span></p>
<p>Pel 3er axioma de Kolmogorov això sembla implicar que
<span class="math notranslate nohighlight">\(P\left(a &lt; X &lt; b\right)\)</span>, sent la unió de tots els punts entre a i b, hauria
de ser també 0. La paradoxa es resol si recordem que el 3er axioma només contempla unions contables!</p>
</div>
<div class="section" id="la-distribucio-uniforme">
<h3><span class="section-number">2.3.7. </span>La distribució uniforme<a class="headerlink" href="#la-distribucio-uniforme" title="Permalink to this headline">¶</a></h3>
<p>La f.d.p més simple es correspon amb la variable aleatòria contínua més simple, escollir
un nombre aleatori dins d’un interval <span class="math notranslate nohighlight">\(\left[a, b\right]\)</span>:</p>
<p><span class="math notranslate nohighlight">\(f_X\left(x; a, b\right) = \left\{\begin{array}{cc} \frac{1}{b-a} &amp; a \leq x \leq b \\ 0 &amp; \mbox{altrament} \end{array}\right.\)</span></p>
<p><strong>Exercicis</strong>:</p>
<ul class="simple">
<li><p>Calculem la f.d.c d’una variable uniforme.</p></li>
<li><p>Doneu un exemple d’un experiment on l’uniforme és un bon model?</p></li>
<li><p>Com generarieu una variable uniforme amb un ordinador?</p></li>
</ul>
<p class="note">Irònicament, i potser contraintuïtivament, l’aleatorietat és molt difícil de generar!</p>
</div>
<div class="section" id="la-familia-gamma">
<h3><span class="section-number">2.3.8. </span>La família Gamma<a class="headerlink" href="#la-familia-gamma" title="Permalink to this headline">¶</a></h3>
<p>Recordeu que podem definir una f.d.p tot normalitzant qualsevol funció no-negativa.</p>
<p>Considerem la següent família de funcions, parameteritzades per <span class="math notranslate nohighlight">\(\alpha, \beta\)</span> doncs:</p>
<p><span class="math notranslate nohighlight">\(h\left(t\right) = \frac{t^{\alpha-1}}{\beta^{\alpha}} e^{-\frac{t}{\beta}}\)</span></p>
<p>definides per <span class="math notranslate nohighlight">\(t\in \left[0, \infty\right)\)</span>. Es pot demostrar que per <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span>,</p>
<p><span class="math notranslate nohighlight">\(\Gamma\left(\alpha\right)=\int_{0}^{\infty}h\left(t\right)dt\)</span> existeix.</p>
<p>Per tant definim la família distribucions <span class="math notranslate nohighlight">\(\mbox{gamma}\left(\alpha, \beta\right)\)</span> com:</p>
<p><span class="math notranslate nohighlight">\(f_X\left(x;\alpha, \beta\right) = \frac{1}{\Gamma\left(\alpha\right)}\frac{x^{\alpha-1}}{\beta^{\alpha}} e^{-\frac{x}{\beta}}\)</span></p>
<p>La família Gamma és important perquè permet modelar una gran varietat d’experiments i està intimament
relacionada amb altres distribucions:</p>
<ul class="simple">
<li><p>Distribució exponencial (si fixem <span class="math notranslate nohighlight">\(\alpha=1\)</span>): la cosina contínua de la f.m.p geomètrica que hem vist abans</p></li>
<li><p>Distribució de <span class="math notranslate nohighlight">\(\chi^2_p\)</span> (si fixem <span class="math notranslate nohighlight">\(\alpha=p/2\)</span> i <span class="math notranslate nohighlight">\(\beta=2\)</span>)</p></li>
</ul>
<p>La f.d.p de la Gamma per diversos valors de <span class="math notranslate nohighlight">\(\alpha\)</span> (k a l’imatge) i <span class="math notranslate nohighlight">\(\beta\)</span> (<span class="math notranslate nohighlight">\(\theta\)</span> a la figura) <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">[Font]</a></p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/650px-Gamma_distribution_pdf.svg.png"><img alt="../_images/650px-Gamma_distribution_pdf.svg.png" src="../_images/650px-Gamma_distribution_pdf.svg.png" style="height: 250px;" /></a>
</div>
</div>
<div class="section" id="la-familia-normal">
<h3><span class="section-number">2.3.9. </span>La família “Normal”<a class="headerlink" href="#la-familia-normal" title="Permalink to this headline">¶</a></h3>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/440px-Normal_Distribution_PDF.svg.png"><img alt="../_images/440px-Normal_Distribution_PDF.svg.png" src="../_images/440px-Normal_Distribution_PDF.svg.png" style="height: 150px;" /></a>
</div>
<p><span class="math notranslate nohighlight">\(f_X\left(x ; \mu, \sigma\right) = \frac{1}{\sqrt{2\pi}} e^{-\frac{\left(x - \mu\right)^2}{\sigma^2}}\)</span></p>
<p>La distribució Normal o Gaussiana és fonamental en estadística, per múltiples raons:</p>
<ul class="simple">
<li><p>Apareix “naturalment” quan sumem/calculem el promig d’un gran nombre de mostres</p></li>
<li><p>És simètrica i parameteritzada per 2 paràmetres intuitius (<span class="math notranslate nohighlight">\(\mu\)</span> i <span class="math notranslate nohighlight">\(\sigma\)</span>)</p></li>
<li><p>Malgrat la seva aparença intimidant, és tractable analíticament</p></li>
</ul>
</div>
<div class="section" id="i-que-te-a-veure-tot-aixo-amb-l-estadistica">
<h3><span class="section-number">2.3.10. </span>I què te a veure tot això amb l’estadística?<a class="headerlink" href="#i-que-te-a-veure-tot-aixo-amb-l-estadistica" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Com vem comentar a l’introducció al curs, l’inferència estadística és la ciència d’establir propietats
d’una població mitjantçant mostres de la mateixa.</p>
<p>Els models probabilístics com els que hem vist darrerament són una de les eines que farem servir
per fer aquesta feina d’inferència.</p>
</div>
<p>Vegem un exemple pràctic, el de l’<strong>Experiment 2</strong> (proves d’anticossos).</p>
<ol class="build arabic simple">
<li><p>Com hem dit, l’estadística comença amb la <strong>recollida de mostres</strong> (dades), en aquest cas, realizar tests d’anticossos a 100 persones a l’atzar i anotar-ne el resultat</p></li>
<li><p>El segon pas en <strong>estadística paramètrica</strong> és la definició d’un model probabilístic que caracteritzi les observacions. Com hem vist abans, un model raonable és que cada una de les 100 mostres és una v.a. de Bernouilli.</p></li>
<li><p>Ara tenim una col.lecció de mostres, <span class="math notranslate nohighlight">\(\left\{x_1, \cdots, x_{100}\right\}\)</span>, on cada <span class="math notranslate nohighlight">\(x_i\in \left\{0, 1\right\}\)</span>, i un model: <span class="math notranslate nohighlight">\(P_X\left(X_i=1\right) = p\)</span>. L’únic que ens falta per poder fer inferència és trobar el valor de <span class="math notranslate nohighlight">\(p\)</span> que millor descriu les observacions (Tema 2). Per exemple un estimador raonable seria la mitjana aritmètica <span class="math notranslate nohighlight">\(\hat{p}=\frac{1}{100}\sum_i x_i= \frac{\mbox{# de +}}{100}\)</span>. Posem que <span class="math notranslate nohighlight">\(\hat{p}=0.1\)</span>.</p></li>
</ol>
<p>Amb aquest estimador, obtingut <strong>només a partir de 100 mostres</strong>, i gràcies als resultats que
veurem en els Temes 1 i 2, ja podríem deduïr propietats de la població en general:</p>
<ul class="simple">
<li><p>Veurem que <span class="math notranslate nohighlight">\(\hat{p}\)</span> és un estimador “sense biaix” de <span class="math notranslate nohighlight">\(p\)</span></p></li>
<li><p>Però també veurem que la variabilitat (ex: variança) de <span class="math notranslate nohighlight">\(\hat{p}\)</span> decreix amb el nombre de mostres, i potser 100 són massa poques…</p></li>
<li><p>També veurem com, a partir de <span class="math notranslate nohighlight">\(\hat{p}\)</span>, podem donar un interval de confiança sobre <span class="math notranslate nohighlight">\(p\)</span> (ja hem vist que <span class="math notranslate nohighlight">\(\sum_i x_i \sim \mbox{Binomial}\left(p, 100 \right)\)</span>…)</p></li>
</ul>
<p class="note">Però per tot això primer hem d’aprofundir més en alguns altres conceptes de probabilitat: les transformacions
de v.a., l’esperança, les distribucions conjuntes i algunes desigualtats.</p>
</div>
</div>
<div class="section" id="funcions-de-variables-aleatories">
<h2><span class="section-number">2.4. </span>Funcions de variables aleatòries<a class="headerlink" href="#funcions-de-variables-aleatories" title="Permalink to this headline">¶</a></h2>
<div class="section" id="transformacions-afins">
<h3><span class="section-number">2.4.1. </span>Transformacions afins<a class="headerlink" href="#transformacions-afins" title="Permalink to this headline">¶</a></h3>
<p>Sovint ens trobarem que el nostre experiment es pot modelar més fàcilment
com la transformació d’una v.a. <span class="math notranslate nohighlight">\(X: \Omega \to \mathcal{X}\)</span> mitjantçant una funció
<span class="math notranslate nohighlight">\(g: \mathcal{X}\to\mathcal{Y}\)</span>: <span class="math notranslate nohighlight">\(Y=g\left(X\right)\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recordem que <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> i <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> denoten l’espai mostral d’<span class="math notranslate nohighlight">\(X\)</span>
i <span class="math notranslate nohighlight">\(Y\)</span>, respectivament.</p>
</div>
<p>Per exemple, una transformació senzilla és l’afí: <span class="math notranslate nohighlight">\(Y = a + b X\)</span></p>
<p>En aquest cas, podem expressar la f.d.c <span class="math notranslate nohighlight">\(F_Y\)</span> en funció de <span class="math notranslate nohighlight">\(F_X\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= P\left( a + b X \leq y \right) \\
                  &amp;= P\left( X \leq \frac{y - a}{b} \right) \\
                  &amp;= F_X\left(\frac{y - a}{b} \right)\end{split}\]</div>
<p class="note">Exercici: Fent servir aquesta identitat, demostreu que si <span class="math notranslate nohighlight">\(X \sim \mathcal{N}\left(\mu, \sigma\right)\)</span>, <span class="math notranslate nohighlight">\(Y = \frac{X - \mu}{\sigma} \sim \mathcal{N}\left(0, 1\right)\)</span></p>
</div>
<div class="section" id="cas-generic">
<h3><span class="section-number">2.4.2. </span>Cas genèric<a class="headerlink" href="#cas-generic" title="Permalink to this headline">¶</a></h3>
<p>Per una funció genèrica, <span class="math notranslate nohighlight">\(g: \mathcal{X}\to\mathcal{Y}\)</span>,
no serà tan senzill caracteritzar la f.d.c de <span class="math notranslate nohighlight">\(Y\)</span> en funció de la d’<span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Sota unes condicions tècniques relativament generals, podrem definir una funció de
probabilitat el conjunt d’esdeveniments associat a l’espai mostral <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> com segueix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P\left(Y \in A\right) &amp; = P\left(\left\{x \in \mathcal{X}: g\left(x\right) \in A \right\}\right) \\
                      &amp; = P\left(X \in g^{-1}\left(A\right)\right)\end{split}\]</div>
<p>on definim el mapa invers <span class="math notranslate nohighlight">\(g^{-1}\left(A\right) = \left\{ x\in \mathcal{X}: g(x) \in A\right\}\)</span> [Casella &amp; Berger 2.1.1]</p>
<p>Vegem la relació entre els tres espais mostrals mitjantçant un diagrama:</p>
<div class="figure align-center" id="id5">
<a class="reference internal image-reference" href="../_images/transformation.png"><img alt="../_images/transformation.png" src="../_images/transformation.png" style="height: 280px;" /></a>
<p class="caption"><span class="caption-text"><span class="math notranslate nohighlight">\(P\left(Y \in A\right) = P\left(X \in g^{-1}\left(A\right)\right) = P\left(\left\{ s \in \Omega: X(s) \in g^{-1}\left(A\right) \right\}\right)\)</span></span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p class="note">Sortosament, normalment no haurem de raonar directament sobre <span class="math notranslate nohighlight">\(\Omega\)</span>, ja que en molts casos
podrem caracteritzar <span class="math notranslate nohighlight">\(Y\)</span> en base a la f.d.c d’<span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
<div class="section" id="f-d-c-i-transformacions-monotones">
<h3><span class="section-number">2.4.3. </span>F.d.c i transformacions monòtones<a class="headerlink" href="#f-d-c-i-transformacions-monotones" title="Permalink to this headline">¶</a></h3>
<p>En general, la f.d.c. d’<span class="math notranslate nohighlight">\(Y\)</span> vé donada per l’expressió [Casella &amp; Berger 2.1.4]:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= P\left( g\left(X\right) \leq y \right) \\
                  &amp; = P_X\left(\left\{x \in \mathcal{X}: g\left(x\right) \leq y \right\}\right)\end{split}\]</div>
<p>En el cas que la funció <span class="math notranslate nohighlight">\(g: \mathcal{X}\to\mathcal{Y}\)</span> sigui monòtona stricta (creixent o decreixent),
tindrem que és injectiva i surjectiva, i per tant podem definir <span class="math notranslate nohighlight">\(g^{-1}: \mathcal{Y}\to\mathcal{X}\)</span>
associant un únic x a cada y. Per exemple, en el cas monòton creixent:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{x \in \mathcal{X}: g\left(x\right) \leq y \right\} &amp; =  \left\{x \in \mathcal{X}: x \leq g^{-1}\left(y\right) \right\} \\\end{split}\]</div>
<p>Per tant podem simplicar l’expressió [Casella &amp; Berger 2.1.3]: <span class="math notranslate nohighlight">\(F_Y\left(y\right) =F_x\left(g^{-1}\left(y\right)\right)\)</span></p>
</div>
<div class="section" id="transformacions-monotones-i-diferenciables">
<h3><span class="section-number">2.4.4. </span>Transformacions monòtones i diferenciables<a class="headerlink" href="#transformacions-monotones-i-diferenciables" title="Permalink to this headline">¶</a></h3>
<p>Si ens restringim a v.a’s contínues i a transformacions estrictament monòtones diferenciables:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= F_x\left(g^{-1}\left(y\right)\right) \mbox{(g creixent)} \\
F_Y\left(y\right) &amp;= 1 - F_x\left(g^{-1}\left(y\right)\right) \mbox{(g decreixent)}\\\end{split}\]</div>
<p>són diferenciables, i aplicant la regla de la cadena arribem al famós resultat de la “transformació per Jacobià” [Casella &amp; Berger 2.1.5]:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_Y\left(y\right) &amp;= f_x\left(g^{-1}\left(y\right)\right)\left|\frac{d g^{-1}\left(y\right)}{dy} \right| \\\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\forall y \in \mathcal{Y} = \left\{y : \exists x\in \mathcal{X},  g(x)=y \right\}\)</span>
on <span class="math notranslate nohighlight">\(\mathcal{X} = \left\{x : f_X\left(x\right) &gt; 0 \right\}\)</span>.</p>
<p class="note">Veure [Casella &amp; Berger 2.1.8] per una extensió on la funció <span class="math notranslate nohighlight">\(g\)</span> és monòtona només sobre alguns intervals!</p>
<p>Veiem aquí un exemple de com el resultat anterior es pot extendre a transformacions
no monòtones interessants en estadística. Considerarem la distribució de la transformació (contínua i diferenciable)</p>
<p><span class="math notranslate nohighlight">\(Y = X^2\)</span></p>
<p>quan <span class="math notranslate nohighlight">\(X \sim \mathcal{N}\left(0, 1\right)\)</span>. Observem que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= P_X\left(-\sqrt{y} \leq X \leq \sqrt{y}\right) \\
                  &amp;= P_X\left(X \leq \sqrt{y}\right) - P_X\left(X \leq -\sqrt{y}\right) \\
                  &amp;= F_X\left(\sqrt{y}\right) - F_X\left(-\sqrt{y}\right)\end{split}\]</div>
<p>Diferenciant i fent servir la simetria de <span class="math notranslate nohighlight">\(F_X\left(x\right)\)</span> respecte 0, obtenim:</p>
<p><span class="math notranslate nohighlight">\(f_y\left(y\right) = y^{-\frac{1}{2}} f_X\left(\sqrt{y}\right) = \frac{y^{-\frac{1}{2}}}{\sqrt{2\pi}} e^{-\frac{y}{2}}\)</span>,
que podem identificar amb la Gamma si fixem <span class="math notranslate nohighlight">\(\alpha=1/2\)</span> i <span class="math notranslate nohighlight">\(\beta=2\)</span>, que és
la <span class="math notranslate nohighlight">\(\chi^2_1\)</span>.</p>
</div>
<div class="section" id="transformacio-integral">
<h3><span class="section-number">2.4.5. </span>Transformació integral<a class="headerlink" href="#transformacio-integral" title="Permalink to this headline">¶</a></h3>
<p>L’última transformació que veurem inspirarà un
algoritme per generar mostres de v.a. contínues amb distribucions
arbitràries (ho veurem a la primera pràctica).</p>
<p class="note">[Casella &amp; Berger 2.1.10] Sigui <span class="math notranslate nohighlight">\(X\)</span> una v.a. contínua caracteritzada per <span class="math notranslate nohighlight">\(F_X\)</span>. Aleshores
la v.a. <span class="math notranslate nohighlight">\(Y = F_X\left(X\right)\)</span> és uniforme entre <span class="math notranslate nohighlight">\(\left[0, 1\right]\)</span></p>
<p>La demostració passa per la definició de la funció:</p>
<p><span class="math notranslate nohighlight">\(F_X^{-1}\left(y\right) = \left\{\begin{array}{cc} \inf \left\{x : F\left(x\right) \geq y \right\} &amp; y \in \left(0, 1\right) \\ \infty &amp; y=1 \\ -\infty &amp; y = 0 \end{array}\right.\)</span></p>
<p>I observant que (compte amb el segon “=”!):</p>
<p><span class="math notranslate nohighlight">\(P\left(Y \leq y \right) = P\left(F_X\left(X\right) \leq y\right) = P\left(X \leq F_X^{-1}\left(y\right)\right) = y\)</span></p>
</div>
<div class="section" id="id1">
<h3><span class="section-number">2.4.6. </span>Qüestionari de repàs<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<ol class="build arabic simple">
<li><p>Quines condicions ha de verificar una f.d.p o una f.m.p?</p></li>
<li><p>Si compro un dècim de loteria de nadal cada any, i hi ha 170M de dècims, quina és la probabilitat que no em toqui en tota la vida?</p></li>
<li><p>Les v.a. són contínues o discretes segons si la seva <span class="math notranslate nohighlight">\(F_X\)</span> és contínua o discreta: cert o fals?</p></li>
<li><p>Per què el fet que una v.a. contínua satisfaci <span class="math notranslate nohighlight">\(P(X=x)=0\)</span> no implica que <span class="math notranslate nohighlight">\(P(a &lt; X \leq b)=0\)</span>?</p></li>
<li><p>La distribució de Poisson és un bona aproximació de la binomial quan ______ i ________.</p></li>
<li><p>Per demostrar el teorema de la transformació integral, hem definit <span class="math notranslate nohighlight">\(F_X^{-1}\left(y\right) = \inf \left\{x : F\left(x\right) \geq y \right\}, y \in (0, 1)\)</span>. Perquè l’infimum i perquè el &gt;=?</p></li>
<li><p>En quins 4 casos podem caracteritzar fàcilment una variable <span class="math notranslate nohighlight">\(Y=g(X)\)</span> en funció de la distribució de X?</p></li>
</ol>
</div>
</div>
<div class="section" id="esperanca-i-moments">
<h2><span class="section-number">2.5. </span>Esperança i moments<a class="headerlink" href="#esperanca-i-moments" title="Permalink to this headline">¶</a></h2>
<div class="section" id="esperanca">
<h3><span class="section-number">2.5.1. </span>Esperança<a class="headerlink" href="#esperanca" title="Permalink to this headline">¶</a></h3>
<p>L’esperança o mitja d’una v.a. <span class="math notranslate nohighlight">\(g\left(X\right)\)</span> es defineix com:</p>
<ul class="simple">
<li><p>Cas continu: <span class="math notranslate nohighlight">\(E\left(g\left(X\right)\right) = \int_{-\infty}^{\infty} g\left(x\right)f_X\left(x\right)dx\)</span></p></li>
<li><p>Cas discret: <span class="math notranslate nohighlight">\(E\left(g\left(X\right)\right) = \sum_{k} g\left(k\right)p_X\left(k\right)\)</span></p></li>
</ul>
<p>Com ja sabeu, l’esperança pot ser un indicador de “localització” però depèn de la dispersió
(ex: variança) de la distribució en questió…</p>
<p class="note">Aquesta definició es coneix com la <a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">Llei de l’Estadístic Inconscient</a>,
perquè en realitat l’existència de <span class="math notranslate nohighlight">\(E\left(g\left(X\right)\right)\)</span> s’hauria de provar formalment.
Recordeu doncs que l’esperança no té perquè existir! L’exemple clàssic és <span class="math notranslate nohighlight">\(g(x)=x\)</span> i amb una distribució de Cauchy.</p>
<p><strong>Exercici</strong>: Podeu imaginar una distribució on l’esperança ens pot donar una idea equivocada?</p>
<p>L’esperança i la mitja aritmètica són cosines germanes. La següent és una interpretació
de la mitja que pot ser útil:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/mitja_Esperança.png"><img alt="../_images/mitja_Esperança.png" src="../_images/mitja_Esperança.png" style="height: 330px;" /></a>
</div>
<p>Per tant <span class="math notranslate nohighlight">\(\bar{X} = \sum_{i} c_i \frac{\mbox{#}\left\{x=c_i\right\}}{N} = \sum_{i} c_i \hat{P}\left(X = c_i\right)\)</span> (S’assembla a l’esperança, no?)</p>
<p>El càlcul l’esperança sol dependre una mica de la forma de la f.d.p o f.m.p
de la v.a. en questió.</p>
<p>Vegem com ho fariem per la f.m.p de Poisson:</p>
<p><span class="math notranslate nohighlight">\(P\left(X = k; \lambda \right) = \frac{\lambda^k}{k!}e^{-\lambda}\)</span></p>
<p>Aplicant la definició:</p>
<div class="math notranslate nohighlight">
\[\begin{split}E(X) &amp; = \sum_{k=0}^{\infty} k \frac{\lambda^k}{k!}e^{-\lambda} \\
     &amp; = \lambda e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{\left(k-1\right)!} \\
     &amp; = \lambda\end{split}\]</div>
<p>On hem fet servir l’identitat <span class="math notranslate nohighlight">\(\sum_{j=0}^{\infty} \frac{\lambda^{j}}{j!}=e^{\lambda}\)</span></p>
<p>Suposem <span class="math notranslate nohighlight">\(X \sim U[0, 1]\)</span> i <span class="math notranslate nohighlight">\(Y=-\log(X)\)</span>:</p>
<ul class="simple">
<li><p>Podem calcular <span class="math notranslate nohighlight">\(E(Y)\)</span> adonant-nos que <span class="math notranslate nohighlight">\(F_Y(y)=P(Y \leq y) = 1 - e^{-y}\)</span>, per tant Y es exponencial amb paràmetre <span class="math notranslate nohighlight">\(\lambda=1\)</span></p></li>
<li><p>Podem calcular <span class="math notranslate nohighlight">\(E(Y) = \int_{0}^1 -\log(x)dx=x - \log(x)|^1_0=1\)</span></p></li>
</ul>
<p class="note">En alguns casos podem escollir entre fer servir la definició amb <span class="math notranslate nohighlight">\(g(x)\)</span>
o bé calcular la f.d.p de <span class="math notranslate nohighlight">\(Y=g(X)\)</span> per calcular-ne <span class="math notranslate nohighlight">\(E(Y)\)</span>.</p>
<p>La majoria de propietats de l’esperança provenen de la
linearitat de l’operador integració/suma [Casella &amp; Berger 2.2.5]*:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(E\left(a X + b Y + c\right) = aE\left( X\right) + b E\left( Y\right) + c\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(X \geq Y\)</span>, aleshores <span class="math notranslate nohighlight">\(E\left(X\right)\geq E\left(Y\right)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(a\geq X \geq b\)</span>, aleshores <span class="math notranslate nohighlight">\(a \geq E\left(X\right)\geq b\)</span></p></li>
</ol>
<p><em>Demostració (1) i (2) a la pissarra, (3) com exercici</em></p>
<p class="note"><a href="#id2"><span class="problematic" id="id3">*</span></a>Nota: En realitat, a [Casella &amp; Berger 2.2.5] contemplen només el cas
<span class="math notranslate nohighlight">\(X=g_1(x)\)</span> i <span class="math notranslate nohighlight">\(Y=g_2(X)\)</span>. Aquí farem una mica de trampa i farem
servir ja les distribucions marginals d’<span class="math notranslate nohighlight">\(X\)</span> i <span class="math notranslate nohighlight">\(Y\)</span> que encara no hem definit
però definirem més endavant.</p>
<p class="build">Tot i que podria semblar un resultat trivial, la linearitat de l’esperança és una propietat
molt útil a la pràctica. Vegem per exemple la seva aplicació en el següent problema:</p>
<p>[Rice 4.1.2 Exemple B] Tenim <span class="math notranslate nohighlight">\(n\)</span> tipus diferents de cupons, i cada cop que comprem cereals ens en donen
un dels <span class="math notranslate nohighlight">\(n\)</span> a l’atzar. Quans cereals haurem de comprar per aconseguir-los tots?</p>
<ol class="arabic simple">
<li><p>Definim <span class="math notranslate nohighlight">\(X_i\)</span> nombre de compres fins que aconseguim el cupó <em>i</em></p></li>
<li><p>El nombre total de compres <span class="math notranslate nohighlight">\(Y=\sum_{i=1}^n X_i\)</span></p></li>
<li><p>Noteu que <span class="math notranslate nohighlight">\(X_i\)</span> es una v.a. geomètrica, amb paràmetre <span class="math notranslate nohighlight">\(p_i = \frac{n -i + 1}{n}\)</span></p></li>
<li><p>Recordem que en aquest cas <span class="math notranslate nohighlight">\(E(X_i) = \frac{1}{p_i}\)</span></p></li>
<li><p>Per linearitat de l’esperança <span class="math notranslate nohighlight">\(E(Y)=\sum_{i=1}^n E(X_i) = \frac{n}{n} + \frac{n}{n-1} + \cdots + n = n\sum_{i=1}^n\frac{1}{i}\)</span></p></li>
</ol>
<p>[Rice 4.1.2 Exemple E] Suposeu que tenim una cartera d’inversió amb dues accions A i B de borsa
amb retorns representats per v.a.’s <span class="math notranslate nohighlight">\(R_A\)</span> i <span class="math notranslate nohighlight">\(R_B\)</span>.</p>
<ol class="build arabic simple">
<li><p>Si invertim una fracció <span class="math notranslate nohighlight">\(\pi\)</span> del nostre capital a A, i <span class="math notranslate nohighlight">\(1-\pi\)</span> a B, tindrem que el retorn final serà: <span class="math notranslate nohighlight">\(R = \pi R_1 + \left(1 - \pi\right)R_2\)</span></p></li>
<li><p>Per linearitat de l’esperança, <span class="math notranslate nohighlight">\(E(R) = \pi E(R_A) + \left(1 - \pi\right)E(R_B)\)</span></p></li>
<li><p>Per tant l’estratègia òptima d’inversió seria <span class="math notranslate nohighlight">\(\pi=1\)</span> si <span class="math notranslate nohighlight">\(E(R_A)&gt;E(R_B)\)</span> i <span class="math notranslate nohighlight">\(\pi=0\)</span> en cas contrari</p></li>
</ol>
<p class="note">Fixeu-vos que una possible correlació entre <span class="math notranslate nohighlight">\(R_A\)</span> i <span class="math notranslate nohighlight">\(R_B\)</span> és irrellevant…</p>
<p class="note">Clarament la gestió de carteres és més complicada que això… què creieu que falla en el nostre model?</p>
</div>
<div class="section" id="moments-i-moments-centrals">
<h3><span class="section-number">2.5.2. </span>Moments i moments centrals<a class="headerlink" href="#moments-i-moments-centrals" title="Permalink to this headline">¶</a></h3>
<p>A partir de l’esperança podem definir altres quantitats caracteritzant
una v.a. [Casella &amp; Berger 2.3.1]. Per tot enter <span class="math notranslate nohighlight">\(n\)</span>, definim:</p>
<ul class="simple">
<li><p>El moment d’ordre <span class="math notranslate nohighlight">\(n\)</span> com: <span class="math notranslate nohighlight">\(\mu_n' = E(X^n)\)</span></p></li>
<li><p>El moment <em>central</em> d’ordre <span class="math notranslate nohighlight">\(n\)</span> com: <span class="math notranslate nohighlight">\(\mu_n = E\left(\left(X - E(X)\right)^n\right)\)</span></p></li>
</ul>
<p>(que recordem no tenen perquè existir!)</p>
<p class="build">Exemples:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu_0'=1\)</span>, <span class="math notranslate nohighlight">\(\mu_1=0\)</span></p></li>
<li><p>La variança, <span class="math notranslate nohighlight">\(\mbox{Var}\left(X\right) = \mu_2\)</span>, que indica la desviació d’X respecte la seva mitja</p></li>
<li><p>L’asimetria (<em>skewness</em>) <span class="math notranslate nohighlight">\(\mu_3\)</span> que ens indica si la cua de la f.d.p està a l’esquerra (&lt;0) o a la dreta (&gt;0) de la mitja</p></li>
</ul>
</div>
<div class="section" id="l-esperanca-com-a-predictor-de-minim-error-quadrat">
<h3><span class="section-number">2.5.3. </span>L’esperança com a predictor de mínim error quadrat<a class="headerlink" href="#l-esperanca-com-a-predictor-de-minim-error-quadrat" title="Permalink to this headline">¶</a></h3>
<p>Amb aquesta última definició podem establir una propietat
fonamental de l’esperança. Considerem el següent problema de predicció
d’una v.a. X tal que minitzem l’error de predicció [Casella &amp; Berger 2.2.6]:</p>
<p class="note">Trobar <span class="math notranslate nohighlight">\(\theta\)</span> tal que <span class="math notranslate nohighlight">\(\min_{\theta} E\left(X - \theta \right)^2\)</span>.</p>
<p>Observem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}E\left(X - \theta \right)^2 &amp;= E\left(X - E(X) + \left(E(X) - \theta\right)\right)^2 \\
                            &amp;= E\left(X - E(X)\right)^2 + E\left(E(X) - \theta\right)^2 + \\
                            &amp; + 2E\left(X - E(X)\right)E\left(E(X) - \theta\right) \\
                            &amp;= \mbox{Var}\left(X\right) + E\left(E(X) - \theta\right)^2 \geq \mbox{Var}\left(X\right)\end{split}\]</div>
<p>(pel tercer “=”, <span class="math notranslate nohighlight">\(E\left(X - E(X)\right)=0\)</span>.) Per tant <span class="math notranslate nohighlight">\(\theta^*=E(X)\)</span>!</p>
</div>
<div class="section" id="funcio-generatriu-de-moments">
<h3><span class="section-number">2.5.4. </span>Funció generatriu de moments<a class="headerlink" href="#funcio-generatriu-de-moments" title="Permalink to this headline">¶</a></h3>
<p>Per una v.a. X, la funció generatriu de moments (<em>f.g.m</em>) (<em>Moment-Generating Function</em>) es defineix com:</p>
<p><span class="math notranslate nohighlight">\(M_X\left(t\right)=E\left(e^{tX}\right)\)</span></p>
<p>suposant que existeix per <span class="math notranslate nohighlight">\(t\in [-\epsilon, \epsilon]\)</span> amb <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span>.</p>
<p>En estadística, la f.g.m es fa servir majoritàriament per tres raons:</p>
<ol class="arabic simple">
<li><p>Calcular els moments d’una distribució que altrament serien molt difícils de calcular, veure [Casella &amp; Berger 2.3.7 i 2.3.8], per exemple els moments d’una Gamma.</p></li>
<li><p>Per calcular distribucions de transformacions afins de v.a. [Casella &amp; Berger 2.3.15]</p></li>
<li><p>Per calcular la distribució de la suma de v.a. independents [Casella &amp; Berger 4.2.12]</p></li>
</ol>
<p>En aquest curs la farem servir més endavant per l’objectiu (3).</p>
<p>Per ara, només mencionar que aquesta utilitat es deriva d’un resultat
fonamental per les f.g.m’s, que és que sota algunes condicions, la f.g.m
caracteritza inequívocament una f.d.c:</p>
<p class="note">[Casella &amp; Berger 2.3.11] Siguin <span class="math notranslate nohighlight">\(F_X\)</span>, <span class="math notranslate nohighlight">\(F_Y\)</span> f.d.c’s per les quals
tots els moments existeixen. Aleshores <span class="math notranslate nohighlight">\(M_X\left(t\right)=M_Y\left(t\right)\)</span>
per <span class="math notranslate nohighlight">\(t\in [-\epsilon, \epsilon]\)</span> amb <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> implica que <span class="math notranslate nohighlight">\(F_X = F_Y\)</span>.</p>
<p>És aquest resultat el que ens permetrà més endavant de calcular
una f.g.m i a partir d’aquesta desfer el camí i obtenir-be la f.d.c. corresponent.</p>
</div>
</div>
<div class="section" id="desigualtats">
<h2><span class="section-number">2.6. </span>Desigualtats<a class="headerlink" href="#desigualtats" title="Permalink to this headline">¶</a></h2>
<div class="section" id="desigualtat-de-markov">
<h3><span class="section-number">2.6.1. </span>Desigualtat de Markov<a class="headerlink" href="#desigualtat-de-markov" title="Permalink to this headline">¶</a></h3>
<p>La desigualta de Markov estableix que, per qualsevol v.a. <span class="math notranslate nohighlight">\(X \geq 0\)</span>,
tal que <span class="math notranslate nohighlight">\(E(X)\)</span> existeix, podem acotar la probabilitat de que <span class="math notranslate nohighlight">\(X\)</span>
excedeixi un cert valor <span class="math notranslate nohighlight">\(t &gt; 0\)</span> per:</p>
<p><span class="math notranslate nohighlight">\(P(X \geq t) \leq \frac{E(X)}{t}\)</span></p>
<p>Per exemple, si fixem <span class="math notranslate nohighlight">\(t = kE(X)\)</span>, podem acotar la proba que <span class="math notranslate nohighlight">\(X\)</span>
excedeixi la seva mitja per un factor k:</p>
<p><span class="math notranslate nohighlight">\(P(X \geq k E(X)) \leq \frac{1}{k}\)</span></p>
<p><em>Demostració</em>:</p>
<p>Descomposar <span class="math notranslate nohighlight">\(E(X)=\int_{-\infty}^t x f_X(x)dx + \int_t^{\infty} x f_X(x)dx\)</span>
i observar que <span class="math notranslate nohighlight">\(\int_{-\infty}^t x f_X(x)dx \geq 0\)</span> i <span class="math notranslate nohighlight">\(\int_t^{\infty} x f_X(x)dx \geq t P(X\geq t)\)</span></p>
</div>
<div class="section" id="desigualtat-de-txebitxev">
<h3><span class="section-number">2.6.2. </span>Desigualtat de Txebitxev<a class="headerlink" href="#desigualtat-de-txebitxev" title="Permalink to this headline">¶</a></h3>
<p>La desigualtat de Markov és molt laxa perquè només fa servir informació sobre <span class="math notranslate nohighlight">\(E(X)\)</span>.
La seva extensió (desigualtat de Txebitxev) ens permetrà establir cotes una mica més
útils. Sigui v.a. <span class="math notranslate nohighlight">\(g(X) \geq 0\)</span>, tal que <span class="math notranslate nohighlight">\(E(g(X))\)</span> existeix, per qualsevol <span class="math notranslate nohighlight">\(t &gt; 0\)</span> tenim:</p>
<p><span class="math notranslate nohighlight">\(P(g(X) \geq t) \leq \frac{E(g(X))}{t}\)</span></p>
<p>A priori això sembla calcat a la de Markov, però vegem-ne una aplicació:</p>
<p class="note">Sigui <span class="math notranslate nohighlight">\(X\)</span> una v.a. amb mitja <span class="math notranslate nohighlight">\(\mu\)</span> i variança <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Aleshores
<span class="math notranslate nohighlight">\(P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}\)</span></p>
<p><em>Demostració</em>: Per demostrar Txebitxev, es segueix la mateixa idea que per Markov.
Per demostrar-ne aquesta aplicació, n’hi ha prou amb aplicar Txebitxev amb <span class="math notranslate nohighlight">\(g(x) = \left(\frac{x - \mu}{\sigma}\right)^2\)</span></p>
</div>
<div class="section" id="desigualtat-de-jensen">
<h3><span class="section-number">2.6.3. </span>Desigualtat de Jensen<a class="headerlink" href="#desigualtat-de-jensen" title="Permalink to this headline">¶</a></h3>
<p>L’última desigualtat que considerarem és la de Jensen, que sota
la seva simplicitat amaga moltíssim poder.</p>
<p>Sigui <span class="math notranslate nohighlight">\(g(x)\)</span> una funció convexa i <span class="math notranslate nohighlight">\(X\)</span> una v.a. tal que <span class="math notranslate nohighlight">\(E(g(x))\)</span>
existeix. Aleshores:</p>
<p><span class="math notranslate nohighlight">\(E(g(x)) \geq g(E(X))\)</span></p>
<p><em>Aplicacions</em>: Moltíssimes, però dues d’immediates són les d’obtenir cotes, per exemple:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E(X^2) \geq (E(X))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(E(\frac{1}{X}) \geq \frac{1}{E(X)}\)</span></p></li>
</ul>
<p><em>Demostració</em>: A la pissarra</p>
</div>
</div>
<div class="section" id="vectors-aleatoris-variables-multivariades">
<h2><span class="section-number">2.7. </span>Vectors aleatoris / Variables multivariades<a class="headerlink" href="#vectors-aleatoris-variables-multivariades" title="Permalink to this headline">¶</a></h2>
<div class="section" id="variables-multivariades">
<h3><span class="section-number">2.7.1. </span>Variables multivariades<a class="headerlink" href="#variables-multivariades" title="Permalink to this headline">¶</a></h3>
<p>Una variable aleatòria multivariada és una extensió d’una v.a.
a múltiples dimensions:</p>
<p class="note"><strong>Definició</strong> Una variable aleatòria multivariada (<em>v.m.</em> pels amics)
és una funció <span class="math notranslate nohighlight">\(\mathbf{X} : \Omega \to \mathcal{X} \subseteq \mathbb{R}^K\)</span>.</p>
<p>Exemples:</p>
<ul class="simple">
<li><p><strong>V.m. contínua</strong>: Mesurem l’alçada, pes i perímetre cranial dels nadons al néixer. Cada mostra es pot interpretar com una v.m. contínua de dimensió 3, que pren valors en <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>.</p></li>
<li><p><strong>V.m. discreta</strong>: Agafem un document de text i en contem el nombre de vegades que apareixen <span class="math notranslate nohighlight">\(K\)</span> paraules d’un diccionari. El resultat és un vector discret que pren valors a <span class="math notranslate nohighlight">\(\mathbb{N}^K\)</span></p></li>
</ul>
</div>
<div class="section" id="funcio-de-densitat-o-de-massa-de-probabilitat-conjuntes-cas-bivariat">
<h3><span class="section-number">2.7.2. </span>Funció de densitat o de massa de probabilitat conjuntes: Cas bivariat<a class="headerlink" href="#funcio-de-densitat-o-de-massa-de-probabilitat-conjuntes-cas-bivariat" title="Permalink to this headline">¶</a></h3>
<p>De manera anàloga al que hem vist per v.a.’s, ver una v.m <span class="math notranslate nohighlight">\((X, Y)\)</span> també
podem definir:</p>
<ul class="simple">
<li><p>Una <em>funció de massa de probabilitat conjunta</em>: <span class="math notranslate nohighlight">\(p_{X,Y}(x, y)\)</span> si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> contable</p></li>
<li><p>Una <em>funció de distribució de probabilitat conjunta</em>: <span class="math notranslate nohighlight">\(f_{X,Y}(x, y)\)</span> si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> incontable</p></li>
</ul>
<p>Per tal de ser vàlides aquestes funcions han de verificar:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p_{X,Y}(x, y) \geq 0\)</span>, <span class="math notranslate nohighlight">\(\sum_{x, y \in \mathcal{X}}p_{X,Y}(x, y)=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_{X,Y}(x, y) \geq 0\)</span>, <span class="math notranslate nohighlight">\(\int\int_{x, y \in \mathcal{X}}f_{X,Y}(x, y)=1\)</span></p></li>
</ul>
<p class="note">Durant aquest repàs, presentarem els conceptes només pel cas bivariat (<span class="math notranslate nohighlight">\(N=2\)</span>),
l’extensió a <span class="math notranslate nohighlight">\(K&gt;2\)</span> és gairebé immediata [Casella &amp; Berger 4.6]</p>
<p>La f.m.p. o la f.d.c. es poden utilitzar per caracteritzar la probabilitat
d’un esdeveniment <span class="math notranslate nohighlight">\(A\)</span> (omitirem el cas discret):</p>
<p><span class="math notranslate nohighlight">\(P((X, Y) \in A) = \int\int_{x, y \in A} f_{X,Y}(x,y)dx dy\)</span></p>
<p><em>Exemple</em>: <span class="math notranslate nohighlight">\((X,Y)\)</span> són les coordenades d’arribada d’un dard llançat
en un tauler de radi <span class="math notranslate nohighlight">\(r &gt;0\)</span>.</p>
<p>Si suposem que sóm uniformement dolents llançant dards, podem caracteritzar la
f.d.p com una uniforme en el cercle de radi <span class="math notranslate nohighlight">\(r\)</span>:</p>
<p><span class="math notranslate nohighlight">\(f(x,y) = \left\{\begin{array}{cc}\frac{1}{\pi}&amp;\mbox{ si } x^2 + y^2 \leq 1 \\ 0  &amp;\mbox{ altrament } \end{array}\right\}\)</span></p>
<p><em>Exercici</em>: Calculeu <span class="math notranslate nohighlight">\(P((X, Y) \in A)\)</span> per <span class="math notranslate nohighlight">\(A = \left\{x, y: t \leq x^2 + y^2 \leq 1\right\}\)</span></p>
</div>
<div class="section" id="distribucions-marginals">
<h3><span class="section-number">2.7.3. </span>Distribucions marginals<a class="headerlink" href="#distribucions-marginals" title="Permalink to this headline">¶</a></h3>
<p>A vegades voldrem caracteritzar només un dels components d’un vector aleatori <span class="math notranslate nohighlight">\((X, Y)\)</span>.</p>
<p>Per exemple, si volem calcular <span class="math notranslate nohighlight">\(P(X \in A_x)\)</span>.</p>
<p>Per fer-ho, farem servir el que s’anomena la f.d.p <em>marginal</em> d’X:</p>
<p><span class="math notranslate nohighlight">\(f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy\)</span> (cas continu)</p>
<p>o:</p>
<p><span class="math notranslate nohighlight">\(p_X(x) = \sum_{k} f_{X,Y}(x, k) dy\)</span> (cas discret)</p>
<p><strong>Exemple:</strong> Quina és la probabilitat que el dard de l’exemple anterior caigui
en la regió <span class="math notranslate nohighlight">\(-1\leq y \leq 1\)</span> (<span class="math notranslate nohighlight">\(r&gt;1\)</span>).</p>
<p class="note">És important recordar que les marginals no contenen tota la informació que hi ha en la conjunta!</p>
</div>
<div class="section" id="exemple-de-variable-multivariada-discreta-multinomial">
<h3><span class="section-number">2.7.4. </span>Exemple de variable multivariada discreta: multinomial<a class="headerlink" href="#exemple-de-variable-multivariada-discreta-multinomial" title="Permalink to this headline">¶</a></h3>
<p>Considerem ara un exemple d’una v.m. discreta per <span class="math notranslate nohighlight">\(K \geq 2\)</span>:</p>
<ul class="simple">
<li><p>Suposeu un experiment en que cada realització pren un entre <span class="math notranslate nohighlight">\(K\)</span> valors discret, amb probabilitats <span class="math notranslate nohighlight">\(p_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\cdots,K\)</span>, <span class="math notranslate nohighlight">\(\sum_i p_i = 1\)</span>.</p></li>
<li><p>Repetim l’experiment <span class="math notranslate nohighlight">\(N\)</span> vegades, cada realització és mutuament independent amb les altres</p></li>
<li><p>Definim la v.m. <span class="math notranslate nohighlight">\(\mathbf{X}=\left[X_1, \cdots, X_K\right]\)</span> com un vector on cada element <span class="math notranslate nohighlight">\(X_i\)</span> conta el nombre de vegades que hem observat el valor <span class="math notranslate nohighlight">\(i\)</span></p></li>
</ul>
<p>Aleshores la v.m <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> segueix una distribució multinomial [Casella &amp; Berger 4.6.2]</p>
<p><span class="math notranslate nohighlight">\(p_{X_1, \cdots, X_K}\left(x_1, \cdots, x_K\right) = N!\Pi_{i=1}^{K}\frac{p_i^{x_i}}{x_i!}\)</span></p>
<p>És obvi que <span class="math notranslate nohighlight">\(p_{X_1, \cdots, X_K}\left(x_1, \cdots, x_K\right)\geq 0\)</span>. Per demostrar que això és efectivament una f.m.p,
haurem d’aplicar el Teorema Binomial [Casella &amp; Berger 4.6.4], que diu que</p>
<p><span class="math notranslate nohighlight">\(\left(p_1 + \cdots + p_N\right)^N = N!\Pi_{i=1}^{K}\frac{p_i^{x_i}}{x_i!}\)</span></p>
<p>i això és igual a 1 ja que <span class="math notranslate nohighlight">\(p_1 + \cdots + p_N=1\)</span>.</p>
<p>Fixeu-vos que:</p>
<ol class="arabic simple">
<li><p>Si K=2, <span class="math notranslate nohighlight">\(X_1\)</span> segueix una distribució binomial amb paràmetres <span class="math notranslate nohighlight">\(p_1, N\)</span> (i <span class="math notranslate nohighlight">\(X_2\)</span> també!)</p></li>
<li><p>La f.m.p. de qualsevol <span class="math notranslate nohighlight">\(X_i\)</span>, és també una binomial (Exercici!)</p></li>
</ol>
<div class="figure align-center" id="id6">
<a class="reference internal image-reference" href="../_images/multinomial_exemple.png"><img alt="../_images/multinomial_exemple.png" src="../_images/multinomial_exemple.png" style="height: 500px;" /></a>
<p class="caption"><span class="caption-text">Això es coneix com el model “Bag-of-Words” d’un document</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="funcio-de-distribucio-cumulativa-conjunta">
<h3><span class="section-number">2.7.5. </span>Funció de distribució cumulativa conjunta<a class="headerlink" href="#funcio-de-distribucio-cumulativa-conjunta" title="Permalink to this headline">¶</a></h3>
<p>Som seria d’esperar, en el cas multivariat també podem definir una f.d.c conjunta. Per exemple,
per <span class="math notranslate nohighlight">\(N=2\)</span> i una v.m. <span class="math notranslate nohighlight">\((X, Y)\)</span>:</p>
<p><span class="math notranslate nohighlight">\(F_{X,Y}(x, y) = P(X \leq x, Y \leq y)\)</span></p>
<p>Cosa que en el cas continu, i en el cas que <span class="math notranslate nohighlight">\(f_{X,Y}\)</span> existeixi, implica:</p>
<p><span class="math notranslate nohighlight">\(F_{X,Y}(x, y) = \int^x_{-\infty}\int^y_{-\infty} f_{X,Y}(x',y')dx'dy'\)</span></p>
<p class="note">Les f.d.c són una mica menys útils en el cas multivariat ja que “només” ens serveixen
per calcular probabilitats d’esdeveniments “rectangulars”</p>
</div>
<div class="section" id="funcio-de-distribucio-de-probabilitat-i-massa-condicionals">
<h3><span class="section-number">2.7.6. </span>Funció de distribució de probabilitat i massa condicionals<a class="headerlink" href="#funcio-de-distribucio-de-probabilitat-i-massa-condicionals" title="Permalink to this headline">¶</a></h3>
<p>De manera anàloga al que vam definir per a probabilitats d’esdeveniments,
podem definir f.d.p.’s o f.m.p’s condicionals.</p>
<p>Considero només el cas continu (pel cas discret la definició
és la mateixa però intercanviat <span class="math notranslate nohighlight">\(f\)</span> per <span class="math notranslate nohighlight">\(p\)</span>). Per qualsevol
<span class="math notranslate nohighlight">\(y\)</span> t.q. <span class="math notranslate nohighlight">\(f_Y(y) &gt; 0\)</span> definim la f.d.p condicional donat
<span class="math notranslate nohighlight">\(Y=y\)</span> com [Casella &amp; Berger 4.2.1 i 4.2.3]:</p>
<p><span class="math notranslate nohighlight">\(f_{X|Y=y}(x) = \frac{f_{X,Y}(x,y)}{f_Y(y)}\)</span></p>
<p>Si <span class="math notranslate nohighlight">\(X\)</span> segueix la distribució <span class="math notranslate nohighlight">\(f_{X|Y=y}(x)\)</span> direm que
<span class="math notranslate nohighlight">\(X | Y=y \sim f_{X|Y=y}\)</span> (sovint omitirem <span class="math notranslate nohighlight">\(y\)</span>)</p>
<p class="note">En realitat fixeu-vos que <span class="math notranslate nohighlight">\(f_{X|Y}(x)\)</span> és una família de distribucions:
per cada possible valor d’<span class="math notranslate nohighlight">\(Y\)</span> tenim una <span class="math notranslate nohighlight">\(f_{X|Y}(x)\)</span> diferent.</p>
</div>
<div class="section" id="llei-de-la-probabilitat-total">
<h3><span class="section-number">2.7.7. </span>Llei de la probabilitat total<a class="headerlink" href="#llei-de-la-probabilitat-total" title="Permalink to this headline">¶</a></h3>
<p>La f.d.p condicional ens permet desenvolupar una expressió equivalent a
la llei de la probabilitat total que vem veure per esdeveniments [Casella &amp; BErger 1.2.11],
[diapo 9, punt (1)].</p>
<p>L’idea és expressar una marginal en funció de la condicional:</p>
<p><span class="math notranslate nohighlight">\(f_{X}(x) = \int^{\infty}_{-\infty} f_{X,Y}(x,y)dy = \int^{\infty}_{-\infty} f_{X|Y=y}(x)f_Y(y)dy\)</span></p>
<p>(una expressió similar es pot obtenir pel cas discret, remplaçant les integrals per sumes i
<span class="math notranslate nohighlight">\(f\)</span> per <span class="math notranslate nohighlight">\(p\)</span>)</p>
<p>Aquesta expressió és molt útil per caracteritzar models jeràrquics, com veurem
a continuació.</p>
</div>
<div class="section" id="exemple-de-model-jerarquic-poisson-binomial">
<h3><span class="section-number">2.7.8. </span>Exemple de model jeràrquic: Poisson-Binomial<a class="headerlink" href="#exemple-de-model-jerarquic-poisson-binomial" title="Permalink to this headline">¶</a></h3>
<div class="figure align-center" id="id7">
<a class="reference internal image-reference" href="../_images/poisson_binomial.png"><img alt="../_images/poisson_binomial.png" src="../_images/poisson_binomial.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-text">Exemple de model jeràrquic amb dos nivells: Modelem les entrades de peatons en una tenda, com un model poisson-binomial. Quina és la distribució d’<span class="math notranslate nohighlight">\(X\)</span>?</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>Gràcies a la “lei de la probabilitat total” que acabem de veure,
podem derivar la distribució d’<span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}p_{X}(k) &amp; = \sum_{n=0}^\infty p_N(n) p_{X|N=n}(k) \\
         &amp; = \sum_{n=k}^\infty \frac{\lambda^n e^{-\lambda}}{n!} {n \choose k}p^{k}\left(1 - p\right)^{n-k}\\
         &amp; = \frac{\left(\lambda p\right)^k}{k!}e^{-\lambda p}\end{split}\]</div>
<p>Per tant <span class="math notranslate nohighlight">\(X \sim \mbox{Poisson}\left(\lambda p\right)\)</span></p>
<p><strong>Exercici</strong>: demostrar com es passa de la 2a la 3a igualtat.</p>
</div>
<div class="section" id="esperanca-condicional">
<h3><span class="section-number">2.7.9. </span>Esperança condicional<a class="headerlink" href="#esperanca-condicional" title="Permalink to this headline">¶</a></h3>
<p>Gràcies a la f.d.p. i la f.m.p. condicionals, podem definir
l’eperança condicional, que juga un rol important en estadística
com veurem tot seguit.</p>
<p><span class="math notranslate nohighlight">\(E(X|Y=y) = \int x f_{X|Y=y}(x) dx\)</span></p>
<p class="note">Noteu que a diferència de l’esperança “normal”, l’esperança condicional
és una variable aleatòria, ja que és una funció d’<span class="math notranslate nohighlight">\(Y\)</span>!</p>
<p>De fet, podem utilitzar l’esperança condicional per calcular l’esperança d’<span class="math notranslate nohighlight">\(X\)</span>,
gràcies al que a vegades s’anomena la “llei de l’esperança total” (en referència
a la llei de la probabilitat total) [Casella &amp; Berger 4.4.3]:</p>
<p><span class="math notranslate nohighlight">\(E(X) = E(E(X|Y))\)</span></p>
<p>Adoneu-vos que l’esperança “exterior” és respecte <span class="math notranslate nohighlight">\(Y\)</span>! Demostració com a exercici.</p>
<p>Aquesta última fórmula es pot fer servir per calcular esperances que altrament
serien molt complicades. Per exemple, considereu el següent model probabilístic:</p>
<ul class="simple">
<li><p>Tenim <span class="math notranslate nohighlight">\(X_i\)</span> tals que <span class="math notranslate nohighlight">\(E(X_i) = \mu\)</span>, <span class="math notranslate nohighlight">\(i=1, \cdots, N\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> és una v.a. independent d’<span class="math notranslate nohighlight">\(X_i\)</span>, amb <span class="math notranslate nohighlight">\(E(N) = \nu\)</span></p></li>
<li><p>Volem caracteritzar <span class="math notranslate nohighlight">\(T = \sum_{i=1}^N X_i\)</span></p></li>
</ul>
<p>(Per exemple, <span class="math notranslate nohighlight">\(X_i\)</span> podria referir-se al gasto d’un client i <span class="math notranslate nohighlight">\(N\)</span> al
nombre de clients que entren a una web. <span class="math notranslate nohighlight">\(T\)</span> seria els ingressos totals.)</p>
<p>Gràcies a l’expressió <span class="math notranslate nohighlight">\(E(T) = E(E(T|N))\)</span>, i fent servir una propietat
que veurem tot seguit sobre v.a.’s independents, tenim:</p>
<p><span class="math notranslate nohighlight">\(E(T) = E(N)E(X) = \mu \nu\)</span>.</p>
<p>I això sense saber res d’<span class="math notranslate nohighlight">\(f_{X_1, \cdots, X_N, N}\)</span>!</p>
</div>
<div class="section" id="esperanca-condicional-i-prediccio">
<h3><span class="section-number">2.7.10. </span>Esperança condicional i predicció<a class="headerlink" href="#esperanca-condicional-i-prediccio" title="Permalink to this headline">¶</a></h3>
<p>L’esperança condicional juga un rol molt
important en els problemes de predicció.</p>
<p>Vem veure a la diapo 62, que l’esperança d’una v.a. <span class="math notranslate nohighlight">\(X\)</span> era el predictor de
mínim error quadrat:</p>
<p><span class="math notranslate nohighlight">\(E(Y) = \arg\min_{\theta} E(Y - \theta)^2\)</span></p>
<p>En molts problemes, el que voldrem és predir <span class="math notranslate nohighlight">\(Y\)</span> en funció
d’un covariat (<em>covariate</em> o <em>feature</em>) <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Per exemple:</p>
<ul class="simple">
<li><p>Predir <span class="math notranslate nohighlight">\(Y\)</span>: la nota de l’exàmen final</p></li>
<li><p>en funció de <span class="math notranslate nohighlight">\(X\)</span> el nombre d’hores d’estudi</p></li>
</ul>
<p>En aquest cas, el problem és de trobar una funció <span class="math notranslate nohighlight">\(h(X)\)</span> tal que
minimitzi l’Error Quadràtic Mitjà:</p>
<p><span class="math notranslate nohighlight">\(\min_{h(x)} E(Y - h(X))^2\)</span> (<em>l’esperança és sobre :math:`X,Y`!</em>)</p>
<p>Gràcies a la llei de l’esperança total, podem escriure:</p>
<p><span class="math notranslate nohighlight">\(E(Y - h(X)\theta)^2 =  E(E(\left(Y - h(X)\right)^2 | X))^2\)</span></p>
<p>I fent servir el resultat esmentat, veiem que aquesta quantitat es minimitza
per <span class="math notranslate nohighlight">\(h(x) = E(Y | X=x)\)</span>!</p>
<p class="note">El millor predictor d’<span class="math notranslate nohighlight">\(Y\)</span> donat <span class="math notranslate nohighlight">\(X\)</span> és <span class="math notranslate nohighlight">\(E(Y | X=x)\)</span>!
Malauradament, aquest predictor requereix un coneixement de <span class="math notranslate nohighlight">\(f_{XY}\)</span>
per ser implementat. Durant el curs veurem altres predictors més útils,
per exemple els predictors linears, on <span class="math notranslate nohighlight">\(h(X) = a + b X\)</span>.</p>
</div>
<div class="section" id="variables-aleatories-independents">
<h3><span class="section-number">2.7.11. </span>Variables aleatòries independents<a class="headerlink" href="#variables-aleatories-independents" title="Permalink to this headline">¶</a></h3>
<p>El concepte d’esdeveniments independents:</p>
<p><span class="math notranslate nohighlight">\(A, B: P(A\cap B) = P(A)P(B)\)</span></p>
<p>es pot extendre a v.a.’s (o a les components d’un v.m.). Pel cas bivariat, tenim:</p>
<p class="note">[Casella &amp; Berger 4.2.5] si <span class="math notranslate nohighlight">\(X, Y \sim f_{X,Y}\)</span> i <span class="math notranslate nohighlight">\(X\sim f_{X}\)</span>,
<span class="math notranslate nohighlight">\(Y\sim f_{Y}\)</span> compleixen que <span class="math notranslate nohighlight">\(f_{X,Y}(x,y) = f_{X}(x)f_Y(y)\)</span>,
aleshores diem que <span class="math notranslate nohighlight">\(X, Y\)</span> són v.a.’s independents.</p>
<ul class="build simple">
<li><p>Podeu verificar que aquesta definició implica que per qualsevol <span class="math notranslate nohighlight">\(A, B\)</span>, els esdeveniments <span class="math notranslate nohighlight">\(X\in A, Y \in B\)</span> són independents.</p></li>
<li><p>Un resultat més sorprenent és que el recíproc també és cert, si existeixen <span class="math notranslate nohighlight">\(g(x), h(y)\)</span> tals que <span class="math notranslate nohighlight">\(f_{X,Y}(x,y) = h(x)g(y)\)</span>, aleshores <span class="math notranslate nohighlight">\(X, Y\)</span> són independents [Casella &amp; Berger 4.2.7]</p></li>
<li><p>El resultat s’extén de manera immediata a les components d’un v.m. amb <span class="math notranslate nohighlight">\(N&gt;2\)</span> [Casella &amp; Berger 4.6.5]</p></li>
</ul>
</div>
<div class="section" id="correlacio-i-covarianca">
<h3><span class="section-number">2.7.12. </span>Correlació i covariança<a class="headerlink" href="#correlacio-i-covarianca" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Fins ara hem vist una caracterització dicotòmica de les relacions entre variables aleatòries: <strong>independents</strong> o <strong>no independents</strong></p></li>
<li><p>Quan les v.a.’s no són independents, sovint és útil caracteritzar-ne el grau d’associació</p></li>
</ul>
<p>La covariança i la correlació ens serveixen per quantificar el grau d’associació <strong>linear</strong>
entre dues v.a.:</p>
<p class="note">[Casella &amp; Berger 4.5.1, 4.5.2] Considerem <span class="math notranslate nohighlight">\(X, Y\)</span> tals que
<span class="math notranslate nohighlight">\(E(X)=\mu_X\)</span>, <span class="math notranslate nohighlight">\(E(Y)=\mu_Y\)</span>, <span class="math notranslate nohighlight">\(\mbox{Var}(X)=\sigma^2_X\)</span>,
<span class="math notranslate nohighlight">\(\mbox{Var}(Y)=\sigma^2_Y\)</span>. Definim la covariança com <span class="math notranslate nohighlight">\(\mbox{Cov}(X,Y) = E((X - \mu_X)(Y - \mu_Y))\)</span>
i la correlació com <span class="math notranslate nohighlight">\(\rho_{X,Y} = \frac{\mbox{Cov}(X,Y)}{\sigma_X \sigma_Y}\)</span></p>
<p><em>Exercici</em>: Demostrar que <span class="math notranslate nohighlight">\(-1 \leq \rho_{X,Y} \leq 1\)</span></p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/covariance.png"><img alt="../_images/covariance.png" src="../_images/covariance.png" style="height: 500px;" /></a>
</div>
<div class="figure align-center" id="id8">
<a class="reference internal image-reference" href="../_images/simpson.png"><img alt="../_images/simpson.png" src="../_images/simpson.png" style="height: 450px;" /></a>
<p class="caption"><span class="caption-text">Exemple: <span class="math notranslate nohighlight">\(X\)</span> és la dosis d’un medicament i <span class="math notranslate nohighlight">\(Y\)</span> és la supervivència.
<span class="math notranslate nohighlight">\(W\)</span> podria ser per exemple, l’ètnia del pacient.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="esperanca-de-funcions-v-a-s-independents">
<h3><span class="section-number">2.7.13. </span>Esperança de funcions v.a.’s independents<a class="headerlink" href="#esperanca-de-funcions-v-a-s-independents" title="Permalink to this headline">¶</a></h3>
<p>L’última propietat que estudiarem abans d’avançar al Tema 2 és la relació entre
v.a.’s independents i l’esperança:</p>
<p class="note">[Casella &amp; Berger 4.2.10] Considereu <span class="math notranslate nohighlight">\(g(X)\)</span> i <span class="math notranslate nohighlight">\(h(Y)\)</span> per dues funcions
i v.a.’s arbitràries. Aleshores, si <span class="math notranslate nohighlight">\(X, Y\)</span> són independents, <span class="math notranslate nohighlight">\(E(g(X)h(Y)) = E(g(X))E(h(Y))\)</span></p>
<p><em>Demostració</em>: Aplicació immediata de la definició d’independència de v.a.’s.</p>
<p>Aquest resultat, com tots els relacionats amb l’esperança sembla trivial però
té conseqüencies importants:</p>
<ol class="arabic simple">
<li><p>Si <span class="math notranslate nohighlight">\(X, Y\)</span> són independents, aleshores <span class="math notranslate nohighlight">\(\mbox{Cov}(X,Y)=\rho_{X,Y} = 0\)</span> (Pregunta: creieu que el recíproc és cert?)</p></li>
<li><p>La funció generatriu de moments d’una suma de variables aleatòries independents és la multiplicació de f.g.m’s</p></li>
</ol>
</div>
<div class="section" id="questionari-repas">
<h3><span class="section-number">2.7.14. </span>Qüestionari repàs<a class="headerlink" href="#questionari-repas" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>Si X, Y son tals que <span class="math notranslate nohighlight">\(\mbox{cov}(X, Y) = 0\)</span>, quines de les següents afirmacions són certes: (a) X, Y són incorrelades, (b) X, Y no tenen perquè ser independents, (c) si X, Y son gaussianes, aleshores són independents, (d) Totes les anteriors son certes.</p></li>
<li><p>Tenim una f.d.p conjunta <span class="math notranslate nohighlight">\(f(x,y,z) = \frac{1}{K} e^{- (x + y + z)}\)</span> amb <span class="math notranslate nohighlight">\(K\)</span> t.q. <span class="math notranslate nohighlight">\(\int \int \int f(x,y,z) = 1\)</span>. Podem dir que X, Y, Z són mutualment independents?</p></li>
<li><p>Volem predir una v.a. Y en funció d’X. Quin és el predictor que en minimitza l’error quadràtic mitjà? Com el podem estimar?</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(X\)</span> és tal que <span class="math notranslate nohighlight">\(\mbox{var}(X)=\sigma^2\)</span> i <span class="math notranslate nohighlight">\(Y = a + b X\)</span>, quina és <span class="math notranslate nohighlight">\(\mbox{var}(Y)\)</span>?</p></li>
<li><p>Quina és la marginal per <span class="math notranslate nohighlight">\(X\)</span> de la conjunta <span class="math notranslate nohighlight">\(f(x,y)=\left\{\begin{array}{cc}\lambda^2 e^{- \lambda y} &amp; o \leq x \leq y \\ 0 &amp; \mbox{altrament} \end{array}\right.\)</span></p></li>
</ol>
</div>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">2. Tema 1: Repàs de probabilitat</a><ul>
<li><a class="reference internal" href="#espais-i-mesures-de-probabilitat">2.1. Espais i mesures de Probabilitat</a><ul>
<li><a class="reference internal" href="#espai-de-probabilitat">2.1.1. Espai de Probabilitat</a></li>
<li><a class="reference internal" href="#mesura-de-probabilitat">2.1.2. Mesura de probabilitat</a></li>
<li><a class="reference internal" href="#algunes-propietats-de-les-mesures-de-probabilitat">2.1.3. Algunes propietats de les mesures de probabilitat</a></li>
<li><a class="reference internal" href="#exemples-d-espais-de-probabilitat">2.1.4. Exemples d’espais de probabilitat</a></li>
<li><a class="reference internal" href="#questionari-de-repas">2.1.5. Qüestionari de repàs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#independencia-i-probabilitat-condicional">2.2. Independència i probabilitat condicional</a><ul>
<li><a class="reference internal" href="#probabilitat-condicional">2.2.1. Probabilitat condicional</a></li>
<li><a class="reference internal" href="#esdeveniments-independents">2.2.2. Esdeveniments independents</a></li>
</ul>
</li>
<li><a class="reference internal" href="#variables-aleatories-i-funcions-de-distribucio">2.3. Variables aleatòries i funcions de distribució</a><ul>
<li><a class="reference internal" href="#variable-aleatoria">2.3.1. Variable aleatòria</a></li>
<li><a class="reference internal" href="#funcio-de-distribucio">2.3.2. Funció de distribució</a></li>
<li><a class="reference internal" href="#funcio-de-massa-o-densitat-de-probabilitat">2.3.3. Funció de massa o densitat de probabilitat</a></li>
<li><a class="reference internal" href="#exemple-funcio-de-distribucio-i-massa-d-una-v-a-geometrica">2.3.4. Exemple: funció de distribució i massa d’una v.a. geomètrica</a></li>
<li><a class="reference internal" href="#altres-v-a-discretes">2.3.5. Altres v.a. discretes</a></li>
<li><a class="reference internal" href="#incis-sobre-les-v-a-continues">2.3.6. Incís sobre les v.a. contínues</a></li>
<li><a class="reference internal" href="#la-distribucio-uniforme">2.3.7. La distribució uniforme</a></li>
<li><a class="reference internal" href="#la-familia-gamma">2.3.8. La família Gamma</a></li>
<li><a class="reference internal" href="#la-familia-normal">2.3.9. La família “Normal”</a></li>
<li><a class="reference internal" href="#i-que-te-a-veure-tot-aixo-amb-l-estadistica">2.3.10. I què te a veure tot això amb l’estadística?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#funcions-de-variables-aleatories">2.4. Funcions de variables aleatòries</a><ul>
<li><a class="reference internal" href="#transformacions-afins">2.4.1. Transformacions afins</a></li>
<li><a class="reference internal" href="#cas-generic">2.4.2. Cas genèric</a></li>
<li><a class="reference internal" href="#f-d-c-i-transformacions-monotones">2.4.3. F.d.c i transformacions monòtones</a></li>
<li><a class="reference internal" href="#transformacions-monotones-i-diferenciables">2.4.4. Transformacions monòtones i diferenciables</a></li>
<li><a class="reference internal" href="#transformacio-integral">2.4.5. Transformació integral</a></li>
<li><a class="reference internal" href="#id1">2.4.6. Qüestionari de repàs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#esperanca-i-moments">2.5. Esperança i moments</a><ul>
<li><a class="reference internal" href="#esperanca">2.5.1. Esperança</a></li>
<li><a class="reference internal" href="#moments-i-moments-centrals">2.5.2. Moments i moments centrals</a></li>
<li><a class="reference internal" href="#l-esperanca-com-a-predictor-de-minim-error-quadrat">2.5.3. L’esperança com a predictor de mínim error quadrat</a></li>
<li><a class="reference internal" href="#funcio-generatriu-de-moments">2.5.4. Funció generatriu de moments</a></li>
</ul>
</li>
<li><a class="reference internal" href="#desigualtats">2.6. Desigualtats</a><ul>
<li><a class="reference internal" href="#desigualtat-de-markov">2.6.1. Desigualtat de Markov</a></li>
<li><a class="reference internal" href="#desigualtat-de-txebitxev">2.6.2. Desigualtat de Txebitxev</a></li>
<li><a class="reference internal" href="#desigualtat-de-jensen">2.6.3. Desigualtat de Jensen</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vectors-aleatoris-variables-multivariades">2.7. Vectors aleatoris / Variables multivariades</a><ul>
<li><a class="reference internal" href="#variables-multivariades">2.7.1. Variables multivariades</a></li>
<li><a class="reference internal" href="#funcio-de-densitat-o-de-massa-de-probabilitat-conjuntes-cas-bivariat">2.7.2. Funció de densitat o de massa de probabilitat conjuntes: Cas bivariat</a></li>
<li><a class="reference internal" href="#distribucions-marginals">2.7.3. Distribucions marginals</a></li>
<li><a class="reference internal" href="#exemple-de-variable-multivariada-discreta-multinomial">2.7.4. Exemple de variable multivariada discreta: multinomial</a></li>
<li><a class="reference internal" href="#funcio-de-distribucio-cumulativa-conjunta">2.7.5. Funció de distribució cumulativa conjunta</a></li>
<li><a class="reference internal" href="#funcio-de-distribucio-de-probabilitat-i-massa-condicionals">2.7.6. Funció de distribució de probabilitat i massa condicionals</a></li>
<li><a class="reference internal" href="#llei-de-la-probabilitat-total">2.7.7. Llei de la probabilitat total</a></li>
<li><a class="reference internal" href="#exemple-de-model-jerarquic-poisson-binomial">2.7.8. Exemple de model jeràrquic: Poisson-Binomial</a></li>
<li><a class="reference internal" href="#esperanca-condicional">2.7.9. Esperança condicional</a></li>
<li><a class="reference internal" href="#esperanca-condicional-i-prediccio">2.7.10. Esperança condicional i predicció</a></li>
<li><a class="reference internal" href="#variables-aleatories-independents">2.7.11. Variables aleatòries independents</a></li>
<li><a class="reference internal" href="#correlacio-i-covarianca">2.7.12. Correlació i covariança</a></li>
<li><a class="reference internal" href="#esperanca-de-funcions-v-a-s-independents">2.7.13. Esperança de funcions v.a.’s independents</a></li>
<li><a class="reference internal" href="#questionari-repas">2.7.14. Qüestionari repàs</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="0_0_Intro_curs.html"
                        title="previous chapter"><span class="section-number">1. </span>Tema 0: Intro al curs</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="0_2_Intro_stats.html"
                        title="next chapter"><span class="section-number">3. </span>Tema 2: Introducció a l’inferència estadística</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="0_2_Intro_stats.html" title="3. Tema 2: Introducció a l’inferència estadística"
             >next</a> |</li>
        <li class="right" >
          <a href="0_0_Intro_curs.html" title="1. Tema 0: Intro al curs"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">104392 - Modelització i Inferència 2020.08.04 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">2. </span>Tema 1: Repàs de probabilitat</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Arnau Tibau Puig.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.2.1.
    </div>
  </body>
</html>