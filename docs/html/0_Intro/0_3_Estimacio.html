
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Tema 3: Estimació &#8212; 104392 - Modelització i Inferència 2020.08.04 documentation</title>
    <link rel="stylesheet" href="../_static/sab-book.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="3. Tema 2: Introducció a l’inferència estadística" href="0_2_Intro_stats.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="0_2_Intro_stats.html" title="3. Tema 2: Introducció a l’inferència estadística"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">104392 - Modelització i Inferència 2020.08.04 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4. </span>Tema 3: Estimació</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="tema-3-estimacio">
<h1><span class="section-number">4. </span>Tema 3: Estimació<a class="headerlink" href="#tema-3-estimacio" title="Permalink to this headline">¶</a></h1>
<div class="section" id="estimacio-per-maxima-versemblanca">
<h2><span class="section-number">4.1. </span>Estimació per Màxima Versemblança<a class="headerlink" href="#estimacio-per-maxima-versemblanca" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ajust-de-distribucions-de-probabilitat">
<h3><span class="section-number">4.1.1. </span>Ajust de distribucions de probabilitat<a class="headerlink" href="#ajust-de-distribucions-de-probabilitat" title="Permalink to this headline">¶</a></h3>
<p>A l’exemple de <a class="reference external" href="https://e-aules.uab.cat/2020-21/pluginfile.php/695686/mod_page/content/2/motivacio_tema_3.pdf">la primera classe del Tema 3</a>
vem veure un exemple de modelització estadística on
partim d’un conjunt de dades que podem modelar com
una mostra i.i.d. d’una població:</p>
<p><span class="math notranslate nohighlight">\(X_i \sim f_X(x;\theta), i=1,\cdots,N\)</span></p>
<p>on:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_X\)</span> és la f.d.p. d’una família de distribucions i</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> són els paràmetres de la mateixa (vector o escalar), també anomenats <em>paràmetres de la població</em>.</p></li>
</ul>
<p class="note">El problema d’<em>estimació de paràmetres</em> o d’<em>ajust de la distribució</em> a partir de les dades
consisteix en estimar <span class="math notranslate nohighlight">\(\theta\)</span> a partir de <span class="math notranslate nohighlight">\(X_1, \cdots, X_N\)</span>.</p>
<p>Normalment se segueix la següent recepta:</p>
<ol class="arabic simple">
<li><p>Explorar dades (estadístics descriptius, gràfics)</p></li>
<li><p>Hipòtesi: Escollir familia (paramètrica) <span class="math notranslate nohighlight">\(f_X(x;\theta)\)</span></p></li>
<li><p>Àjustar paràmetre <span class="math notranslate nohighlight">\(\theta\)</span> segons algun criteri</p></li>
<li><p>Comprovar l’hipòtesi: <strong>bondat d’ajust</strong> (Tema 4)</p></li>
</ol>
<p class="note">A la darrera classe vem veure un exemple d’aplicació d’aquesta recepta
on el criteri d’ajust era <strong>heurístic</strong>: visualitzar l’histograma conjuntament
amb <span class="math notranslate nohighlight">\(f_X(x;\theta)\)</span> per diversos valors de <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/heuristic_fit.png"><img alt="../_images/heuristic_fit.png" src="../_images/heuristic_fit.png" style="height: 350px;" /></a>
</div>
<p class="note">Aquest procés heurístic de prova-i-error és (1) ineficient i (2) difícil de justificar
quantitativament</p>
<p>Un altre mètode
d’estimació molt versàtil i millor fonamentat
és el de <strong>Màxima Versemblança</strong>.</p>
<p>Per començar, definim la <em>log-versemblança</em>:</p>
<div class="math notranslate nohighlight">
\[L(\theta; x_1, \cdots, x_N) = \log \left(f_{X_1, \cdots, X_N}(x_1, \cdots, x_N; \theta) \right)\]</div>
<p>on <span class="math notranslate nohighlight">\(f_{X_1, \cdots, X_N}(x_1, \cdots, x_N; \theta)\)</span> és la
f.d.p. conjunta de la mostra.</p>
<p>En el cas d’una mostra iid, la log-versemblança es simplifica a:</p>
<div class="math notranslate nohighlight">
\[L(\theta; x_1, \cdots, x_N) = \sum_{i=1}^N \log f_X(x_i;\theta)\]</div>
<p class="note">Per mostres de v.a. discretes, la log-versemblança es calcula a partir de la f.m.p. conjunta <span class="math notranslate nohighlight">\(p_{X_1, \cdots, X_N}(x_1, \cdots, x_N; \theta)\)</span>
enlloc de la f.d.c.</p>
<p>Algunes propietats de la log-versemblança:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L(\theta; x_1, \cdots, x_N)\)</span> és un funció <span class="math notranslate nohighlight">\(\Theta \times \mathbb{R}^N \to \mathbb{R}\)</span>.</p></li>
<li><p>Donada una mostra en particular <span class="math notranslate nohighlight">\(X_1=x_1, \cdots, X_N=x_n\)</span>, <span class="math notranslate nohighlight">\(L(\theta; x_1, \cdots, x_N)\)</span> és un funció de <span class="math notranslate nohighlight">\(\theta \in \Theta \to \mathbb{R}\)</span>.</p></li>
<li><p>Com que la mostra és una v.a., <span class="math notranslate nohighlight">\(L(\theta; X_1, \cdots, X_N)\)</span> és una v.a. per cada valor de <span class="math notranslate nohighlight">\(\theta\)</span>!</p></li>
<li><p>Com vem veure a la primera classe, aquesta funció es pot interpretar com un criteri de qualitat de <span class="math notranslate nohighlight">\(\theta\)</span> a l’hora d’<em>explicar</em> les dades observades (quan més gran, millor explicades).</p></li>
</ul>
<p>Per tant, sembla raonable definir un estimador del(s)
paràmetre(s) <span class="math notranslate nohighlight">\(\theta\)</span> com:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \arg \max L(\theta; x_1, \cdots, x_N)\]</div>
<p>Aquest és el que s’anomena <strong>Estimador de Màxima Versemblança</strong> (EMV) o MLE per les
seves sigles en anglès.</p>
</div>
<div class="section" id="calcul-de-l-emv">
<h3><span class="section-number">4.1.2. </span>Càlcul de l’EMV<a class="headerlink" href="#calcul-de-l-emv" title="Permalink to this headline">¶</a></h3>
<p>En alguns casos, l’EMV es pot calcular
analíticament, resolent el problema d’optimització associat.
Per exemple en una mostra d’una família Gaussiana,</p>
<div class="math notranslate nohighlight">
\[L(\mu, \sigma; x_1, \cdots, x_N) = - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2} - N \log(\sqrt{2 \pi} \sigma)\]</div>
<p>Aquesta funció és diferenciable i concava en <span class="math notranslate nohighlight">\(\mu\)</span> i  <span class="math notranslate nohighlight">\(\sigma\)</span>,
per tant el seu màxim existeix i haurà de verificar la <strong>condició d’optimalitat</strong>:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mu, \sigma} L(\mu, \sigma; x_1, \cdots, x_N) = 0\]</div>
<p>Això ens porta a un sistema d’equacions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}- \sum_i \frac{x_i - \mu}{\sigma^2} &amp;= 0 \\
\sum_i \frac{(x_i - \mu)^2}{\sigma^3} - \frac{N}{\sigma} = 0\end{split}\]</div>
<p>d’on podem concloure que l’EMV és:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\mu} &amp;= \bar{x} \\
\hat{\sigma} &amp;= \frac{1}{N}\sum_i (x_i - \bar{x})^2\end{split}\]</div>
<p>(noteu que <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> no és igual que <span class="math notranslate nohighlight">\(S_X^2\)</span>!)</p>
<p>És important tenir en compte que:</p>
<ol class="arabic simple">
<li><p>No sempre podrem calcular els EMV de manera analítica.</p></li>
<li><p>En alguns casos ho podrem calcular numèricament (fent servir el mètode de descens del gradient, o de Newton)</p></li>
<li><p>En alguns casos, l’EMV no serà únic (i.e. la log-versemblança tindrà més d’un màxim)</p></li>
<li><p>En molts casos la log-versemblança no serà concava, o diferenciable, per tant l’EMV pot ser computacionalment molt difícil de calcular</p></li>
</ol>
<p class="note">Malgrat aquestes limitacions, el mètdode de la Màxima Versemblança
ens proporciona un mètode bastant genèric per trobar estimadors.</p>
</div>
<div class="section" id="exemple-emv-d-una-multinomial">
<h3><span class="section-number">4.1.3. </span>Exemple: EMV d’una multinomial<a class="headerlink" href="#exemple-emv-d-una-multinomial" title="Permalink to this headline">¶</a></h3>
<p>Continuem amb un parell més d’exemples d’aplicació del EMV.</p>
<p>Un model molt útil en estadística és el model <em>multinomial</em>, que
s’utilitza quan tenim observacions tabulades, per exemple: un histograma,
el nombre de respostes d’un qüestionari per edat de l’entrevistat,
l’incidència d’una malaltia per regió geogràfica, etc.</p>
<p>En tots aquests casos, es poden resumir les N observacions d’una mostra en un
vector <span class="math notranslate nohighlight">\(X_1, \cdots, X_M\)</span> on <span class="math notranslate nohighlight">\(X_i\)</span> es correspon amb el nombre d’observacions dins la casella <span class="math notranslate nohighlight">\(i\)</span>,
i hi ha M caselles i <span class="math notranslate nohighlight">\(\sum_i X_i = N\)</span>.</p>
<p><strong>Important</strong>: Noteu que en aquest cas <span class="math notranslate nohighlight">\(X_i\)</span> no és iid!</p>
<p>El model <em>multinomial</em> suposa que la f.d.m. conjunta de <span class="math notranslate nohighlight">\(X_1, \cdots, X_M\)</span> vé donada per:</p>
<div class="math notranslate nohighlight">
\[p(x_1, \cdots, x_m; p_1, \cdots, p_M) = \frac{N!}{\Pi_i {x_i!}}\Pi_i p_i^{x_i}\]</div>
<p>on <span class="math notranslate nohighlight">\(p_1, \cdots, p_M\)</span> són els paràmetres de la població, tals que <span class="math notranslate nohighlight">\(\sum_i p_i = 1\)</span>,
i per construcció <span class="math notranslate nohighlight">\(\sum_i X_i = N\)</span>.</p>
<p>A partir d’aquesta f.d.m conjunta, i una mostra <span class="math notranslate nohighlight">\(X_1=x_1, \cdots, X_M=x_m\)</span>
podem calcular la log-versemblança:</p>
<div class="math notranslate nohighlight">
\[L( p_1, \cdots, p_M; x_1, \cdots, x_m) \propto - \sum_i \log (x_i!) + \sum_i x_i \log p_i\]</div>
<p>(on ignorem els termes que no depènen d’<span class="math notranslate nohighlight">\(x_i\)</span> o <span class="math notranslate nohighlight">\(p_i\)</span>.)</p>
<p>Com que sabem que <span class="math notranslate nohighlight">\(\sum_i p_i = 1\)</span> , podem imposar la restricció que <span class="math notranslate nohighlight">\(p_M = 1 - \sum_{i=1}^{M-1} p_i\)</span>,
i tindrem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L( p_1, \cdots, p_{M-1}; x_1, \cdots, x_m) \propto&amp;  - \sum_i \log (x_i!) + \sum_{i=1}^{M-1} x_i \log p_i \\
    &amp; + (N - \sum_{i=1}^{M-1} x_i ) \log (1 - \sum_{i=1}^{M-1} p_i)\end{split}\]</div>
<p>Calculant-ne el gradient i igualant-lo a 0 (exercici per casa), podrem concloure que l’EMV d’una multinomial és simplement:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_i = \frac{x_i}{N}\]</div>
<p class="note">En els exemples que hem vist fins ara (Gaussiana, Multinomial, Poisson..), excepte el model de precipitació a través d’una Gamma,
l’EMV es correspon amb l’estimador que hauriem escollit sense saber la teoria de Màxima Versemblança…
Val la pena complicar-nos la vida amb aquesta teoria!?</p>
</div>
<div class="section" id="exemple-emv-amb-dades-censurades">
<h3><span class="section-number">4.1.4. </span>Exemple: EMV amb dades censurades<a class="headerlink" href="#exemple-emv-amb-dades-censurades" title="Permalink to this headline">¶</a></h3>
<p>L’EMV és una metodologia molt més potent del que hem vist fins ara. D’na banda, com veurem tot seguit,
ens permet estimar paràmetres en casos on l’intuició potser ens fallaria. Per altra banda, com veurem
més endavant, els EMVs té unes propietats estadístiques interessants.</p>
<p>Considerem l’exemple següent: Estem interessats en modelar
la supervivència d’uns pacients sota un tractament mèdic determinat.
Considerem <span class="math notranslate nohighlight">\(X_i\)</span> l’edat en anys de defunció del pacient <span class="math notranslate nohighlight">\(i\)</span>.
Durant la durada del nostre estudi, alguns dels pacients moriran
però alguns altres seguiran vius. Per tant, per aquests últims
pacients l’únic que sabrem és que <span class="math notranslate nohighlight">\(X_i \geq e_i\)</span> on
<span class="math notranslate nohighlight">\(e_i\)</span> és l’edat del pacient en el moment d’acabar l’estudi.</p>
<p>Per tant necessitem modelar la versemblança d’una mostra
mixta d’observacions <span class="math notranslate nohighlight">\(X_i\)</span> i esdeveniments <span class="math notranslate nohighlight">\(X_j \geq e_j\)</span>,
on aquestes últimes s’anomenen “observacions censurades” (com
si algú ens hagués “censurat” les dades, en aquest cas l’univers).</p>
<p>Anomenem <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> el subconjunt
de pacients morts (i que per tant dels que hem pogut observar-ne l’edat de defunció)
i <span class="math notranslate nohighlight">\(\bar{\mathcal{M}}\)</span> el subconjunt de pacients vius (dels que només sabem q
que <span class="math notranslate nohighlight">\(X_i \geq e_i\)</span>).</p>
<p>La funció de log-versemblança
que utilitzarem en aquest cas és:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \log P\left(\left( \cap_{i: \mathcal{M}}{ X_i=x_i} \right) \cap \left(\cap_{i: \bar{\mathcal{M}}}{ X_i \geq e_i}\right); \theta \right)\]</div>
<p>Si la mostra és iid, això es simplificarà a:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \sum_{i: \mathcal{M}}\log p_X(x_i; \theta) + \sum_{i: \bar{\mathcal{M}}} \log(1 -  F_X(e_i; \theta))\]</div>
<p>on <span class="math notranslate nohighlight">\(p_X(x_i; \theta)\)</span> és la f.m.p. del nostre model d’edat de defunció i <span class="math notranslate nohighlight">\(F_X(e_i; \theta)\)</span> n’és la f.d.c. corresponent.</p>
<p>Suposem que modelem l’edat de defunció dels pacients segons una llei geomètrica:</p>
<div class="math notranslate nohighlight">
\[p_X(x; \rho) = (1 - \rho)^{x - 1} \rho\]</div>
<p>on <span class="math notranslate nohighlight">\(\rho \in [0, 1]\)</span> és el paràmetre de la població.
La f.d.c. és <span class="math notranslate nohighlight">\(F_x(x ;\rho) = 1 - (1 -\rho)^x\)</span>
i per tant podem calcular la log-versemblança com:</p>
<div class="math notranslate nohighlight">
\[L(\rho) = \left|\mathcal{M}\right| \log \rho + \sum_{i: \mathcal{M}}(x_i -1) \log(1 -\rho) + \sum_{i: \bar{\mathcal{M}}}e_i \log(1 -  \rho)\]</div>
<p><strong>Exercici</strong>: Acabar de calcular <span class="math notranslate nohighlight">\(\hat{\rho} = \arg \max  L(\rho)\)</span></p>
</div>
</div>
<div class="section" id="propietats-asimptotiques-de-l-emv">
<h2><span class="section-number">4.2. </span>Propietats asimptòtiques de l’EMV<a class="headerlink" href="#propietats-asimptotiques-de-l-emv" title="Permalink to this headline">¶</a></h2>
<div class="section" id="biaix-varianca-eqm">
<h3><span class="section-number">4.2.1. </span>Biaix, Variança, EQM…<a class="headerlink" href="#biaix-varianca-eqm" title="Permalink to this headline">¶</a></h3>
<p>Recordem que la log-versemblança <span class="math notranslate nohighlight">\(L(\theta; X_1, \cdots, X_N)\)</span>
és una v.a. per cada <span class="math notranslate nohighlight">\(\theta\)</span> (i.e. una “funció aleatòria”)
i per tant l’EMV <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> també és una v.a.! Per caracteritzar-lo
haurem de fer servir les eines que vem desenvolupar al Tema 2:</p>
<ul class="simple">
<li><p><strong>Biaix</strong>: <span class="math notranslate nohighlight">\(b(\hat{\theta}) := E(\hat{\theta} - \theta_0)\)</span></p></li>
<li><p><strong>Variança</strong>: <span class="math notranslate nohighlight">\(\mbox{Var}(\hat{\theta}) = E((\hat{\theta} - E(\hat{\theta}))^2)\)</span></p></li>
<li><p><strong>Error Quadràtic Mitjà</strong>: <span class="math notranslate nohighlight">\(\mbox{MSE}(\hat{\theta}) = E((\hat{\theta} - \theta_0)^2)\)</span></p></li>
<li><p><strong>La seva f.d.p.</strong>: <span class="math notranslate nohighlight">\(f_{\hat{\theta}}(x)\)</span></p></li>
</ul>
<p class="note"><strong>IMPORTANT</strong>: Tot el que segueix <strong>suposa</strong> una mostra i.i.d. generada segons un
model <span class="math notranslate nohighlight">\(X_i \sim f_X(x;\theta_0); i=1,\cdots,N\)</span>, on <span class="math notranslate nohighlight">\(\theta_0\)</span>
és el valor <strong>real però desconegut</strong> del paràmetre a estimar. Per tant totes les esperances
que tractem són relatives a aquesta <span class="math notranslate nohighlight">\(f_X(x;\theta_0)\)</span>!</p>
<p>En general l’EMV no tindrà una forma analítica que es presti a
calcular-ne el biaix, variança o MSE, i molt menys a
caracteritzar-ne la distribució.</p>
<p>La gran avantatge dels EMV és que, asimptòticament,
es poden caracteritzar relativament fàcilment. Primer definim
què és el que volem dir per “asimptòtic”. Explicitant la dependència
de l’EMV amb el tamany de la mostra:</p>
<p><span class="math notranslate nohighlight">\(\hat{\theta}^N = \arg \max  L(\theta; X_1, \cdots, X_N)\)</span></p>
<p>el que ens interessarà és caracteritzar el biaix, variança i
f.d.p de <span class="math notranslate nohighlight">\(\hat{\theta}^N\)</span> a mesura que <span class="math notranslate nohighlight">\(N \to \infty\)</span></p>
<p>Començarem aquesta caracterització amb el següent resultat,
que provarem de manera informal:</p>
<p class="note"><strong>Teorema 3.1</strong>: Donada una mostra iid, i per <span class="math notranslate nohighlight">\(f(x; \theta)\)</span> prou “suaus”,
<span class="math notranslate nohighlight">\(\frac{1}{N}L(\theta; X_1, \cdots, X_N) = \frac{1}{N}\sum_i \log f(X_i; \theta)\)</span>
convergeix en probabilitat a <span class="math notranslate nohighlight">\(E(\log f_X(x; \theta))\)</span>.</p>
<p><em>“Demostració”</em>: Resulta de l’aplicació de la <a class="reference external" href="https://atibaup.github.io/ModInfer_2020/slides/0_Intro/0_2_Intro_stats.html#25">LLei Feble dels Grans Nombres</a>
que vem veure al Tema 2. Per tant només hauriem de comprovar que podem aplicar-la, és a dir que:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E(\log f(X_i; \theta))\)</span> existeix i que</p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{Var}(\log f(X_i; \theta))\)</span> és finita</p></li>
</ul>
<p>que dóna lloc a la condició “prou “suaus”” de la proposició.</p>
</div>
<div class="section" id="consistencia">
<h3><span class="section-number">4.2.2. </span>Consistència<a class="headerlink" href="#consistencia" title="Permalink to this headline">¶</a></h3>
<p><strong>Definició</strong>: Un estimador és consistent si, a mesura
que el tamany de la mostra augmenta, l’estimador
convergeix en probabilitat al paràmetre d’interès:</p>
<div class="math notranslate nohighlight">
\[\lim_{N\to \infty} P(|\hat{\theta}^N - \theta_0|&gt;\epsilon) = 0\]</div>
<p class="note"><strong>Teorema 3.2</strong>: Sota algunes condicions de regularitat per <span class="math notranslate nohighlight">\(f_X(x; \theta)\)</span>,
l’EMV és un estimador consistent.</p>
<p><em>“Demostració”</em>: Pel <strong>Teorema 3.1</strong> hem vist que
<span class="math notranslate nohighlight">\(\frac{1}{N}L(\theta; X_1, \cdots, X_N)  \to E(\log f_X(x; \theta))\)</span>
en probabilitat. No podrem demostrar-ho en aquest curs,
però sembla raonable esperar que, sota algunes condicions,
el <span class="math notranslate nohighlight">\(\hat{\theta}^N\)</span> que maximitza l’expressió de l’esquerra
hauria de maximitzar l’expressió de la dreta i viceversa.</p>
<p>Sota aquest supòsit, només ens cal verificar que <span class="math notranslate nohighlight">\(\theta_0\)</span> maximitza
<span class="math notranslate nohighlight">\(E(\log f_X(x; \theta))\)</span> per concloure que <span class="math notranslate nohighlight">\(\hat{\theta}^N \to \theta_0\)</span>.
Fem-ho:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial \theta} E(\log f_X(x; \theta)) &amp; = \frac{\partial}{\partial \theta} \int_x  \log f(x; \theta) f(x; \theta_0) dx \\
&amp; =  \int_x  \frac{\partial}{\partial \theta}\log f(x; \theta) f(x; \theta_0) dx \\
&amp; = \int_x  \frac{1}{f(x; \theta)} \frac{\partial}{\partial \theta} f(x; \theta) f(x; \theta_0) dx\end{split}\]</div>
<p class="note">Estem cometent bastants sacrilegis intercanviant l’ordre dels operadors
integrals i diferencials sense donar explicacions… però ens haurem de
creure que és possible per la majoria de <span class="math notranslate nohighlight">\(f_X\)</span> d’interès.</p>
<p>Noteu que per <span class="math notranslate nohighlight">\(\theta = \theta_0\)</span>, aquesta última expressió resulta:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\int_x  \frac{1}{f(x; \theta_0)} \left.\frac{\partial}{\partial \theta} f(x; \theta) \right|_{\theta=\theta_0} f(x; \theta_0) dx &amp; =  \int_x  \left. \frac{\partial}{\partial \theta} f(x; \theta_0) \right|_{\theta=\theta_0} dx\\
&amp; =  \frac{\partial}{\partial \theta} \int_x   f(x; \theta_0)dx \\
&amp; = 0\end{split}\]</div>
<p>per tant <span class="math notranslate nohighlight">\(\theta = \theta_0\)</span> és tal que <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \theta} E(\log f_X(x; \theta))=0\)</span>
i si <span class="math notranslate nohighlight">\(E(\log f_X(x; \theta))\)</span> és concava, n’és un màxim. Amb això
podem “concloure” que <span class="math notranslate nohighlight">\(\hat{\theta}^N \to \theta_0\)</span>.</p>
</div>
<div class="section" id="distribucio-asimptotica-de-l-emv">
<h3><span class="section-number">4.2.3. </span>Distribució asimptòtica de l’EMV<a class="headerlink" href="#distribucio-asimptotica-de-l-emv" title="Permalink to this headline">¶</a></h3>
<p>Per ara hem vist que l’EMV té una propietat bona, la consistència: quan el tamany
de la mostra augmenta, l’estimador convergeix al valor del
paràmetre de la població.</p>
<p>La caracterització asimptòtica de l’EMV no s’acaba aquí: de fet,
tot seguit veurem que <strong>la distribució de l’EMV és Gaussiana,
centrada en el paràmetre d’interès</strong> <span class="math notranslate nohighlight">\(\theta_0\)</span> <strong>(asimptòticament
sense biaix!) i amb una variança que decreix amb N</strong>.</p>
<p class="note"><strong>Teorema 3.3</strong>: Sota algunes condicions de regularitat de
<span class="math notranslate nohighlight">\(f_X\)</span>, <span class="math notranslate nohighlight">\(\sqrt{N {I}(\theta_0)}(\hat{\theta}^N - \theta_0) \Rightarrow \mathcal{N}(0, 1)\)</span>, on
<span class="math notranslate nohighlight">\({I}(\theta) = - E\left(\frac{\partial^2}{\partial \theta^2}\log f(X; \theta) \right)\)</span>
és la <strong>Informació de Fisher</strong>.</p>
<p>Abans de donar un esboç de la prova d’aquest resultat, mirem d’entendre’l.
Aquest resultat implica:</p>
<ol class="arabic simple">
<li><p>L’EMV és <strong>asimptòticament sense biaix</strong>, <span class="math notranslate nohighlight">\(\lim_{N \to \infty} E(\hat{\theta}^N - \theta_0) =0\)</span>.</p></li>
<li><p>La seva <strong>variança asimptòtica</strong> és inversament proporcional a N, i per tant l’EMV és <strong>consistent</strong></p></li>
<li><p>Al límit, i independentment de la distribució de la mostra, <strong>l’EMV es comporta com una Gaussiana!</strong></p></li>
<li><p>La <strong>variança asimptòtica</strong> depèn d’aquesta quantitat un pèl esotèrica <span class="math notranslate nohighlight">\({I}(\theta)\)</span>…</p></li>
</ol>
<p>Ara comprovarem computacionalment el resultat per un cas en particular,
quan <span class="math notranslate nohighlight">\(X \sim \mbox{Poisson}(\lambda_0)\)</span>. Tenim que</p>
<div class="math notranslate nohighlight">
\[\log f_X(x;\lambda) = x\log \lambda - \lambda - \log x!\]</div>
<p>i per tant:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2}{\partial \theta^2} \log f_X(x;\lambda) = -\frac{x}{\lambda^2}\]</div>
<p>aleshores: <span class="math notranslate nohighlight">\({I}(\lambda)= - E\left(-\frac{X}{\lambda^2} \right)=\frac{1}{\lambda}\)</span>.</p>
<p>Per altra banda, l’EMV d’una mostra
iid de Poisson és simplement el moment mostral (Exercici!):</p>
<p><span class="math notranslate nohighlight">\(\hat{\lambda}^N = \bar{x}\)</span></p>
<p>Per tant, asimptòticament: <span class="math notranslate nohighlight">\(\hat{\lambda}^N \sim \mathcal{N}(\theta_0, \frac{\lambda_0}{N})\)</span></p>
<div class="highlight-R notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="m">100</span> <span class="c1"># Tamany de cada mostra</span>
<span class="n">n</span> <span class="o">=</span> <span class="m">1000</span> <span class="c1"># Nombre de repeticions</span>
<span class="n">lambda</span> <span class="o">=</span> <span class="m">15</span> <span class="c1"># Paràmetre de la població</span>

<span class="n">emv_poisson</span> <span class="o">&lt;-</span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">n</span><span class="p">){</span>
  <span class="n">sample</span> <span class="o">=</span> <span class="nf">rpois</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
  <span class="n">emv_poisson[i]</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">x</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">emv_poisson</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">emv_poisson</span><span class="p">),</span> <span class="m">0.1</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">emv_poisson</span><span class="p">,</span> <span class="m">20</span><span class="p">,</span> <span class="n">freq</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nf">dnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda</span><span class="p">,</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">lambda</span><span class="o">/</span><span class="n">N</span><span class="p">)),</span> <span class="n">col</span><span class="o">=</span><span class="s">&#39;green&#39;</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../_images/ex_asimptotic.png"><img alt="../_images/ex_asimptotic.png" class="align-center" src="../_images/ex_asimptotic.png" style="height: 250px;" /></a>
</div>
<div class="section" id="justificacio-de-la-distribucio-asimptotica-de-l-emv">
<h3><span class="section-number">4.2.4. </span>Justificació de la distribució asimptòtica de l’EMV<a class="headerlink" href="#justificacio-de-la-distribucio-asimptotica-de-l-emv" title="Permalink to this headline">¶</a></h3>
<p>Ara procedirem a justificar el Teorema 3.3, sense arribar a provar-lo, cosa
que requeriria tècniques molt més avançades que les d’aquest curs.</p>
<p>Primer de tot, alleugerirem la notació establint, com ja hem fet servir
en exemples anteriors:</p>
<p><span class="math notranslate nohighlight">\(L(\theta) := L( \theta; x_1, \cdots, x_m)\)</span></p>
<p>Imaginem-nos una <span class="math notranslate nohighlight">\(L(\theta)\)</span> “simpàtica” al voltant de <span class="math notranslate nohighlight">\(\theta_0\)</span>:</p>
<a class="reference internal image-reference" href="../_images/likelihood_ex.png"><img alt="../_images/likelihood_ex.png" class="align-center" src="../_images/likelihood_ex.png" style="height: 300px;" /></a>
<p>Recordeu de càlcul que l’expansió de Taylor de segon ordre d’una funció “suau”
<span class="math notranslate nohighlight">\(f(x)\)</span> al voltant d’un punt <span class="math notranslate nohighlight">\(x_0\)</span> és:</p>
<div class="math notranslate nohighlight">
\[f(x) \approx f(x_0) + (x - x_0) f'(x_0)\]</div>
<p>Aleshores suposant que <span class="math notranslate nohighlight">\(L'(\theta)\)</span> és “suau”, tindrem que:</p>
<div class="math notranslate nohighlight">
\[L'(\theta) \approx L'(\theta_0) + (x - x_0) L''(\theta_0)\]</div>
<p>i per tant, per <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> que maximitza <span class="math notranslate nohighlight">\(L(\theta)\)</span>,
haurà de verificar la condició d’optimalitat:</p>
<div class="math notranslate nohighlight">
\[0 = L'(\hat{\theta}) \approx L'(\theta_0) + (\hat{\theta} - \theta_0) L''(\theta_0)\]</div>
<p>Cosa que ens permet concloure que:</p>
<div class="math notranslate nohighlight">
\[(\theta - \theta_0) \approx \frac{L'(\theta_0)}{L''(\theta_0)}\]</div>
<p>Gràficament:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/likelihood_ex_2.png"><img alt="../_images/likelihood_ex_2.png" src="../_images/likelihood_ex_2.png" style="height: 500px;" /></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/likelihood_ex_3.png"><img alt="../_images/likelihood_ex_3.png" src="../_images/likelihood_ex_3.png" style="height: 500px;" /></a>
</div>
<p>Abans de continuar, farem un petit escalat de la darrera equació:</p>
<div class="math notranslate nohighlight">
\[\sqrt{N}(\theta - \theta_0) \approx \frac{\frac{1}{\sqrt{N}} L'(\theta_0)}{ \frac{1}{N} L''(\theta_0)}\]</div>
<p>Ara, fixem-nos que en el cas iid, el numerador:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\sqrt{N}} L'(\theta_0) =\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \frac{\partial}{\partial \theta} \log f_X(x_i;\theta_0)\]</div>
<p>és una suma de v.a. iid (<span class="math notranslate nohighlight">\(\log f_X(x_i;\theta_0)\)</span>) amb mitja 0, per la raó que hem vist
en el Teorema 3.1, i variança:</p>
<div class="math notranslate nohighlight">
\[\mbox{Var}(\frac{1}{\sqrt{N}} L'(\theta_0)) =E( \frac{\partial}{\partial \theta} \log f_X(X;\theta_0))^2\]</div>
<p>per propietats de la variança de la suma de v.a. iid.</p>
<p>Per continuar, necessitarem un resultat auxiliar:</p>
<p class="note">Lema 3.4: Sota algunes condicions de regularitat de
<span class="math notranslate nohighlight">\(f_X\)</span>, <span class="math notranslate nohighlight">\(E( \frac{\partial}{\partial \theta} \log f_X(X;\theta))^2 = - E( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)) = I(\theta)\)</span></p>
<p><em>Justificació</em>: Com que <span class="math notranslate nohighlight">\(\int f_X(x;\theta)dx = 1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}0 &amp; = \frac{\partial}{\partial \theta} \int f_X(x;\theta)dx = \int  \frac{\partial}{\partial \theta} f_X(x;\theta) dx\\
0 &amp; = \int  (\frac{\partial}{\partial \theta} \log f_X(X;\theta))  f_X(x;\theta) dx = \frac{\partial}{\partial \theta} \int  (\frac{\partial}{\partial \theta} \log f_X(X;\theta))  f_X(x;\theta) dx \\
0 &amp; = \int  (\frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta))  f_X(x;\theta)dx + \\
  &amp; + \int (\frac{\partial}{\partial \theta} \log f_X(X;\theta))^2 f_X(x;\theta)dx\end{split}\]</div>
<p>Combinant aquest últim resultat amb l’aplicació del TLC, podem concloure que</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\sqrt{N}} L'(\theta_0) \Rightarrow \mathcal{N}(0, I(\theta_0))\]</div>
<p>Per altra banda, pel denominador tenim:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} L''(\theta) = \frac{1}{N} \sum_i \frac{\partial^2}{\partial \theta^2} \log f_X(x_i;\theta)\]</div>
<p>que, per la Llei dels Grans Nombres:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} L''(\theta) \to E(\frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)) = -I(\theta)\]</div>
<p>Combinant aquests dos resultats, veiem que</p>
<div class="math notranslate nohighlight">
\[\frac{\frac{1}{\sqrt{N}} L'(\theta_0)}{ \frac{1}{N} L''(\theta_0)} \Rightarrow  \mathcal{N}(0, I^{-1}(\theta_0))\]</div>
<p>que és el resultat que buscàvem justificar.</p>
<p><strong>Interpretació de la Informació de Fisher</strong> (<span class="math notranslate nohighlight">\(I(\theta)\)</span>):</p>
<ul class="simple">
<li><p>Fixeu-vos que  <span class="math notranslate nohighlight">\(L''(\theta_0)\)</span> és asimptòticament proporcional a <span class="math notranslate nohighlight">\(I(\theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(L''(\theta_0)\)</span> mesura la corvatura de <span class="math notranslate nohighlight">\(L(\theta)\)</span> al voltant de <span class="math notranslate nohighlight">\(\theta_0\)</span></p></li>
<li><p>Quan més corbatura, menys variança, quan més “plana” més variança.</p></li>
<li><p>=&gt; La “forma” de la nostra distribució determina la variança asimptòtica de l’estimador</p></li>
</ul>
</div>
<div class="section" id="eficiencia-i-cota-de-cramer-rao">
<h3><span class="section-number">4.2.5. </span>Eficiència i Cota de Cramer-Rao<a class="headerlink" href="#eficiencia-i-cota-de-cramer-rao" title="Permalink to this headline">¶</a></h3>
<p>L’últim concepte teòric que considerarem en aquest tema
és el de l’eficiència.</p>
<p>En aquest curs no ho hem vist, però hi ha altres metodologies
per obtenir estimadors com el <a class="reference external" href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">Mètode dels Moments</a>,
els estimadors de <a class="reference external" href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">James-Stein</a>,
o els <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes_estimator">estimadors Bayesians</a>.</p>
<p>Per tant, per un mateix paràmetre, podem trobar-nos amb diferents
“receptes” per construir-ne un estimador.</p>
<p class="note">La pregunta que ens ocupa és: Donats diferents estimadors d’un mateix paràmetre, com n’escollim el millor?</p>
<p>La resposta a aquesta pregunta és: “depèn”.</p>
<p>Dependrà del que volguem
fer a posteriori amb aquest estimador, però un criteri bastant acceptat
és el de comparar-los segons el seu Error Quadràtic Mitjà (denominat MSE per les sigles en anglès), que
com sabem es pot descomposar com la suma de la Variança de l’Estimador
i del quadrat del seu biaix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mbox{MSE}(\hat{\theta}) &amp; = E(\hat{\theta}^N - \theta_0)^2 \\
                         &amp; = \mbox{Var}(\hat{\theta}) + b(\hat{\theta})^2\end{split}\]</div>
<p>Si ens restringim a estimadors sense biaix, comparar-ne l’MSE és
equivalent a comparar-ne les variànces, cosa que dona lloc a la definició
d’eficiència:</p>
<p class="note">Un estimador sense biaix és <strong>eficient</strong> si té menys o igual variança que
qualsevol altre estimador</p>
<p>Aquesta definició no és massa constructiva: per trobar l’estimador eficient,
hauriem de construïr tots els estimadors possibles (infinits!), calcular-ne
la variança, i finalment escollir el que en té menys.</p>
<p>Per sort, un dels resultats més importants de l’estadística,
desenvolupat als anys 40 del s. XX per Harald Crámer i C.R. Rao,
ens diu:</p>
<p class="note"><strong>Teorema 3.5</strong>: Sigui <span class="math notranslate nohighlight">\(X_1, \cdots, X_N\)</span> una mostra iid
amb <span class="math notranslate nohighlight">\(X \sim f_X(x;\theta_0)\)</span> i <span class="math notranslate nohighlight">\(\tilde{\theta}\)</span> un estimador
sense biaix de <span class="math notranslate nohighlight">\(\theta_0\)</span>. Aleshores, sota certes condicions
de regularitat de <span class="math notranslate nohighlight">\(f_X(x;\theta)\)</span>,
<span class="math notranslate nohighlight">\(\mbox{Var}(\tilde{\theta}) \geq \frac{1}{N I(\theta_0)}\)</span>.</p>
<p>Fixeu-vos-hi que per tant, <strong>l’EMV és asimptòticament eficient</strong>
ja que <span class="math notranslate nohighlight">\(\mbox{Var}(\hat{\theta}) \to \frac{1}{N I(\theta_0)}\)</span>.</p>
</div>
</div>
<div class="section" id="intervals-de-confianca-per-emvs">
<h2><span class="section-number">4.3. </span>Intervals de confiança per EMVs<a class="headerlink" href="#intervals-de-confianca-per-emvs" title="Permalink to this headline">¶</a></h2>
<div class="section" id="intervals-de-confianca">
<h3><span class="section-number">4.3.1. </span>Intervals de confiança<a class="headerlink" href="#intervals-de-confianca" title="Permalink to this headline">¶</a></h3>
<p>Durant el curs ja hem treballat diverses vegades amb Intervals de Confiança,
que vam introduïr per primer cop a la Pràctica #2.</p>
<p>Un <strong>interval de confiança</strong> de nivell <span class="math notranslate nohighlight">\(1-\alpha\)</span> (a la Pràctica #2
parlàvem de nivell <span class="math notranslate nohighlight">\(\alpha\)</span> enlloc de <span class="math notranslate nohighlight">\(1-\alpha\)</span> però aquesta darrera convenció
és més comuna) per un paràmetre <span class="math notranslate nohighlight">\(\mu\)</span> és un estadístic
(per tant una v.a. que és una funció de la mostra)
format per dos nombres <span class="math notranslate nohighlight">\(L\)</span> i <span class="math notranslate nohighlight">\(U\)</span> tals que:</p>
<div class="math notranslate nohighlight">
\[P([L, U] \ni \mu) = 1 - \alpha\]</div>
<p class="note">És important entendre que la quantitat aleatòria aquí és el conjunt <span class="math notranslate nohighlight">\([L, U]\)</span>
i no <span class="math notranslate nohighlight">\(\mu\)</span>. L’interpretació d’aquesta probabilitat és que, si agaféssim M mostres
(cada una de tamany N) i calculéssim M intervals (un per cada mostra), hauriem
d’esperar que una fracció <span class="math notranslate nohighlight">\(1 - \alpha\)</span> dels mateixos contenen <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</div>
<div class="section" id="intervals-exactes">
<h3><span class="section-number">4.3.2. </span>Intervals exactes<a class="headerlink" href="#intervals-exactes" title="Permalink to this headline">¶</a></h3>
<p>En alguns casos específics, podrem calcular I.C’s de manera “exacta”,
és a dir, calculant la f.d.p. <span class="math notranslate nohighlight">\(f_{\hat{\theta}}\)</span> de l’estimador
del paràmetre d’interès, i utilitzant-la per calcular un I.C.</p>
<p>Aquest és el cas, per exemple, de l’EMV de la mitja i variança d’una mostra Gaussiana,
on, com vam veure a la <a class="reference external" href="https://e-aules.uab.cat/2020-21/mod/assign/view.php?id=178383">Pràctica 2</a>:</p>
<div class="math notranslate nohighlight">
\[\frac{\sqrt{N}(\hat{\mu} - \mu)}{\hat{\sigma}} \sim t_{N-1}\]</div>
<p>ja que l’EMV per la mitja i variança Gaussiana és
<span class="math notranslate nohighlight">\(\hat{\mu}=\bar{x}\)</span> i <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{N-1}{N}S_X^2\)</span>.
Per altra banda, <a class="reference external" href="https://atibaup.github.io/ModInfer_2020/slides/0_Intro/0_2_Intro_stats.html#29">un dels resultats que vam veure al Tema 2</a> és que:</p>
<div class="math notranslate nohighlight">
\[\frac{(N-1)S_X^2}{\sigma^2} \sim \chi_{N-1}^2\]</div>
<p>Podem fer servir aquests dos resultats per
calcular IC’s per <span class="math notranslate nohighlight">\(\hat{\mu}, \hat{\sigma}\)</span> com segueix:</p>
<p>1) Com que la distribució <span class="math notranslate nohighlight">\(t_{N-1}\)</span> és simètrica
al voltant de 0, i denotant per <span class="math notranslate nohighlight">\(\phi_t(x)\)</span>
la seva f.d.c. inversa, tindrem que:</p>
<div class="math notranslate nohighlight">
\[P(-\phi_t(\frac{\alpha}{2}) \leq \frac{\sqrt{N}(\hat{\mu} - \mu)}{\hat{\sigma}} \leq \phi_t(\frac{\alpha}{2}))\]</div>
<p>d’aquí podem concloure que l’interval de confiança de nivell <span class="math notranslate nohighlight">\(1 - \alpha\)</span>
per <span class="math notranslate nohighlight">\(\mu\)</span> és:</p>
<div class="math notranslate nohighlight">
\[\hat{\mu} \pm \phi_t(\frac{\alpha}{2})\sqrt{\frac{\hat{\sigma}^2}{N}}\]</div>
<p class="note">Noteu la similitud i les diferències respecte l’interval de confiança que obtindriem pel TLC.</p>
<ol class="arabic simple" start="2">
<li><p>Per altra banda com que</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\frac{N \hat{\sigma}^2}{\sigma^2} = \frac{(N-1)S_X^2}{\sigma} \sim \chi_{N-1}^2\]</div>
<p>i denotant per <span class="math notranslate nohighlight">\(\phi_{\chi_{N-1}^2}(x)\)</span> la f.d.c. inversa
d’una <span class="math notranslate nohighlight">\(\chi_{N-1}^2\)</span>, tindrem:</p>
<div class="math notranslate nohighlight">
\[P\left(\phi_{\chi_{N-1}^2}(1 - \frac{\alpha}{2}) \leq \frac{N \hat{\sigma}}{\sigma} \leq \phi_{\chi_{N-1}^2}(\frac{\alpha}{2})\right) = 1- \alpha\]</div>
<p>i per tant, l’IC de nivell <span class="math notranslate nohighlight">\(1-\alpha\)</span>:</p>
<div class="math notranslate nohighlight">
\[\left[\frac{N \hat{\sigma}^2}{\phi_{\chi_{N-1}^2}(\frac{\alpha}{2})}, \frac{N \hat{\sigma}^2}{\phi_{\chi_{N-1}^2}(1 - \frac{\alpha}{2})}\right]\]</div>
<p>que com podeu observar no és simètric com en el cas anterior.</p>
</div>
<div class="section" id="intervals-aproximats-asimptotics">
<h3><span class="section-number">4.3.3. </span>Intervals aproximats asimptòtics<a class="headerlink" href="#intervals-aproximats-asimptotics" title="Permalink to this headline">¶</a></h3>
<p>En general serà difícil caracteritzar la f.d.p. dels nostres estimadors, i
per tant haurem de recórrer a aproximacions, com la que vam veure
en la teoria asimptòtica de l’EMV (Teorema 3.3 d’aquestes diapos):</p>
<div class="math notranslate nohighlight">
\[\sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \Rightarrow \mathcal{N}(0, 1)\]</div>
<p>on <span class="math notranslate nohighlight">\(I(\theta)\)</span> és
la Informació de Fisher. Si sapiguéssim el valor de <span class="math notranslate nohighlight">\(\theta_0\)</span>, podriem fer
servir el desenvolupament que ja hem fet servir múltiples vegades
per trobar un IC de nivell <span class="math notranslate nohighlight">\(1  - \alpha\)</span>. Per N suficientment gran:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P\left(\sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \leq \phi\left(1 - \frac{\alpha}{2}\right)\right) &amp;\approx 1 - \frac{\alpha}{2} \\
P\left(\sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \leq \phi\left(\frac{\alpha}{2}\right)\right) &amp; \approx \frac{\alpha}{2}\end{split}\]</div>
<p>on <span class="math notranslate nohighlight">\(\phi(x)\)</span> és la f.d.c. inversa (funció de quantils) d’una normal estàndard.</p>
<p>Per tant, sabent que <span class="math notranslate nohighlight">\(\phi\left(1 - \frac{\alpha}{2}\right)= -  \phi\left(\frac{\alpha}{2}\right)\)</span>,</p>
<div class="math notranslate nohighlight">
\[P\left( \phi\left(\frac{\alpha}{2}\right) \leq \sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \leq -\phi\left(\frac{\alpha}{2}\right)\right) \approx 1 - \alpha\]</div>
<p>cosa que justificaria el següent IC aproximat per l’EMV d’una mostra iid d’una
població <span class="math notranslate nohighlight">\(f_X(x;\theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} \pm \phi\left(\frac{\alpha}{2}\right) \sqrt{\frac{1}{N I(\theta_0)}}\]</div>
<p>on <span class="math notranslate nohighlight">\(\phi\)</span> és la f.d.c. inversa d’una Normal estàndard i <span class="math notranslate nohighlight">\(I(\theta)\)</span> és
la Informació de Fisher.</p>
<p>Com que no coneixem <span class="math notranslate nohighlight">\(\theta_0\)</span>, aquesta última expressió és inútil ja que depèn
de <span class="math notranslate nohighlight">\(I(\theta_0)\)</span>.</p>
<p>Farem doncs servir el <strong>principi de substitució</strong> (plug-in principle),
que ja hem fet servir altres vegades, i substituïrem <span class="math notranslate nohighlight">\(I(\theta_0)\)</span> per <span class="math notranslate nohighlight">\(I(\hat{\theta})\)</span>,
sota el precepte de que per N suficientment gran <span class="math notranslate nohighlight">\(\hat{\theta} \to \theta_0\)</span>.</p>
<p>Per tant arribem a:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} \pm \phi\left(\frac{\alpha}{2}\right) \sqrt{\frac{1}{N I(\hat{\theta})}}\]</div>
<p class="note">Fixeu-vos en la similitud/diferències entre aquesta expressió i els Intervals de Confiança obtinguts
en el cas on podíem derivar intervals de confiança exactes.</p>
</div>
<div class="section" id="unes-notes-sobre-el-calcul-de-i-hat-theta">
<h3><span class="section-number">4.3.4. </span>Unes notes sobre el càlcul de <span class="math notranslate nohighlight">\(I(\hat{\theta})\)</span><a class="headerlink" href="#unes-notes-sobre-el-calcul-de-i-hat-theta" title="Permalink to this headline">¶</a></h3>
<p>A l’última entrega de problemes, he vist que hi havia una mica de confusió
respecte el càlcul de la Informació de Fisher <span class="math notranslate nohighlight">\(I(\theta)\)</span>.</p>
<p>En molts casos, podrem calcular <span class="math notranslate nohighlight">\(I(\theta)\)</span> analíticament,
a través d’una de les seves dues definicions alternatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(\theta) &amp;= E( \frac{\partial}{\partial \theta} \log f_X(X;\theta))^2 \\
I(\theta) &amp;= - E( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta))\end{split}\]</div>
<p>(Podem escollir la que ens vagi millor per al problema.) Fixeu-vos que la v.a.
aleatòria de la que volem calcular l’esperança aquí és una funció de
<span class="math notranslate nohighlight">\(\log f_X(X;\theta)\)</span> (la seva derivada al quadrat o la seva  segona derivada), que a la vegada és una funció de <span class="math notranslate nohighlight">\(X\)</span>.
L’esperança, per tant, es calcula respecte a <span class="math notranslate nohighlight">\(X \sim f_X\)</span>.</p>
<p>En alguns casos, calcular aquesta esperança serà massa difícil (com
per exemple a l’Exercici 2 de l’entrega de Problemes), i haurem
de calcular-la de manera aproximada. És aquí on podem recórrer de nou a la <a class="reference external" href="https://atibaup.github.io/ModInfer_2020/slides/0_Intro/0_2_Intro_stats.html#25">Llei dels Grans Nombres</a>,
que ens diu que per qualsevol funció <span class="math notranslate nohighlight">\(g(x)\)</span> d’una variable aleatòria
<span class="math notranslate nohighlight">\(Y\)</span>, i sota algunes condicions de regularitat,</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N}\sum_i g(Y_i) \to E(g(Y))\]</div>
<p>quan <span class="math notranslate nohighlight">\(N \to \infty\)</span>. Per tant, si tenim una mostra gran
de <span class="math notranslate nohighlight">\(X \sim f_X\)</span> (o si la podem simular), podrem aproximar:</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(\theta) &amp;\approx \frac{1}{N}\sum_i \left(\frac{\partial}{\partial \theta} \log f_X(X_i;\theta)\right)^2 \\
I(\theta) &amp;\approx - \frac{1}{N}\sum_i \left( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)\right)\end{split}\]</div>
<p>Fixeu-vos que aquesta última expressió dóna lloc a l’aproximació
que veu trobar alguns per Internet:</p>
<div class="math notranslate nohighlight">
\[I(\theta) \approx - \frac{1}{N}\sum_i \left( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)\right) = \frac{1}{N}\frac{\partial^2}{\partial \theta^2} L(\theta)\]</div>
<p>quan la mostra és iid, on simplement hem aplicat la definició de <span class="math notranslate nohighlight">\(L(\theta)\)</span> i
intercanviat l’ordre de l’operador suma i derivada.</p>
</div>
<div class="section" id="intervals-aproximats-per-bootstrap">
<h3><span class="section-number">4.3.5. </span>Intervals aproximats per Bootstrap<a class="headerlink" href="#intervals-aproximats-per-bootstrap" title="Permalink to this headline">¶</a></h3>
<p>L’última tècnica que veurem per calcular Intervals de Confiança
és la més potent, ja que no requereix cap suposició
pel que fa a la distribució de la mostra, i funciona en el règim no-asimptòtic.</p>
<p>Fixeu-vos que en el desenvolupament anterior ens hem basat (implícitament)
en que coneixiem la distribució de:</p>
<div class="math notranslate nohighlight">
\[\Lambda = \hat{\theta} - \theta_0 \sim f_{\Lambda}(x;\theta_0)\]</div>
<p>Si coneixem <span class="math notranslate nohighlight">\(f_{\Lambda}\)</span>, vol dir que també coneixem la seva f.d.c.
inversa, <span class="math notranslate nohighlight">\(\phi_{\Lambda}\)</span> i per tant podem trobar Intervals
de Confiança ja que:</p>
<div class="math notranslate nohighlight">
\[P\left( \phi_{\Lambda}\left(\frac{\alpha}{2}\right)\leq \hat{\theta} - \theta_0 \leq \phi_{\Lambda}\left(1 - \frac{\alpha}{2}\right)\right) = 1 - \alpha\]</div>
<p>En el cas asimptòtic que hem vist abans, teniem que <span class="math notranslate nohighlight">\(\Lambda \approx \mathcal{N}\left(0, \frac{1}{N I(\hat{\theta})}\right)\)</span>.</p>
<p>En general, no coneixem <span class="math notranslate nohighlight">\(f_{\Lambda}(x;\theta_0)\)</span> i per tant
ens quedem encallats.</p>
<p>El mètode de Bootstrap paramètric, el que proposa és:</p>
<ol class="arabic simple">
<li><p>Generar una mostra Bootstrap de talla M: <span class="math notranslate nohighlight">\(\Lambda_i \sim f_{\Lambda}(x;\hat{\theta}), i=1, \cdots, M\)</span> (fixeu-vos que hem remplaçat <span class="math notranslate nohighlight">\(\theta_0\)</span> per <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>)</p></li>
<li><p>Estimar <span class="math notranslate nohighlight">\(\phi_{\Lambda}\left(\frac{\alpha}{2}\right)\)</span> i <span class="math notranslate nohighlight">\(\phi_{\Lambda}\left(1 - \frac{\alpha}{2}\right)\)</span> a partir dels quantils de la mostra <span class="math notranslate nohighlight">\(\Lambda_1, \cdots, \Lambda_M\)</span></p></li>
</ol>
<p>per trobar un interval de confiança aproximat:</p>
<div class="math notranslate nohighlight">
\[P\left( \hat{\phi}_{\Lambda}\left(\frac{\alpha}{2}\right)\leq \hat{\theta} - \theta_0 \leq \hat{\phi}_{\Lambda}\left(1 - \frac{\alpha}{2}\right)\right) \approx 1 - \alpha\]</div>
<p class="note">Ens queda aclarir com generar <span class="math notranslate nohighlight">\(\Lambda_i \sim f_{\Lambda}(x;\hat{\theta})\)</span>…</p>
<p>Per generar <span class="math notranslate nohighlight">\(\Lambda \sim f_{\Lambda}(x;\hat{\theta})\)</span>, ho farem
de manera indirecta, ja que com hem dit en general no coneixem
<span class="math notranslate nohighlight">\(f_{\Lambda}\)</span>. El que sí que coneixem, en principi, és <span class="math notranslate nohighlight">\(f_X(x;\theta)\)</span>,
la f.d.p. o f.m.p de la nostra mostra.</p>
<p>Per tant generarem <span class="math notranslate nohighlight">\(\Lambda\)</span> indirectament. Per fer-ho, repetirem <span class="math notranslate nohighlight">\(M\)</span> vegades
el següent procés:</p>
<p>1. Generar <strong>una</strong> mostra bootstrap de <span class="math notranslate nohighlight">\(X \sim f_X(x;\hat{\theta})\)</span> de talla N, segons el valor
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span> que hem trobat a partir de les nostres dades.</p>
<ol class="arabic simple" start="2">
<li><p>Calcularem l’EMV sobre aquesta mostra bootstrap, i l’anomenarem <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span></p></li>
<li><p>Calcularem <span class="math notranslate nohighlight">\(\Lambda = \hat{\theta}^* - \hat{\theta}\)</span></p></li>
</ol>
<p>Al final d’aquest procés haurem obtingut la mostra Bootstrap de <span class="math notranslate nohighlight">\(\Lambda\)</span> i
podrem calcular-ne els quantils, per calcular l’IC aproximat que en vist en l’anterior diapo.</p>
</div>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">4. Tema 3: Estimació</a><ul>
<li><a class="reference internal" href="#estimacio-per-maxima-versemblanca">4.1. Estimació per Màxima Versemblança</a><ul>
<li><a class="reference internal" href="#ajust-de-distribucions-de-probabilitat">4.1.1. Ajust de distribucions de probabilitat</a></li>
<li><a class="reference internal" href="#calcul-de-l-emv">4.1.2. Càlcul de l’EMV</a></li>
<li><a class="reference internal" href="#exemple-emv-d-una-multinomial">4.1.3. Exemple: EMV d’una multinomial</a></li>
<li><a class="reference internal" href="#exemple-emv-amb-dades-censurades">4.1.4. Exemple: EMV amb dades censurades</a></li>
</ul>
</li>
<li><a class="reference internal" href="#propietats-asimptotiques-de-l-emv">4.2. Propietats asimptòtiques de l’EMV</a><ul>
<li><a class="reference internal" href="#biaix-varianca-eqm">4.2.1. Biaix, Variança, EQM…</a></li>
<li><a class="reference internal" href="#consistencia">4.2.2. Consistència</a></li>
<li><a class="reference internal" href="#distribucio-asimptotica-de-l-emv">4.2.3. Distribució asimptòtica de l’EMV</a></li>
<li><a class="reference internal" href="#justificacio-de-la-distribucio-asimptotica-de-l-emv">4.2.4. Justificació de la distribució asimptòtica de l’EMV</a></li>
<li><a class="reference internal" href="#eficiencia-i-cota-de-cramer-rao">4.2.5. Eficiència i Cota de Cramer-Rao</a></li>
</ul>
</li>
<li><a class="reference internal" href="#intervals-de-confianca-per-emvs">4.3. Intervals de confiança per EMVs</a><ul>
<li><a class="reference internal" href="#intervals-de-confianca">4.3.1. Intervals de confiança</a></li>
<li><a class="reference internal" href="#intervals-exactes">4.3.2. Intervals exactes</a></li>
<li><a class="reference internal" href="#intervals-aproximats-asimptotics">4.3.3. Intervals aproximats asimptòtics</a></li>
<li><a class="reference internal" href="#unes-notes-sobre-el-calcul-de-i-hat-theta">4.3.4. Unes notes sobre el càlcul de <span class="math notranslate nohighlight">\(I(\hat{\theta})\)</span></a></li>
<li><a class="reference internal" href="#intervals-aproximats-per-bootstrap">4.3.5. Intervals aproximats per Bootstrap</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="0_2_Intro_stats.html"
                        title="previous chapter"><span class="section-number">3. </span>Tema 2: Introducció a l’inferència estadística</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="0_2_Intro_stats.html" title="3. Tema 2: Introducció a l’inferència estadística"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">104392 - Modelització i Inferència 2020.08.04 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4. </span>Tema 3: Estimació</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Arnau Tibau Puig.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.2.1.
    </div>
  </body>
</html>