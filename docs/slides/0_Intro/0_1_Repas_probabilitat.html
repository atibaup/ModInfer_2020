<!DOCTYPE html>


<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2. Tema 1: Repàs de probabilitat &mdash; 104392 - Modelització i Inferència 2020.08.04 documentation</title>
    
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/styles.css" type="text/css" />
    <link rel="stylesheet" href="../_static/slides.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <link rel="stylesheet" href="../_static/../../html/_static/sab-slides.css" type="text/css" />
    
    
    
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2020.08.04',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/common.js"></script>
    
    <script type="text/javascript" src="../_static/slides.js"></script>
    <script type="text/javascript" src="../_static/sync.js"></script>
    <script type="text/javascript" src="../_static/controller.js"></script>
    <script type="text/javascript" src="../_static/init.js"></script>
    
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="top" title="104392 - Modelització i Inferència 2020.08.04 documentation" href="../index.html" />
    <link rel="prev" title="1. Tema 0: Intro al curs" href="0_0_Intro_curs.html" /> 
  </head>
  <body>

<section
   id="slide_container"
   class='slides layout-regular'>


  
<article class="slide level-1" id="tema-1-repas-de-probabilitat">

<h1>Tema 1: Repàs de probabilitat</h1>




<div class="slide-no">1</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="espais-i-mesures-de-probabilitat">

<h2>Espais i mesures de Probabilitat</h2>




<div class="slide-no">2</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="espai-de-probabilitat">

<h3>Espai de Probabilitat</h3>

<p class="note">Durant el Tema 1 haurem d’anar una mica ràpid. És impossible fer un curs de probabilitat
en 2 setmanes, però per sort ja n’heu fet un!</p>
<p>Un <strong>espai de probabilitat</strong> és un model matemàtic del resultat d’un <strong>experiment aleatori</strong>.</p>
<p>Consisteix en un triplet <span class="math notranslate nohighlight">\(\left(\Omega, \mathcal{A}, P\right)\)</span>:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\Omega\)</span>: l’<strong>espai mostral</strong>, conjunt de resultats possibles d’un experiment</li>
<li><span class="math notranslate nohighlight">\(\mathcal{A} \subseteq 2^{\Omega}\)</span>: el conjunt d’<strong>esdeveniments</strong>, una família de subconjunts d’<span class="math notranslate nohighlight">\(\Omega\)</span></li>
<li><span class="math notranslate nohighlight">\(P\)</span>: una <strong>mesura de probabilitat</strong>, una funció <span class="math notranslate nohighlight">\(\mathcal{A} \rightarrow \left[0, 1\right]\)</span></li>
</ul>
<p><em>Recordatori</em>: <span class="math notranslate nohighlight">\(2^{\Omega}\)</span> és el conjunt de tots els sub-conjunts d’<span class="math notranslate nohighlight">\(\Omega\)</span>, incloent-hi <span class="math notranslate nohighlight">\(\emptyset\)</span> i <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>



<div class="slide-no">3</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="mesura-de-probabilitat">

<h3>Mesura de probabilitat</h3>

<p>Una <strong>mesura de probabilitat</strong> <span class="math notranslate nohighlight">\(P: \mathcal{A} \rightarrow \left[0, 1\right]\)</span>
ha de satisfer els següents axiomes (de Kolmogorov):</p>
<ol class="arabic simple">
<li><span class="math notranslate nohighlight">\(P\left(\Omega\right)=1\)</span></li>
<li><span class="math notranslate nohighlight">\(\forall A\in\mathcal{A}\)</span>, <span class="math notranslate nohighlight">\(P\left(A\right)\geq 0\)</span></li>
<li>Per <span class="math notranslate nohighlight">\(A_1,A_2,A_3, \cdots \in \mathcal{A}\)</span> disjunts, <span class="math notranslate nohighlight">\(P\left(\cup_i A_i\right) = \sum_i P\left(A_i\right)\)</span></li>
</ol>
<p class="note">Fixeu-vos que tenim llibertat a l’hora de definir <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> pels esdeveniments que ens
interessen (sempre i quan formin una <span class="math notranslate nohighlight">\(\sigma\)</span>-àlgebra.)</p>



<div class="slide-no">4</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id8">

<h3>Mesura de probabilitat (2)</h3>

<p>Això és una construcció axiomàtica de Probabilitat, formalitzada per Andrey Kolmogorov.</p>
<p>Noteu que no hem associat cap interpretació al significat físic dels valors de <span class="math notranslate nohighlight">\(P\)</span>. Dues interpretacions típiques:</p>
<ul class="simple">
<li><strong>Frequentista</strong>: <span class="math notranslate nohighlight">\(P\left(A\right)\)</span> representa la frequència amb que observariem l’esdeveniment <cite>A</cite> si realitzéssim un gran nombre d’experiments</li>
<li><strong>Bayesiana</strong>: <span class="math notranslate nohighlight">\(P\left(A\right)\)</span> representa la nostra certesa sobre l’ocurrència de l’esdeveniment <cite>A</cite></li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Les dues interpretacions no són completament ortogonals, però són l’orígen d’un munt de
discussions filosòfiques i a vegades dogmàtiques. Si us interessa el tema us recomano
<a class="reference external" href="https://projecteuclid.org/euclid.ba/1340370429">Objections to Bayesian statistics</a>.</p>
</div>



<div class="slide-no">5</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id9">

<h3>Aquest no és un curs de probabilitat…</h3>

<p>Aquest no és un curs de probabilitat, per tant amagarem “detalls” important sota l’alfombra:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\mathcal{A}\)</span> en realitat ha de ser una <span class="math notranslate nohighlight">\(\sigma\)</span>-àlgebra (conté <span class="math notranslate nohighlight">\(\emptyset\)</span>, tancat per unió contable i complement)</li>
<li>Per a conjunts <span class="math notranslate nohighlight">\(\Omega\)</span> contables, podem tirar milles considerant <span class="math notranslate nohighlight">\(\mathcal{A} = 2^{\Omega}\)</span></li>
<li>La cosa es complica quan <span class="math notranslate nohighlight">\(\Omega\)</span> no és discret (exemples: l’alçada d’una població, el nivell d’expressió d’un gen)</li>
</ul>
<p class="note"><strong>Recomano</strong> donar una ullada al [Casella &amp; Berger] o a una altra de les referències
bibliogràfiques per una intro no tècnica a les <span class="math notranslate nohighlight">\(\sigma\)</span>-àlgebres</p>



<div class="slide-no">6</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="algunes-propietats-de-les-mesures-de-probabilitat">

<h3>Algunes propietats de les mesures de probabilitat</h3>

<p class="note"><strong>Teorema [Casella &amp; Berger 1.2.8 i 1.2.9]</strong> Per una mesura de probabilitat <span class="math notranslate nohighlight">\(P\)</span> i
qualsevol esdeveniments <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span>, tenim:</p>
<ol class="note arabic simple">
<li><span class="math notranslate nohighlight">\(P\left(\emptyset\right)=0\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A\right) \leq 1\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A^c\right) = 1 - P\left(A\right)\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(B \cap A^c\right) = P\left(B\right) - P\left(A \cap B\right)\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A \cup B\right) = P\left(A\right) + P\left(B\right) - P\left(A \cap B\right)\)</span></li>
<li>Si <span class="math notranslate nohighlight">\(A \subseteq B\)</span>, aleshores <span class="math notranslate nohighlight">\(P\left(A\right) \leq P\left(B\right)\)</span></li>
</ol>
<p><strong>Demostració</strong>: Punts (1), (2), (3), exercici :) (recomano començar pel 3er punt).
Punts (4)-(6) tot seguit.</p>



<div class="slide-no">7</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id10">

<h3>Guia de la demostració punts (4)-(6)</h3>

<p>Pel punt (4), només cal observar que <span class="math notranslate nohighlight">\(B = \left(B \cap A\right) \cup \left(B \cap A^c\right)\)</span> (exercici).
D’aquesta identitat i tenint en compte que <span class="math notranslate nohighlight">\(B \cap A\)</span> i <span class="math notranslate nohighlight">\(B \cap A^c\)</span> son disjunts,
s’en dedueix l’expressió usant el 3er axioma de Kolmogorov.</p>
<p>Pel punt (5), utilitzem la següent identitat <span class="math notranslate nohighlight">\(A \cup B = A \cup \left(B \cap A^c\right)\)</span> i apliquem el punt (4).</p>
<p>Finalment el punt (6) el demostrem observant que si <span class="math notranslate nohighlight">\(A \subseteq B\)</span> aleshores <span class="math notranslate nohighlight">\(A \cap B = A\)</span>
i que <span class="math notranslate nohighlight">\(0 \leq P\left(B \cap A^c\right) = P\left(B\right) - P\left(A\right)\)</span>.</p>



<div class="slide-no">8</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id11">

<h3>Un parell més de resultats útils</h3>

<p>Els següents són propietats interessants relatives a col.leccions de conjunts:</p>
<p class="note"><strong>Teorema [Casella &amp; Berger 1.2.11]</strong> Si <span class="math notranslate nohighlight">\(P\)</span> és una mesura de probabilitat:</p>
<ol class="note arabic simple">
<li>Per cualsevol partició <span class="math notranslate nohighlight">\(C_1, \cdots, C_N\)</span> d’ <span class="math notranslate nohighlight">\(\Omega\)</span>, <span class="math notranslate nohighlight">\(P\left(A\right) = \sum_i P\left(A \cap C_i \right)\)</span></li>
<li><span class="math notranslate nohighlight">\(A_1, A_2 \cdots, \in \mathcal{A}\)</span>, <span class="math notranslate nohighlight">\(P\left(\cup_i A_i\right) \leq \sum_i P\left(A_i \right)\)</span> (desigualtat de Boole)</li>
</ol>
<p><strong>Demostració</strong>: (1) tot seguit, (2) exercici.</p>



<div class="slide-no">9</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id12">

<h3>Demostració punt (1) resultat anterior</h3>

<p>Demostració punt (1): Recordem que una partició <span class="math notranslate nohighlight">\(C_1, \cdots, C_N\)</span> d’ <span class="math notranslate nohighlight">\(\Omega\)</span>
és una col.lecció de conjunts tal que <span class="math notranslate nohighlight">\(\cup_i C_i = \Omega\)</span> i <span class="math notranslate nohighlight">\(C_i \cap C_j = \emptyset, \forall i\neq j\)</span>.</p>
<p>Tenim doncs la següent cadena d’identitats:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A &amp;= A \cap \Omega \\
A &amp; = A \cap \cup_i C_i \\
A &amp; = \cup_i \left( A \cap C_i \right)\\
P\left(A\right) &amp; = P\left(\cup_i \left( A \cap C_i\right)\right)\end{split}\]</div>
<p>i com que <span class="math notranslate nohighlight">\(A \cap C_i\)</span> i <span class="math notranslate nohighlight">\(A \cap C_j\)</span> son disjunts, el resultat
s’obté considerant el 3er axioma de Kolmogorov.</p>



<div class="slide-no">10</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="exemples-d-espais-de-probabilitat">

<h3>Exemples d’espais de probabilitat</h3>

<p><strong>Experiment 1</strong>: Modelar el resultat de llançar un dau de 6 cares</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\Omega = \left\{1, 2, 3, 4, 5, 6\right\}\)</span></li>
<li><span class="math notranslate nohighlight">\(\mathcal{A} = \left\{ \left\{1\right\}, \left\{2\right\}, \cdots, \left\{1, 2\right\}, \cdots, \emptyset, \Omega \right\}\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(x\right) = \frac{1}{6}, x \in \Omega\)</span></li>
</ul>
<p class="note"><strong>Exercici</strong>: Com definirieu <span class="math notranslate nohighlight">\(P\left(A\right)\)</span> per a qualsevol <span class="math notranslate nohighlight">\(A \in \mathcal{A}\)</span>?</p>
<ul class="build simple">
<li>Resposta: <span class="math notranslate nohighlight">\(P\left(A\right) = \sum_{x \in A} P\left(x\right)\)</span>. Podeu comprovar que aquesta construcció satisfà els axiomes.</li>
</ul>



<div class="slide-no">11</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id13">

<h3>Exemples d’espais de probabilitat (2)</h3>

<p><strong>Experiment 2</strong>: Escollir 100 persones i fer-els-hi una prova d’anticossos per SARS-COV-2</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\Omega = \left\{+, -\right\}^{100}\)</span></li>
<li><span class="math notranslate nohighlight">\(\mathcal{A} = ?\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A\right) = ?\)</span></li>
</ul>
<p><strong>Experiment 3</strong>: Escollir aleatòriament un estudiant d’aquesta classe i mesurar-ne la seva alçada</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\Omega = \left[0, \infty \right)\)</span></li>
<li><span class="math notranslate nohighlight">\(\mathcal{A} = ?\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A\right) = ?\)</span></li>
</ul>



<div class="slide-no">12</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="questionari-de-repas">

<h3>Qüestionari de repàs</h3>

<ol class="arabic simple">
<li>Un espai de probabilitat és el triplet d’un ______________, un ______________ i una _____________.</li>
<li>Quina dels següents assercions <strong>no</strong> és un axioma de Kolmogorov:</li>
</ol>
<ol class="loweralpha simple">
<li>Si <span class="math notranslate nohighlight">\(A \cap B = \emptyset\)</span>, <span class="math notranslate nohighlight">\(P\left(A \cup B \right) = P\left(A \right) + P\left( B \right)\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A\right) \leq 1, \forall A \in \mathcal{A}\)</span></li>
<li><span class="math notranslate nohighlight">\(P\left(A\right) \geq 0, \forall A \in \mathcal{A}\)</span></li>
</ol>
<ol class="arabic simple" start="3">
<li>Quin és l’<span class="math notranslate nohighlight">\(\Omega\)</span> i l’<span class="math notranslate nohighlight">\(\mathcal{A}\)</span> del següent experiment: <em>mesurar la vida útil en dies dels ordinadors Macbook Pro.</em></li>
<li>Quin és l’<span class="math notranslate nohighlight">\(\Omega\)</span> i l’<span class="math notranslate nohighlight">\(\mathcal{A}\)</span> de l’experiment: <em>llençar un dau fins que treiem un 6.</em></li>
</ol>



<div class="slide-no">13</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="independencia-i-probabilitat-condicional">

<h2>Independència i probabilitat condicional</h2>




<div class="slide-no">14</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="probabilitat-condicional">

<h3>Probabilitat condicional</h3>

<p>Donats <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span>, amb <span class="math notranslate nohighlight">\(P\left(B\right) &gt; 0\)</span>,
<span class="math notranslate nohighlight">\(P\left(A|B\right) = \frac{P\left(A \cap B\right)}{P\left(B\right)}\)</span> (aquesta construcció satisfà els axiomes de Kolmogorov)</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/proba_condicional.png"><img alt="../_images/proba_condicional.png" src="../_images/proba_condicional.png" style="height: 300px;" /></a>
</div>



<div class="slide-no">15</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id14">

<h3>Probabilitat condicional (2)</h3>

<p><span class="math notranslate nohighlight">\(P\left(\cdot|B\right)\)</span> és la restricció de <span class="math notranslate nohighlight">\(P\)</span> al subconjunt d’esdeveniments B. Alguns preguntes/petits exercicis interessants:</p>
<ol class="build arabic simple">
<li>Tindria sentit definir <span class="math notranslate nohighlight">\(P\left(A|B\right)\)</span> si <span class="math notranslate nohighlight">\(P\left(B\right) = 0\)</span>?</li>
<li>Si <span class="math notranslate nohighlight">\(A \cap B = \emptyset\)</span>, <span class="math notranslate nohighlight">\(P\left(A|B\right)\)</span>?</li>
<li>Com podem interpretar si <span class="math notranslate nohighlight">\(P\left(A|B\right) =P\left(A\right)\)</span>? Podeu donar un exemple “físic”?</li>
<li>Si <span class="math notranslate nohighlight">\(A \subseteq B\)</span>, quina relació hi ha entre <span class="math notranslate nohighlight">\(P\left(A|B\right)\)</span> i <span class="math notranslate nohighlight">\(P\left(A\right)\)</span>?</li>
</ol>



<div class="slide-no">16</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="esdeveniments-independents">

<h3>Esdeveniments independents</h3>

<p>Diem que <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span>, són independents si:</p>
<p><span class="math notranslate nohighlight">\(P\left(A \cap B\right) =P\left(A\right)P\left(B\right)\)</span></p>
<p>Això és equivalent a <span class="math notranslate nohighlight">\(P\left(A|B\right) =P\left(A\right)\)</span> si <span class="math notranslate nohighlight">\(P\left(B\right) &gt; 0\)</span>.</p>
<p>Algunes preguntes [Casella &amp; Berger Teorema 1.3.9] (mirem de respondre per intució primer i matemàticament després):</p>
<ol class="build arabic simple">
<li>Si <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span> son independents, què podem dir de <span class="math notranslate nohighlight">\(A, B^c\)</span>?</li>
<li>Si <span class="math notranslate nohighlight">\(A, B \in \mathcal{A}\)</span> son independents, què podem dir de <span class="math notranslate nohighlight">\(A^c, B^c\)</span>?</li>
</ol>



<div class="slide-no">17</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id15">

<h3>Precaució, estimat conductor</h3>

<p>Per exemple, l’independència conjunta no implica independència de parells:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/ex_independencia.png"><img alt="../_images/ex_independencia.png" src="../_images/ex_independencia.png" style="height: 300px;" /></a>
</div>
<p>Calculem <span class="math notranslate nohighlight">\(P\left(A \cap B \cap C\right)\)</span> i <span class="math notranslate nohighlight">\(P\left(B \cap C\right)\)</span>…</p>
<p><em>Nota:</em> l’independència de parells tampoc implica independència mútua (veure Problema)</p>



<div class="slide-no">18</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id16">

<h3>Independència mútua</h3>

<p>Per resoldre aquests problemes, fa falta una definició molt més estricta
de la noció d’independència en conjunts d’esdeveniments:</p>
<p class="note"><strong>Definició</strong> <span class="math notranslate nohighlight">\(A_1, A_2 \cdots, \in \mathcal{A}\)</span> són mutualment independents si per cualsevol
subcol.lecció <span class="math notranslate nohighlight">\(A_{i_1}, A_{i_2} \cdots, \in \mathcal{A}\)</span>, tenim que <span class="math notranslate nohighlight">\(P\left(\cap_j A_{i_j}\right) = \Pi_j P\left(A_{i_j}\right)\)</span></p>
<p>(En aquest curs, quan parlem de mostres independents, estarem assumint independència mútua)</p>



<div class="slide-no">19</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="variables-aleatories-i-funcions-de-distribucio">

<h2>Variables aleatòries i funcions de distribució</h2>




<div class="slide-no">20</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="variable-aleatoria">

<h3>Variable aleatòria</h3>

<p class="note"><strong>Definició</strong> Una variable aleatòria (<em>v.a.</em> pels amics) és una funció <span class="math notranslate nohighlight">\(X : \Omega \to \mathcal{X} \subseteq \mathbb{R}\)</span>.</p>
<p>Podem doncs definir una funció de probabilitat [Casella &amp; Berger 1.4.2]:</p>
<p><span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = P\left(\left\{s\in \Omega: X\left(s\right) \in A \right\}\right)\)</span></p>
<p>que satisfà els axiomes de Kolmogorov. Aquesta definició es pot especialitzar
quan <span class="math notranslate nohighlight">\(\Omega, \mathcal{X}\)</span> són contables:</p>
<p><span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = \sum_{s\in \Omega: X\left(s\right) \in A } P\left(s\right)\)</span></p>
<p class="note">Enlloc de treballar amb <span class="math notranslate nohighlight">\(P_X\left(X \in A\right)\)</span>, en general caracteritzarem les v.a. a través de les seves funcions de distribució, de massa o de densitat.</p>



<div class="slide-no">21</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id17">

<h3>Il.lustració d’una v.a. i la seva funció de probabilitat</h3>

<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="../_images/v.a.png"><img alt="../_images/v.a.png" src="../_images/v.a.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-text">Diagrama explicatiu de la identitat <span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = P\left(\left\{s\in \Omega: X\left(s\right) \in A \right\}\right)\)</span>.
Podem caracteritzar l’esdeveniment <span class="math notranslate nohighlight">\(X \in A\)</span> relatiu a una v.a. <span class="math notranslate nohighlight">\(X\)</span> en funció de l’esdeveniment <span class="math notranslate nohighlight">\(\left\{s\in \Omega: X\left(s\right) \in A \right\}\)</span>
en l’espai mostral d’orígen. En aquest curs no ho tindrem en compte, però en realitat
no totes les funcions <span class="math notranslate nohighlight">\(X : \Omega \to \mathcal{X} \subseteq \mathbb{R}\)</span> són admissibles,
només les <a class="reference external" href="https://en.wikipedia.org/wiki/Measurable_function">mesurables</a>.</span></p>
</div>



<div class="slide-no">22</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id18">

<h3>Variables aleatòries simplones</h3>

<p>Per entendre un concepte, sempre va bé intentar reflexionar primer sobre
els casos més extremadament simples.</p>
<ul class="simple">
<li>Q:<em>Quina seria la v.a. més simple?</em></li>
<li>R: La v.a. constant, definida com <span class="math notranslate nohighlight">\(X : \Omega \to 0\)</span></li>
<li>Q: <em>I la 2a més simple?</em></li>
<li>R: La v.a. de Bernouilli, definida com <span class="math notranslate nohighlight">\(X : \Omega \to \left\{0, 1\right\}\)</span></li>
</ul>
<p>Aplicant la definició anterior, tenim que la v.a. de Bernouilli està completament
caracterizada per un sol paràmetre <span class="math notranslate nohighlight">\(p = P\left(\left\{s\in \Omega: X\left(s\right) = 1\right\}\right)\)</span></p>



<div class="slide-no">23</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id19">

<h3>Exemple de variable aleatòria discreta: binomial</h3>

<p>Revisitem l’<strong>Experiment 2</strong> anterior (escollim 100 persones i fem una prova d’anticossos per SARS-COV-2)</p>
<ul class="simple">
<li>Teniem que <span class="math notranslate nohighlight">\(\Omega = \left\{+, -\right\}^{100}\)</span></li>
<li>Definim v.a. <span class="math notranslate nohighlight">\(X : \left\{+, -\right\}^{100} \to \mbox{Nombre de +} \in \left[0, 100\right]\)</span></li>
</ul>
<p><strong>Exercici</strong>: Fent servir l’identitat <span class="math notranslate nohighlight">\(P_X\left(X \in A\right) = \sum_{s\in \Omega: X\left(s\right) \in A } P\left(s\right)\)</span>, derivem <span class="math notranslate nohighlight">\(P_X\left(X=k\right)\)</span>.</p>



<div class="slide-no">24</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id20">

<h3>Exemple de variable aleatòria discreta: binomial (2)</h3>

<p>Primer determinem el conjunt <span class="math notranslate nohighlight">\(\left\{s\in \Omega: X\left(s\right) \in A\right\}\)</span> sobre el qual haurem de sumar:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{s\in \Omega: X\left(s\right) \in A\right\} &amp;= \left\{s\in \Omega: X\left(s\right)= k\right\}\\
&amp;= \mbox{Totes les seqüencies amb exactament k +}\end{split}\]</div>
<p>Fixeu-vos que hi ha <span class="math notranslate nohighlight">\({n \choose k}\)</span> seqüencies amb <span class="math notranslate nohighlight">\(k\)</span> “+” d’entre <span class="math notranslate nohighlight">\(n=100\)</span> individus. Per altra banda,
si assumim que cada individu és + de manera independent, tenim que cada seqüència
succeeix amb probabilitat <span class="math notranslate nohighlight">\(p^k\left(1-p\right)^{n-k}\)</span>.</p>
<p>Per tant deduïm que <span class="math notranslate nohighlight">\(P_X\left(X=k\right) = {n \choose k}p^k\left(1-p\right)^{n-k}\)</span> (distribució binomial)</p>
<p class="note">Què passa si alguns individus són membres d’una mateixa família?</p>



<div class="slide-no">25</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id21">

<h3>Qüestionari de repàs</h3>

<ol class="note arabic simple">
<li>Donat un espai mostral <span class="math notranslate nohighlight">\(\Omega\)</span>, quin seria el conjunt d’esdeveniments més “petit”?</li>
<li>Si <span class="math notranslate nohighlight">\(A \cap B = \emptyset\)</span>, vol dir que A, B són esdeveniments independents?</li>
<li>Quin és l’espai mostral d’una v.a. <span class="math notranslate nohighlight">\(X: \Omega \to \mathcal{X}\)</span>?</li>
<li>Quina és la probabilitat de la seqüència [+,-,+,-] si <span class="math notranslate nohighlight">\(P\left(+\right)=0.3\)</span> i cada esdeveniment +/- és mutualment independent?</li>
</ol>



<div class="slide-no">26</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id22">

<h3>Exemple de variable aleatòria contínua</h3>

<p>Revisitem l’<strong>Experiment 3</strong>. Escollim un estudiant d’aquesta classe i aquest cop mesurem la raó alçada/pes:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\Omega = \left(0, \infty \right) \times \left(0, \infty \right)\)</span></li>
<li><span class="math notranslate nohighlight">\(Z: (x, y) \in \Omega \to \frac{x}{y} \in \left(0, \infty \right)\)</span></li>
<li>Com calculariem <span class="math notranslate nohighlight">\(P_Z\left(Z \in A\right)\)</span>? <em>Necessitarem de fer alguna suposició addicional sobre les v.a. X i Y</em></li>
</ul>
<p class="note">En la gran majoria de problemes haurem de fer una hipòtesi sobre el model aleatori de les observacions (hipòtesi que després haurem de validar comprovant la <em>bondat de l’ajust</em>)</p>



<div class="slide-no">27</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="funcio-de-distribucio">

<h3>Funció de distribució</h3>

<p class="note"><strong>Definició</strong> La funció de distribució cumulativa (f.d.c.) d’una v.a. es defineix com <span class="math notranslate nohighlight">\(F\left(x\right) = P\left(X \leq x\right)\)</span>.</p>
<p>De fet qualsevol funció pot ser una f.d.c si compleix [Casella &amp; Berger Teorema 1.5.3]:</p>
<ol class="arabic simple">
<li><span class="math notranslate nohighlight">\(\lim_{x\to -\infty} F(x) = 0\)</span> i <span class="math notranslate nohighlight">\(\lim_{x\to \infty} F(x) = 1\)</span></li>
<li><span class="math notranslate nohighlight">\(F(x)\)</span> és no-decreixent</li>
<li><span class="math notranslate nohighlight">\(F(x)\)</span> és contínua per la dreta (<span class="math notranslate nohighlight">\(\lim_{x\to x_0^+} F(x) = x_0\)</span>)</li>
</ol>
<p class="note">El més important es que la f.d.c caracteritza únicament una variable aleatòria: si <span class="math notranslate nohighlight">\(F_X = F_Y\)</span>, aleshores <span class="math notranslate nohighlight">\(X\)</span> i <span class="math notranslate nohighlight">\(Y\)</span> són idènticament distribuïdes [Casella &amp; Berger 1.5.8 i 1.5.10]</p>



<div class="slide-no">28</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="funcio-de-massa-o-densitat-de-probabilitat">

<h3>Funció de massa o densitat de probabilitat</h3>

<p>A voltes ens serà més pràctic treballar amb un altre objecte, la funció de massa de probabilitat (f.m.p.) <span class="math notranslate nohighlight">\(p_X\)</span> o de densitat de probabilitat (f.d.p) <span class="math notranslate nohighlight">\(f_X\)</span>.</p>
<ul class="simple">
<li><strong>Cas discret</strong>: <span class="math notranslate nohighlight">\(p_X\left(k\right) = P_X\left(X=k\right)\)</span> (noteu que <span class="math notranslate nohighlight">\(F_X\left(x\right) = \sum_{k=-\infty}^{x}p_X\left(k\right))\)</span>)</li>
<li><strong>Cas “continu”</strong>: La funció <span class="math notranslate nohighlight">\(f_X\)</span> tal que <span class="math notranslate nohighlight">\(F_X\left(x\right) = \int_{-\infty}^x f_X\left(t\right)dt\)</span></li>
<li><strong>Cas “mixte”</strong>:  No les podrem caracteritzar amb una f.m.p o una f.d.p, però recordeu que existeixen v.a. que no són discretes ni contínues!</li>
</ul>
<p class="note">Aquí ens desviem una mica de la notació de [Casella &amp; Berger] al fer servir <span class="math notranslate nohighlight">\(p_X\)</span> enlloc de <span class="math notranslate nohighlight">\(f_X\)</span> per la f.m.p.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Estem ometent molts “detalls” tècnics importants… Hi ha variables contínues per les que <span class="math notranslate nohighlight">\(f_X\)</span> no existeix.</p>
</div>



<div class="slide-no">29</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id23">

<h3>Funció de massa o densitat de probabilitat (2)</h3>

<p>Tal i com hem fist per la f.d.c, tenim un resultat similar per la f.d.p o la f.m.p: <span class="math notranslate nohighlight">\(f_X\left(x\right)\)</span> (<span class="math notranslate nohighlight">\(p_X\left(k\right)\)</span>)
és una f.d.p (f.m.p) si i només si [Casella &amp; Berger 1.6.5]:</p>
<ol class="loweralpha simple">
<li><span class="math notranslate nohighlight">\(f_X\left(x\right) \geq 0, \forall x\)</span> (<span class="math notranslate nohighlight">\(p_X\left(k\right) \geq 0, \forall k\)</span>)</li>
<li><span class="math notranslate nohighlight">\(\int_{\infty}^{-\infty} f_X\left(x\right)dx = 1\)</span> (<span class="math notranslate nohighlight">\(\sum_{\infty}^{\infty} p_X\left(k\right) = 1\)</span>)</li>
</ol>
<p>Per tant podem construir una f.d.p. a partir de qualsevol funció <span class="math notranslate nohighlight">\(h\left(x\right)\)</span> no-negativa, definint:</p>
<p><span class="math notranslate nohighlight">\(K = \int_{-\infty}^{\infty} h\left(x\right)dx\)</span> (també coneguda com <em>funció de partició</em>)</p>
<p>i <span class="math notranslate nohighlight">\(f_X\left(x\right) = \frac{h\left(x\right)}{K}\)</span>. Això es fa servir per exemple
en uns objectes anomentats <a class="reference external" href="https://en.wikipedia.org/wiki/Graphical_model">Models Gràfics Probabilístics</a>.</p>



<div class="slide-no">30</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="exemple-funcio-de-distribucio-i-massa-d-una-v-a-geometrica">

<h3>Exemple: funció de distribució i massa d’una v.a. geomètrica</h3>

<p>Considerem la variable aleatòria corresponent a l’experiment de
llançar una moneda fins que surti cara.</p>
<ul class="build simple">
<li>L’espai mostral és: <span class="math notranslate nohighlight">\(\Omega = \left\{C, XC, XXC, \cdots \right\}\)</span></li>
<li>Definim la v.a. <span class="math notranslate nohighlight">\(X\)</span> com el nombre de creus que obtenim abans de la primera cara.</li>
</ul>
<p>Si suposem que:</p>
<ol class="arabic simple">
<li>Cada llançament és independent de l’altre (pregunta: podeu imaginar una situació en que no ho fos)</li>
<li>La probabilitat d’obtenir cara és <span class="math notranslate nohighlight">\(p\)</span></li>
</ol>
<p>Podem calcular <span class="math notranslate nohighlight">\(p_X\left(k\right)=?\)</span></p>



<div class="slide-no">31</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id24">

<h3>Exemple: funció de distribució i massa d’una v.a. geomètrica</h3>

<p>La f.m.p és la distribució geomètrica:</p>
<p><span class="math notranslate nohighlight">\(p_X\left(k\right) = P\left(\mbox{X}\right)^{k-1}P\left(\mbox{C}\right) = \left(1-p\right)^{k-1}p\)</span></p>
<p>A partir de la qual podem calcular la f.d.c:</p>
<p><span class="math notranslate nohighlight">\(F_X\left(x\right) = \sum_{k=1}^x p_X\left(k\right) = \sum_{k=1}^x \left(1-p\right)^{k-1}p\)</span></p>
<p>utilitzant l’identitat <span class="math notranslate nohighlight">\(\sum_{k=1}^x \rho^{x-1}=\frac{1-\rho^x}{1-\rho}\)</span>, podem arribar a:</p>
<p><span class="math notranslate nohighlight">\(F_X\left(x\right) = 1 - \left(1-p\right)^x\)</span></p>
<p>Seria interessant que comprovéssiu que <span class="math notranslate nohighlight">\(F_X\left(x\right)\)</span> compleix les condicions per
ser una f.d.c.</p>



<div class="slide-no">32</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id25">

<h3>Propietat <em>memoryless</em> de les v.a. geomètriques</h3>

<p>Una v.a. <span class="math notranslate nohighlight">\(X\)</span> és <em>memoryless</em> si:</p>
<p><span class="math notranslate nohighlight">\(P\left(X &gt; m+n | X &gt; m\right) = P\left(X &gt; n \right)\)</span></p>
<p><em>Exercici:</em> Comprovem que aquesta propietat es verifica per la <span class="math notranslate nohighlight">\(p_X\left(k\right)\)</span> geomètrica.</p>
<ul class="simple">
<li>L’interpretació de la propietat és interessant, per exemple, en el contexte de la loteria: No haver guanyat després de jugar 10 cops no incrementa la probabilitat que guanyem en els següents 10 cops…</li>
<li>Aquesta propietat no és tant freqüent com podria semblar.</li>
<li>Aquesta f.m.p és interessant per modelar problemes de <em>temps de vida</em>, per exemple: fallada d’un component electrònic, on la probabilitat de que falli <strong>no canvia amb el temps</strong>.</li>
</ul>



<div class="slide-no">33</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="altres-v-a-discretes">

<h3>Altres v.a. discretes</h3>

<p>A través dels exemples, fins ara ja hem vist 4 tipus de variables aleatòries discretes:</p>
<ul class="simple">
<li>Uniforme, <span class="math notranslate nohighlight">\(X \in \left\{0, \cdots, k-1\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = c \right)=\frac{1}{k}\)</span></li>
<li>Bernouilli, <span class="math notranslate nohighlight">\(X \in \left\{0, 1\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = 1; p \right)=p\)</span></li>
<li>Binomial, <span class="math notranslate nohighlight">\(X \in \left\{0, \cdots, n\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = k; p, n \right)={n\choose k}p^k\left(1-p\right)^{n-k}\)</span></li>
<li>Geomètrica, <span class="math notranslate nohighlight">\(X \in \left\{1, \cdots,\right\}\)</span>, <span class="math notranslate nohighlight">\(P\left(X = k; p \right)= p\left(1-p\right)^{k-1}\)</span></li>
</ul>
<p class="note">Exercici: podeu trobar un experiment “físic” que es correspongui a cada una de les v.a. anteriors?</p>
<p>Us recomano donar un cop d’ull pel vostre compte a dues distribucions famoses més,
la <strong>hipergeomètrica</strong> i la <strong>binomial negativa</strong>. Ara donarem una ullada a la de Poisson.</p>



<div class="slide-no">34</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id26">

<h3>La distribució de Poisson</h3>

<p>La distribució de Poisson es pot motivar físicament amb el següent exemple. Suposeu volem modelar # de clients que arriben en un interval T:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/poisson_motivation.png"><img alt="../_images/poisson_motivation.png" src="../_images/poisson_motivation.png" style="height: 300px;" /></a>
</div>
<ul class="simple">
<li>els intervals de temps <span class="math notranslate nohighlight">\(\delta t_i = \frac{T}{N}, N \gg 1\)</span>, aleshores <span class="math notranslate nohighlight">\(B_i\)</span> és aproximadament Bernouilli(p)</li>
<li>els esdeveniments <span class="math notranslate nohighlight">\(B_i\)</span> són independents</li>
</ul>



<div class="slide-no">35</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id27">

<h3>La distribució de Poisson (II)</h3>

<p>Per tant <span class="math notranslate nohighlight">\(X\)</span> és aproximadament <span class="math notranslate nohighlight">\(\mbox{Binomial}\left(N, p\right)\)</span> on N és el nombre d’intervals en el periòde.</p>
<p>La distribució de Poisson apareix quan tenim que <span class="math notranslate nohighlight">\(p \to 0\)</span> i <span class="math notranslate nohighlight">\(N \to \infty\)</span>
mantenint el nombre mig d’arribades per interval de temps fixe, que anomenarem <span class="math notranslate nohighlight">\(\lambda = Np\)</span>.</p>
<p>La f.m.p de  <span class="math notranslate nohighlight">\(X\)</span> és aleshores, quan <span class="math notranslate nohighlight">\(n \to \infty\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P\left(X = k; \lambda \right) &amp; =\frac{n!}{k!\left(n-k\right)!}\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k} \\
                     &amp; =\frac{\lambda^k}{k!}\frac{n!}{\left(n-k\right)!}\frac{1}{n^k}\left(1 - \frac{\lambda}{n}\right)^n\left(1-\frac{\lambda}{n}\right)^{-k} \\
                     &amp; \to \frac{\lambda^k}{k!}e^{-\lambda}\end{split}\]</div>
<p><em>Exercici</em>: Justificar l’últim pas!</p>



<div class="slide-no">36</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="incis-sobre-les-v-a-continues">

<h3>Incís sobre les v.a. contínues</h3>

<p>Hem vist que una variable aleatòria contínua es caracteritza per una funció
de densitat de probabilitat <span class="math notranslate nohighlight">\(f_X\)</span> tal que:</p>
<p><span class="math notranslate nohighlight">\(F_X\left(x\right) = \int_{-\infty}^x f_X\left(t\right)dt\)</span></p>
<p>per tant tenim que</p>
<p><span class="math notranslate nohighlight">\(P\left(a &lt; X \leq b\right) = \int_{a}^b f_X\left(t\right)dt\)</span></p>
<p>Una conseqüència d’aquesta definició quan <span class="math notranslate nohighlight">\(b \to a\)</span> és el que pot semblar paradoxal:</p>
<p><span class="math notranslate nohighlight">\(P\left(X = x\right) = 0\)</span></p>
<p>Pel 3er axioma de Kolmogorov això sembla implicar que
<span class="math notranslate nohighlight">\(P\left(a &lt; X &lt; b\right)\)</span>, sent la unió de tots els punts entre a i b, hauria
de ser també 0. La paradoxa es resol si recordem que el 3er axioma només contempla unions contables!</p>



<div class="slide-no">37</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="la-distribucio-uniforme">

<h3>La distribució uniforme</h3>

<p>La f.d.p més simple es correspon amb la variable aleatòria contínua més simple, escollir
un nombre aleatori dins d’un interval <span class="math notranslate nohighlight">\(\left[a, b\right]\)</span>:</p>
<p><span class="math notranslate nohighlight">\(f_X\left(x; a, b\right) = \left\{\begin{array}{cc} \frac{1}{b-a} &amp; a \leq x \leq b \\ 0 &amp; \mbox{altrament} \end{array}\right.\)</span></p>
<p><strong>Exercicis</strong>:</p>
<ul class="simple">
<li>Calculem la f.d.c d’una variable uniforme.</li>
<li>Doneu un exemple d’un experiment on l’uniforme és un bon model?</li>
<li>Com generarieu una variable uniforme amb un ordinador?</li>
</ul>



<div class="slide-no">38</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="la-familia-gamma">

<h3>La família Gamma</h3>

<p>Recordeu que podem definir una f.d.p tot normalitzant qualsevol funció no-negativa.</p>
<p>Considerem la següent família de funcions, parameteritzades per <span class="math notranslate nohighlight">\(\alpha, \beta\)</span> doncs:</p>
<p><span class="math notranslate nohighlight">\(h\left(t\right) = \frac{t^{\alpha-1}}{\beta^{\alpha}} e^{-\frac{t}{\beta}}\)</span></p>
<p>definides per <span class="math notranslate nohighlight">\(t\in \left[0, \infty\right)\)</span>. Es pot demostrar que per <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span>,</p>
<p><span class="math notranslate nohighlight">\(\Gamma\left(\alpha\right)=\int_{0}^{\infty}h\left(t\right)dt\)</span> existeix.</p>
<p>Per tant definim la família distribucions <span class="math notranslate nohighlight">\(\mbox{gamma}\left(\alpha, \beta\right)\)</span> com:</p>
<p><span class="math notranslate nohighlight">\(f_X\left(x;\alpha, \beta\right) = \frac{1}{\Gamma\left(\alpha\right)}\frac{x^{\alpha-1}}{\beta^{\alpha}} e^{-\frac{x}{\beta}}\)</span></p>



<div class="slide-no">39</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id28">

<h3>Cosines germanes de la Gamma</h3>

<p>La família Gamma és important perquè permet modelar una gran varietat d’experiments i està intimament
relacionada amb altres distribucions:</p>
<ul class="simple">
<li>Distribució exponencial (si fixem <span class="math notranslate nohighlight">\(\alpha=1\)</span>): la cosina contínua de la f.m.p geomètrica que hem vist abans</li>
<li>Distribució de <span class="math notranslate nohighlight">\(\chi^2_p\)</span> (si fixem <span class="math notranslate nohighlight">\(\alpha=p/2\)</span> i <span class="math notranslate nohighlight">\(\beta=2\)</span>)</li>
</ul>
<p>La f.d.p de la Gamma per diversos valors de <span class="math notranslate nohighlight">\(\alpha\)</span> (k a l’imatge) i <span class="math notranslate nohighlight">\(\beta\)</span> (<span class="math notranslate nohighlight">\(\theta\)</span> a la figura) <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">[Font]</a></p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/650px-Gamma_distribution_pdf.svg.png"><img alt="../_images/650px-Gamma_distribution_pdf.svg.png" src="../_images/650px-Gamma_distribution_pdf.svg.png" style="height: 250px;" /></a>
</div>



<div class="slide-no">40</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="la-familia-normal">

<h3>La família “Normal”</h3>

<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/440px-Normal_Distribution_PDF.svg.png"><img alt="../_images/440px-Normal_Distribution_PDF.svg.png" src="../_images/440px-Normal_Distribution_PDF.svg.png" style="height: 150px;" /></a>
</div>
<p><span class="math notranslate nohighlight">\(f_X\left(x ; \mu, \sigma\right) = \frac{1}{\sqrt{2\pi}} e^{-\frac{\left(x - \mu\right)^2}{\sigma^2}}\)</span></p>
<p>La distribució Normal o Gaussiana és fonamental en estadística, per múltiples raons:</p>
<ul class="simple">
<li>Apareix “naturalment” quan sumem/calculem el promig d’un gran nombre de mostres</li>
<li>És simètrica i parameteritzada per 2 paràmetres intuitius (<span class="math notranslate nohighlight">\(\mu\)</span> i <span class="math notranslate nohighlight">\(\sigma\)</span>)</li>
<li>Malgrat la seva aparença intimidant, és tractable analíticament</li>
</ul>



<div class="slide-no">41</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="i-que-te-a-veure-tot-aixo-amb-l-estadistica">

<h3>I què te a veure tot això amb l’estadística?</h3>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Com vem comentar a l’introducció al curs, l’inferència estadística és la ciència d’establir propietats
d’una població mitjantçant mostres de la mateixa.</p>
<p class="last">Els models probabilístics com els que hem vist darrerament són una de les eines que farem servir
per fer aquesta feina d’inferència.</p>
</div>
<p>Vegem un exemple pràctic, el de l’<strong>Experiment 2</strong> (proves d’anticossos).</p>
<ol class="build arabic simple">
<li>Com hem dit, l’estadística comença amb la <strong>recollida de mostres</strong> (dades), en aquest cas, realizar tests d’anticossos a 100 persones a l’atzar i anotar-ne el resultat</li>
<li>El segon pas en <strong>estadística paramètrica</strong> és la definició d’un model probabilístic que caracteritzi les observacions. Com hem vist abans, un model raonable és que cada una de les 100 mostres és una v.a. de Bernouilli.</li>
<li>Ara tenim una col.lecció de mostres, <span class="math notranslate nohighlight">\(\left\{x_1, \cdots, x_{100}\right\}\)</span>, on cada <span class="math notranslate nohighlight">\(x_i\in \left\{0, 1\right\}\)</span>, i un model: <span class="math notranslate nohighlight">\(P_X\left(X_i=1\right) = p\)</span>. L’únic que ens falta per poder fer inferència és trobar el valor de <span class="math notranslate nohighlight">\(p\)</span> que millor descriu les observacions (Tema 2). Per exemple un estimador raonable seria la mitjana aritmètica <span class="math notranslate nohighlight">\(\hat{p}=\frac{1}{100}\sum_i x_i= \frac{\mbox{# de +}}{100}\)</span>. Posem que <span class="math notranslate nohighlight">\(\hat{p}=0.1\)</span>.</li>
</ol>



<div class="slide-no">42</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id29">

<h3>I què te a veure tot això amb l’estadística?</h3>

<p>Amb aquest estimador, obtingut <strong>només a partir de 100 mostres</strong>, i gràcies als resultats que
veurem en els Temes 1 i 2, ja podríem deduïr propietats de la població en general:</p>
<ul class="simple">
<li>Veurem que <span class="math notranslate nohighlight">\(\hat{p}\)</span> és un estimador “sense biaix” de <span class="math notranslate nohighlight">\(p\)</span></li>
<li>Però també veurem que la variabilitat (ex: variança) de <span class="math notranslate nohighlight">\(\hat{p}\)</span> decreix amb el nombre de mostres, i potser 100 són massa poques…</li>
<li>També veurem com, a partir de <span class="math notranslate nohighlight">\(\hat{p}\)</span>, podem donar un interval de confiança sobre <span class="math notranslate nohighlight">\(p\)</span> (ja hem vist que <span class="math notranslate nohighlight">\(\sum_i x_i \sim \mbox{Binomial}\left(p, 100 \right)\)</span>…)</li>
</ul>
<p class="note">Però per tot això primer hem d’aprofundir més en alguns altres conceptes de probabilitat: les transformacions
de v.a., l’esperança, les distribucions conjuntes i algunes desigualtats.</p>



<div class="slide-no">43</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="funcions-de-variables-aleatories">

<h2>Funcions de variables aleatòries</h2>




<div class="slide-no">44</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="transformacions-afins">

<h3>Transformacions afins</h3>

<p>Sovint ens trobarem que el nostre experiment es pot modelar més fàcilment
com la transformació d’una v.a. <span class="math notranslate nohighlight">\(X: \Omega \to \mathcal{X}\)</span> mitjantçant una funció
<span class="math notranslate nohighlight">\(g: \mathcal{X}\to\mathcal{Y}\)</span>: <span class="math notranslate nohighlight">\(Y=g\left(X\right)\)</span></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Recordem que <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> i <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> denoten l’espai mostral d’<span class="math notranslate nohighlight">\(X\)</span>
i <span class="math notranslate nohighlight">\(Y\)</span>, respectivament.</p>
</div>
<p>Per exemple, una transformació senzilla és l’afí: <span class="math notranslate nohighlight">\(Y = a + b X\)</span></p>
<p>En aquest cas, podem expressar la f.d.c <span class="math notranslate nohighlight">\(F_Y\)</span> en funció de <span class="math notranslate nohighlight">\(F_X\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= P\left( a + b X \leq y \right) \\
                  &amp;= P\left( X \leq \frac{y - a}{b} \right) \\
                  &amp;= F_X\left(\frac{y - a}{b} \right)\end{split}\]</div>
<p class="note">Exercici: Fent servir aquesta identitat, demostreu que si <span class="math notranslate nohighlight">\(X \sim \mathcal{N}\left(\mu, \sigma\right)\)</span>, <span class="math notranslate nohighlight">\(Y = \frac{X - \mu}{\sigma} \sim \mathcal{N}\left(0, 1\right)\)</span></p>



<div class="slide-no">45</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="cas-generic">

<h3>Cas genèric</h3>

<p>Per una funció genèrica, <span class="math notranslate nohighlight">\(g: \mathcal{X}\to\mathcal{Y}\)</span>,
no serà tan senzill caracteritzar la f.d.c de <span class="math notranslate nohighlight">\(Y\)</span> en funció de la d’<span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Sota unes condicions tècniques relativament generals, podrem definir una funció de
probabilitat el conjunt d’esdeveniments associat a l’espai mostral <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> com segueix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P\left(Y \in A\right) &amp; = P\left(\left\{x \in \mathcal{X}: g\left(x\right) \in A \right\}\right) \\
                      &amp; = P\left(X \in g^{-1}\left(A\right)\right)\end{split}\]</div>
<p>on definim el mapa invers <span class="math notranslate nohighlight">\(g^{-1}\left(A\right) = \left\{ x\in \mathcal{X}: g(x) \in A\right\}\)</span> [Casella &amp; Berger 2.1.1]</p>



<div class="slide-no">46</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id30">

<h3>Il.lustració de l’identitat anterior</h3>

<p>Vegem la relació entre els tres espais mostrals mitjantçant un diagrama:</p>
<div class="figure align-center" id="id5">
<a class="reference internal image-reference" href="../_images/transformation.png"><img alt="../_images/transformation.png" src="../_images/transformation.png" style="height: 280px;" /></a>
<p class="caption"><span class="caption-text"><span class="math notranslate nohighlight">\(P\left(Y \in A\right) = P\left(X \in g^{-1}\left(A\right)\right) = P\left(\left\{ s \in \Omega: X(s) \in g^{-1}\left(A\right) \right\}\right)\)</span></span></p>
</div>
<p class="note">Sortosament, normalment no haurem de raonar directament sobre <span class="math notranslate nohighlight">\(\Omega\)</span>, ja que en molts casos
podrem caracteritzar <span class="math notranslate nohighlight">\(Y\)</span> en base a la f.d.c d’<span class="math notranslate nohighlight">\(X\)</span>.</p>



<div class="slide-no">47</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="f-d-c-i-transformacions-monotones">

<h3>F.d.c i transformacions monòtones</h3>

<p>En general, la f.d.c. d’<span class="math notranslate nohighlight">\(Y\)</span> vé donada per l’expressió [Casella &amp; Berger 2.1.4]:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= P\left( g\left(X\right) \leq y \right) \\
                  &amp; = P_X\left(\left\{x \in \mathcal{X}: g\left(x\right) \leq y \right\}\right)\end{split}\]</div>
<p>En el cas que la funció <span class="math notranslate nohighlight">\(g: \mathcal{X}\to\mathcal{Y}\)</span> sigui monòtona stricta (creixent o decreixent),
tindrem que és injectiva i surjectiva, i per tant podem definir <span class="math notranslate nohighlight">\(g^{-1}: \mathcal{Y}\to\mathcal{X}\)</span>
associant un únic x a cada y. Per exemple, en el cas monòton creixent:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{x \in \mathcal{X}: g\left(x\right) \leq y \right\} &amp; =  \left\{x \in \mathcal{X}: x \leq g^{-1}\left(y\right) \right\} \\\end{split}\]</div>
<p>Per tant podem simplicar l’expressió [Casella &amp; Berger 2.1.3]: <span class="math notranslate nohighlight">\(F_Y\left(y\right) =F_x\left(g^{-1}\left(y\right)\right)\)</span></p>



<div class="slide-no">48</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="transformacions-monotones-i-diferenciables">

<h3>Transformacions monòtones i diferenciables</h3>

<p>Si ens restringim a v.a’s contínues i a transformacions estrictament monòtones diferenciables:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= F_x\left(g^{-1}\left(y\right)\right) \mbox{(g creixent)} \\
F_Y\left(y\right) &amp;= 1 - F_x\left(g^{-1}\left(y\right)\right) \mbox{(g decreixent)}\\\end{split}\]</div>
<p>són diferenciables, i aplicant la regla de la cadena arribem al famós resultat de la “transformació per Jacobià” [Casella &amp; Berger 2.1.5]:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_Y\left(y\right) &amp;= f_x\left(g^{-1}\left(y\right)\right)\left|\frac{d g^{-1}\left(y\right)}{dy} \right| \\\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\forall y \in \mathcal{Y} = \left\{y : \exists x\in \mathcal{X},  g(x)=y \right\}\)</span>
on <span class="math notranslate nohighlight">\(\mathcal{X} = \left\{x : f_X\left(x\right) &gt; 0 \right\}\)</span>.</p>
<p class="note">Veure [Casella &amp; Berger 2.1.8] per una extensió on la funció <span class="math notranslate nohighlight">\(g\)</span> és monòtona només sobre alguns intervals!</p>



<div class="slide-no">49</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id31">

<h3>Aplicació: derivació de la distribució de <span class="math notranslate nohighlight">\(\chi^2_1\)</span></h3>

<p>Veiem aquí un exemple de com el resultat anterior es pot extendre a transformacions
no monòtones interessants en estadística. Considerarem la distribució de la transformació (contínua i diferenciable)</p>
<p><span class="math notranslate nohighlight">\(Y = X^2\)</span></p>
<p>quan <span class="math notranslate nohighlight">\(X \sim \mathcal{N}\left(0, 1\right)\)</span>. Observem que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_Y\left(y\right) &amp;= P_X\left(-\sqrt{y} \leq X \leq \sqrt{y}\right) \\
                  &amp;= P_X\left(X \leq \sqrt{y}\right) - P_X\left(X \leq -\sqrt{y}\right) \\
                  &amp;= F_X\left(\sqrt{y}\right) - F_X\left(-\sqrt{y}\right)\end{split}\]</div>
<p>Diferenciant i fent servir la simetria de <span class="math notranslate nohighlight">\(F_X\left(x\right)\)</span> respecte 0, obtenim:</p>
<p><span class="math notranslate nohighlight">\(f_y\left(y\right) = y^{-\frac{1}{2}} f_X\left(\sqrt{y}\right) = \frac{y^{-\frac{1}{2}}}{\sqrt{2\pi}} e^{-\frac{y}{2}}\)</span>,
que podem identificar amb la Gamma si fixem <span class="math notranslate nohighlight">\(\alpha=1/2\)</span> i <span class="math notranslate nohighlight">\(\beta=2\)</span>, que és
la <span class="math notranslate nohighlight">\(\chi^2_1\)</span>.</p>



<div class="slide-no">50</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="transformacio-integral">

<h3>Transformació integral</h3>

<p>L’última transformació que veurem inspirarà un
algoritme per generar mostres de v.a. contínues amb distribucions
arbitràries (ho veurem a la primera pràctica).</p>
<p class="note">[Casella &amp; Berger 2.1.10] Sigui <span class="math notranslate nohighlight">\(X\)</span> una v.a. contínua caracteritzada per <span class="math notranslate nohighlight">\(F_X\)</span>. Aleshores
la v.a. <span class="math notranslate nohighlight">\(Y = F_X\left(X\right)\)</span> és uniforme entre <span class="math notranslate nohighlight">\(\left[0, 1\right]\)</span></p>
<p>La demostració passa per la definició de la funció:</p>
<p><span class="math notranslate nohighlight">\(F_X^{-1}\left(y\right) = \left\{\begin{array}{cc} \inf \left\{x : F\left(x\right) \geq y \right\} &amp; y \in \left(0, 1\right) \\ \infty &amp; y=1 \\ -\infty &amp; y = 0 \end{array}\right.\)</span></p>
<p>I observant que (compte amb el segon “=”!):</p>
<p><span class="math notranslate nohighlight">\(P\left(Y \leq y \right) = P\left(F_X\left(X\right) \leq y\right) = P\left(X \leq F_X^{-1}\left(y\right)\right) = y\)</span></p>



<div class="slide-no">51</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id1">

<h3>Qüestionari de repàs</h3>

<ol class="build arabic simple">
<li>Quines condicions ha de verificar una f.d.p o una f.m.p?</li>
<li>Si compro un dècim de loteria de nadal cada any, i hi ha 170M de dècims, quina és la probabilitat que no em toqui en tota la vida?</li>
<li>Les v.a. són contínues o discretes segons si la seva <span class="math notranslate nohighlight">\(F_X\)</span> és contínua o discreta: cert o fals?</li>
<li>Per què el fet que una v.a. contínua satisfaci <span class="math notranslate nohighlight">\(P(X=x)=0\)</span> no implica que <span class="math notranslate nohighlight">\(P(a &lt; X \leq b)=0\)</span>?</li>
<li>La distribució de Poisson és un bona aproximació de la binomial quan ______ i ________.</li>
<li>Per demostrar el teorema de la transformació integral, hem definit <span class="math notranslate nohighlight">\(F_X^{-1}\left(y\right) = \inf \left\{x : F\left(x\right) \geq y \right\}, y \in (0, 1)\)</span>. Perquè l’infimum i perquè el &gt;=?</li>
<li>En quins 4 casos podem caracteritzar fàcilment una variable <span class="math notranslate nohighlight">\(Y=g(X)\)</span> en funció de la distribució de X?</li>
</ol>



<div class="slide-no">52</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="esperanca-i-moments">

<h2>Esperança i moments</h2>




<div class="slide-no">53</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="esperanca">

<h3>Esperança</h3>

<p>L’esperança o mitja d’una v.a. <span class="math notranslate nohighlight">\(g\left(X\right)\)</span> es defineix com:</p>
<ul class="simple">
<li>Cas continu: <span class="math notranslate nohighlight">\(E\left(g\left(X\right)\right) = \int_{-\infty}^{\infty} g\left(x\right)f_X\left(x\right)dx\)</span></li>
<li>Cas discret: <span class="math notranslate nohighlight">\(E\left(g\left(X\right)\right) = \sum_{k} g\left(k\right)p_X\left(k\right)\)</span></li>
</ul>
<p>Com ja sabeu, l’esperança pot ser un indicador de “localització” però depèn de la dispersió
(ex: variança) de la distribució en questió…</p>
<p class="note">Aquesta definició es coneix com la <a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician">Llei de l’Estadístic Inconscient</a>,
perquè en realitat l’existència de <span class="math notranslate nohighlight">\(E\left(g\left(X\right)\right)\)</span> s’hauria de provar formalment.
Recordeu doncs que l’esperança no té perquè existir! L’exemple clàssic és <span class="math notranslate nohighlight">\(g(x)=x\)</span> i amb una distribució de Cauchy.</p>
<p><strong>Exercici</strong>: Podeu imaginar una distribució on l’esperança ens pot donar una idea equivocada?</p>



<div class="slide-no">54</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id32">

<h3>Esperança</h3>

<p>L’esperança i la mitja aritmètica són cosines germanes. La següent és una interpretació
de la mitja que pot ser útil:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/mitja_Esperança.png"><img alt="../_images/mitja_Esperança.png" src="../_images/mitja_Esperança.png" style="height: 330px;" /></a>
</div>
<p>Per tant <span class="math notranslate nohighlight">\(\bar{X} = \sum_{i} c_i \frac{\mbox{#}\left\{x=c_i\right\}}{N} = \sum_{i} c_i \hat{P}\left(X = c_i\right)\)</span> (S’assembla a l’esperança, no?)</p>



<div class="slide-no">55</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id33">

<h3>Exemple de càlcul d’esperances: v.a. de Poisson</h3>

<p>El càlcul l’esperança sol dependre una mica de la forma de la f.d.p o f.m.p
de la v.a. en questió.</p>
<p>Vegem com ho fariem per la f.m.p de Poisson:</p>
<p><span class="math notranslate nohighlight">\(P\left(X = k; \lambda \right) = \frac{\lambda^k}{k!}e^{-\lambda}\)</span></p>
<p>Aplicant la definició:</p>
<div class="math notranslate nohighlight">
\[\begin{split}E(X) &amp; = \sum_{k=0}^{\infty} k \frac{\lambda^k}{k!}e^{-\lambda} \\
     &amp; = \lambda e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{\left(k-1\right)!} \\
     &amp; = \lambda\end{split}\]</div>
<p>On hem fet servir l’identitat <span class="math notranslate nohighlight">\(\sum_{j=0}^{\infty} \frac{\lambda^{j}}{j!}=e^{\lambda}\)</span></p>



<div class="slide-no">56</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id34">

<h3>Exemple de càlcul d’esperances: menys logaritme d’una v.a. uniforme</h3>

<p>Suposem <span class="math notranslate nohighlight">\(X \sim U[0, 1]\)</span> i <span class="math notranslate nohighlight">\(Y=-\log(X)\)</span>:</p>
<ul class="simple">
<li>Podem calcular <span class="math notranslate nohighlight">\(E(Y)\)</span> adonant-nos que <span class="math notranslate nohighlight">\(F_Y(y)=P(Y \leq y) = 1 - e^{-y}\)</span>, per tant Y es exponencial amb paràmetre <span class="math notranslate nohighlight">\(\lambda=1\)</span></li>
<li>Podem calcular <span class="math notranslate nohighlight">\(E(Y) = \int_{0}^1 -\log(x)dx=x - \log(x)|^1_0=1\)</span></li>
</ul>
<p class="note">En alguns casos podem escollir entre fer servir la definició amb <span class="math notranslate nohighlight">\(g(x)\)</span>
o bé calcular la f.d.p de <span class="math notranslate nohighlight">\(Y=g(X)\)</span> per calcular-ne <span class="math notranslate nohighlight">\(E(Y)\)</span>.</p>



<div class="slide-no">57</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id35">

<h3>Propietats de l’esperança</h3>

<p>La majoria de propietats de l’esperança provenen de la
linearitat de l’operador integració/suma [Casella &amp; Berger 2.2.5]*:</p>
<ol class="arabic simple">
<li><span class="math notranslate nohighlight">\(E\left(a X + b Y + c\right) = aE\left( X\right) + b E\left( Y\right) + c\)</span></li>
<li>Si <span class="math notranslate nohighlight">\(X \geq Y\)</span>, aleshores <span class="math notranslate nohighlight">\(E\left(X\right)\geq E\left(Y\right)\)</span></li>
<li>Si <span class="math notranslate nohighlight">\(a\geq X \geq b\)</span>, aleshores <span class="math notranslate nohighlight">\(a \geq E\left(X\right)\geq b\)</span></li>
</ol>
<p><em>Demostració (1) i (2) a la pissarra, (3) com exercici</em></p>
<p class="note"><a href="#id2"><span class="problematic" id="id3">*</span></a>Nota: En realitat, a [Casella &amp; Berger 2.2.5] contemplen només el cas
<span class="math notranslate nohighlight">\(X=g_1(x)\)</span> i <span class="math notranslate nohighlight">\(Y=g_2(X)\)</span>. Aquí farem una mica de trampa i farem
servir ja les distribucions marginals d’<span class="math notranslate nohighlight">\(X\)</span> i <span class="math notranslate nohighlight">\(Y\)</span> que encara no hem definit
però definirem més endavant.</p>



<div class="slide-no">58</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id36">

<h3>Aplicació de la linearitat de l’E: Cupons</h3>

<p class="build">Tot i que podria semblar un resultat trivial, la linearitat de l’esperança és una propietat
molt útil a la pràctica. Vegem per exemple la seva aplicació en el següent problema:</p>
<p>[Rice 4.1.2 Exemple B] Tenim <span class="math notranslate nohighlight">\(n\)</span> tipus diferents de cupons, i cada cop que comprem cereals ens en donen
un dels <span class="math notranslate nohighlight">\(n\)</span> a l’atzar. Quans cereals haurem de comprar per aconseguir-los tots?</p>
<ol class="arabic simple">
<li>Definim <span class="math notranslate nohighlight">\(X_i\)</span> nombre de compres fins que aconseguim el cupó <em>i</em></li>
<li>El nombre total de compres <span class="math notranslate nohighlight">\(Y=\sum_{i=1}^n X_i\)</span></li>
<li>Noteu que <span class="math notranslate nohighlight">\(X_i\)</span> es una v.a. geomètrica, amb paràmetre <span class="math notranslate nohighlight">\(p_i = \frac{n -i + 1}{n}\)</span></li>
<li>Recordem que en aquest cas <span class="math notranslate nohighlight">\(E(X_i) = \frac{1}{p_i}\)</span></li>
<li>Per linearitat de l’esperança <span class="math notranslate nohighlight">\(E(Y)=\sum_{i=1}^n E(X_i) = \frac{n}{n} + \frac{n}{n-1} + \cdots + n = n\sum_{i=1}^n\frac{1}{i}\)</span></li>
</ol>



<div class="slide-no">59</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id37">

<h3>Aplicació de la linearitat de l’E: Optimització d’inversions</h3>

<p>[Rice 4.1.2 Exemple E] Suposeu que tenim una cartera d’inversió amb dues accions A i B de borsa
amb retorns representats per v.a.’s <span class="math notranslate nohighlight">\(R_A\)</span> i <span class="math notranslate nohighlight">\(R_B\)</span>.</p>
<ol class="build arabic simple">
<li>Si invertim una fracció <span class="math notranslate nohighlight">\(\pi\)</span> del nostre capital a A, i <span class="math notranslate nohighlight">\(1-\pi\)</span> a B, tindrem que el retorn final serà: <span class="math notranslate nohighlight">\(R = \pi R_1 + \left(1 - \pi\right)R_2\)</span></li>
<li>Per linearitat de l’esperança, <span class="math notranslate nohighlight">\(E(R) = \pi E(R_A) + \left(1 - \pi\right)E(R_B)\)</span></li>
<li>Per tant l’estratègia òptima d’inversió seria <span class="math notranslate nohighlight">\(\pi=1\)</span> si <span class="math notranslate nohighlight">\(E(R_A)&gt;E(R_B)\)</span> i <span class="math notranslate nohighlight">\(\pi=0\)</span> en cas contrari</li>
</ol>
<p class="note">Fixeu-vos que una possible correlació entre <span class="math notranslate nohighlight">\(R_A\)</span> i <span class="math notranslate nohighlight">\(R_B\)</span> és irrellevant…</p>
<p class="note">Clarament la gestió de carteres és més complicada que això… què creieu que falla en el nostre model?</p>



<div class="slide-no">60</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="moments-i-moments-centrals">

<h3>Moments i moments centrals</h3>

<p>A partir de l’esperança podem definir altres quantitats caracteritzant
una v.a. [Casella &amp; Berger 2.3.1]. Per tot enter <span class="math notranslate nohighlight">\(n\)</span>, definim:</p>
<ul class="simple">
<li>El moment d’ordre <span class="math notranslate nohighlight">\(n\)</span> com: <span class="math notranslate nohighlight">\(\mu_n' = E(X^n)\)</span></li>
<li>El moment <em>central</em> d’ordre <span class="math notranslate nohighlight">\(n\)</span> com: <span class="math notranslate nohighlight">\(\mu_n = E\left(\left(X - E(X)\right)^n\right)\)</span></li>
</ul>
<p>(que recordem no tenen perquè existir!)</p>
<p class="build">Exemples:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\mu_0'=1\)</span>, <span class="math notranslate nohighlight">\(\mu_1=0\)</span></li>
<li>La variança, <span class="math notranslate nohighlight">\(\mbox{Var}\left(X\right) = \mu_2\)</span>, que indica la desviació d’X respecte la seva mitja</li>
<li>L’asimetria (<em>skewness</em>) <span class="math notranslate nohighlight">\(\mu_3\)</span> que ens indica si la cua de la f.d.p està a l’esquerra (&lt;0) o a la dreta (&gt;0) de la mitja</li>
</ul>



<div class="slide-no">61</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="l-esperanca-com-a-predictor-de-minim-error-quadrat">

<h3>L’esperança com a predictor de mínim error quadrat</h3>

<p>Amb aquesta última definició podem establir una propietat
fonamental de l’esperança. Considerem el següent problema de predicció
d’una v.a. X tal que minitzem l’error de predicció [Casella &amp; Berger 2.2.6]:</p>
<p class="note">Trobar <span class="math notranslate nohighlight">\(\theta\)</span> tal que <span class="math notranslate nohighlight">\(\min_{\theta} E\left(X - \theta \right)^2\)</span>.</p>
<p>Observem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}E\left(X - \theta \right)^2 &amp;= E\left(X - E(X) + \left(E(X) - \theta\right)\right)^2 \\
                            &amp;= E\left(X - E(X)\right)^2 + E\left(E(X) - \theta\right)^2 + \\
                            &amp; + 2E\left(X - E(X)\right)E\left(E(X) - \theta\right) \\
                            &amp;= \mbox{Var}\left(X\right) + E\left(E(X) - \theta\right)^2 \geq \mbox{Var}\left(X\right)\end{split}\]</div>
<p>(pel tercer “=”, <span class="math notranslate nohighlight">\(E\left(X - E(X)\right)=0\)</span>.) Per tant <span class="math notranslate nohighlight">\(\theta^*=E(X)\)</span>!</p>



<div class="slide-no">62</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="funcio-generatriu-de-moments">

<h3>Funció generatriu de moments</h3>

<p>Per una v.a. X, la funció generatriu de moments (<em>f.g.m</em>) (<em>Moment-Generating Function</em>) es defineix com:</p>
<p><span class="math notranslate nohighlight">\(M_X\left(t\right)=E\left(e^{tX}\right)\)</span></p>
<p>suposant que existeix per <span class="math notranslate nohighlight">\(t\in [-\epsilon, \epsilon]\)</span> amb <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span>.</p>
<p>En estadística, la f.g.m es fa servir majoritàriament per tres raons:</p>
<ol class="arabic simple">
<li>Calcular els moments d’una distribució que altrament serien molt difícils de calcular, veure [Casella &amp; Berger 2.3.7 i 2.3.8], per exemple els moments d’una Gamma.</li>
<li>Per calcular distribucions de transformacions afins de v.a. [Casella &amp; Berger 2.3.15]</li>
<li>Per calcular la distribució de la suma de v.a. independents [Casella &amp; Berger 4.2.12]</li>
</ol>



<div class="slide-no">63</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id38">

<h3>Funció generatriu de moments (2)</h3>

<p>En aquest curs la farem servir més endavant per l’objectiu (3).</p>
<p>Per ara, només mencionar que aquesta utilitat es deriva d’un resultat
fonamental per les f.g.m’s, que és que sota algunes condicions, la f.g.m
caracteritza inequívocament una f.d.c:</p>
<p class="note">[Casella &amp; Berger 2.3.11] Siguin <span class="math notranslate nohighlight">\(F_X\)</span>, <span class="math notranslate nohighlight">\(F_Y\)</span> f.d.c’s per les quals
tots els moments existeixen. Aleshores <span class="math notranslate nohighlight">\(M_X\left(t\right)=M_Y\left(t\right)\)</span>
per <span class="math notranslate nohighlight">\(t\in [-\epsilon, \epsilon]\)</span> amb <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> implica que <span class="math notranslate nohighlight">\(F_X = F_Y\)</span>.</p>
<p>És aquest resultat el que ens permetrà més endavant de calcular
una f.g.m i a partir d’aquesta desfer el camí i obtenir-be la f.d.c. corresponent.</p>



<div class="slide-no">64</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="desigualtats">

<h2>Desigualtats</h2>




<div class="slide-no">65</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="desigualtat-de-markov">

<h3>Desigualtat de Markov</h3>

<p>La desigualta de Markov estableix que, per qualsevol v.a. <span class="math notranslate nohighlight">\(X \geq 0\)</span>,
tal que <span class="math notranslate nohighlight">\(E(X)\)</span> existeix, podem acotar la probabilitat de que <span class="math notranslate nohighlight">\(X\)</span>
excedeixi un cert valor <span class="math notranslate nohighlight">\(t &gt; 0\)</span> per:</p>
<p><span class="math notranslate nohighlight">\(P(X \geq t) \leq \frac{E(X)}{t}\)</span></p>
<p>Per exemple, si fixem <span class="math notranslate nohighlight">\(t = kE(X)\)</span>, podem acotar la proba que <span class="math notranslate nohighlight">\(X\)</span>
excedeixi la seva mitja per un factor k:</p>
<p><span class="math notranslate nohighlight">\(P(X \geq k E(X)) \leq \frac{1}{k}\)</span></p>
<p><em>Demostració</em>:</p>
<p>Descomposar <span class="math notranslate nohighlight">\(E(X)=\int_{-\infty}^t x f_X(x)dx + \int_t^{\infty} x f_X(x)dx\)</span>
i observar que <span class="math notranslate nohighlight">\(\int_{-\infty}^t x f_X(x)dx \geq 0\)</span> i <span class="math notranslate nohighlight">\(\int_t^{\infty} x f_X(x)dx \geq t P(X\geq t)\)</span></p>



<div class="slide-no">66</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="desigualtat-de-txebitxev">

<h3>Desigualtat de Txebitxev</h3>

<p>La desigualtat de Markov és molt laxa perquè només fa servir informació sobre <span class="math notranslate nohighlight">\(E(X)\)</span>.
La seva extensió (desigualtat de Txebitxev) ens permetrà establir cotes una mica més
útils. Sigui v.a. <span class="math notranslate nohighlight">\(g(X) \geq 0\)</span>, tal que <span class="math notranslate nohighlight">\(E(g(X))\)</span> existeix, per qualsevol <span class="math notranslate nohighlight">\(t &gt; 0\)</span> tenim:</p>
<p><span class="math notranslate nohighlight">\(P(g(X) \geq t) \leq \frac{E(g(X))}{t}\)</span></p>
<p>A priori això sembla calcat a la de Markov, però vegem-ne una aplicació:</p>
<p class="note">Sigui <span class="math notranslate nohighlight">\(X\)</span> una v.a. amb mitja <span class="math notranslate nohighlight">\(\mu\)</span> i variança <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Aleshores
<span class="math notranslate nohighlight">\(P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}\)</span></p>
<p><em>Demostració</em>: Per demostrar Txebitxev, es segueix la mateixa idea que per Markov.
Per demostrar-ne aquesta aplicació, n’hi ha prou amb aplicar Txebitxev amb <span class="math notranslate nohighlight">\(g(x) = \left(\frac{x - \mu}{\sigma}\right)^2\)</span></p>



<div class="slide-no">67</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="desigualtat-de-jensen">

<h3>Desigualtat de Jensen</h3>

<p>L’última desigualtat que considerarem és la de Jensen, que sota
la seva simplicitat amaga moltíssim poder.</p>
<p>Sigui <span class="math notranslate nohighlight">\(g(x)\)</span> una funció convexa i <span class="math notranslate nohighlight">\(X\)</span> una v.a. tal que <span class="math notranslate nohighlight">\(E(g(x))\)</span>
existeix. Aleshores:</p>
<p><span class="math notranslate nohighlight">\(E(g(x)) \geq g(E(X))\)</span></p>
<p><em>Aplicacions</em>: Moltíssimes, però dues d’immediates són les d’obtenir cotes, per exemple:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(E(X^2) \geq (E(X))^2\)</span></li>
<li><span class="math notranslate nohighlight">\(E(\frac{1}{X}) \geq \frac{1}{E(X)}\)</span></li>
</ul>
<p><em>Demostració</em>: A la pissarra</p>



<div class="slide-no">68</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-2" id="vectors-aleatoris-variables-multivariades">

<h2>Vectors aleatoris / Variables multivariades</h2>




<div class="slide-no">69</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="variables-multivariades">

<h3>Variables multivariades</h3>

<p>Una variable aleatòria multivariada és una extensió d’una v.a.
a múltiples dimensions:</p>
<p class="note"><strong>Definició</strong> Una variable aleatòria multivariada (<em>v.m.</em> pels amics)
és una funció <span class="math notranslate nohighlight">\(\mathbf{X} : \Omega \to \mathcal{X} \subseteq \mathbb{R}^K\)</span>.</p>
<p>Exemples:</p>
<ul class="simple">
<li><strong>V.m. contínua</strong>: Mesurem l’alçada, pes i perímetre cranial dels nadons al néixer. Cada mostra es pot interpretar com una v.m. contínua de dimensió 3, que pren valors en <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>.</li>
<li><strong>V.m. discreta</strong>: Agafem un document de text i en contem el nombre de vegades que apareixen <span class="math notranslate nohighlight">\(K\)</span> paraules d’un diccionari. El resultat és un vector discret que pren valors a <span class="math notranslate nohighlight">\(\mathbb{N}^K\)</span></li>
</ul>



<div class="slide-no">70</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="funcio-de-densitat-o-de-massa-de-probabilitat-conjuntes-cas-bivariat">

<h3>Funció de densitat o de massa de probabilitat conjuntes: Cas bivariat</h3>

<p>De manera anàloga al que hem vist per v.a.’s, ver una v.m <span class="math notranslate nohighlight">\((X, Y)\)</span> també
podem definir:</p>
<ul class="simple">
<li>Una <em>funció de massa de probabilitat conjunta</em>: <span class="math notranslate nohighlight">\(p_{X,Y}(x, y)\)</span> si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> contable</li>
<li>Una <em>funció de distribució de probabilitat conjunta</em>: <span class="math notranslate nohighlight">\(f_{X,Y}(x, y)\)</span> si <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> incontable</li>
</ul>
<p>Per tal de ser vàlides aquestes funcions han de verificar:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(p_{X,Y}(x, y) \geq 0\)</span>, <span class="math notranslate nohighlight">\(\sum_{x, y \in \mathcal{X}}p_{X,Y}(x, y)=1\)</span></li>
<li><span class="math notranslate nohighlight">\(f_{X,Y}(x, y) \geq 0\)</span>, <span class="math notranslate nohighlight">\(\int\int_{x, y \in \mathcal{X}}f_{X,Y}(x, y)=1\)</span></li>
</ul>
<p class="note">Durant aquest repàs, presentarem els conceptes només pel cas bivariat (<span class="math notranslate nohighlight">\(N=2\)</span>),
l’extensió a <span class="math notranslate nohighlight">\(K&gt;2\)</span> és gairebé immediata [Casella &amp; Berger 4.6]</p>



<div class="slide-no">71</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id39">

<h3>Funció de densitat o de massa de probabilitat conjuntes: Cas bivariat (2)</h3>

<p>La f.m.p. o la f.d.c. es poden utilitzar per caracteritzar la probabilitat
d’un esdeveniment <span class="math notranslate nohighlight">\(A\)</span> (omitirem el cas discret):</p>
<p><span class="math notranslate nohighlight">\(P((X, Y) \in A) = \int\int_{x, y \in A} f_{X,Y}(x,y)dx dy\)</span></p>
<p><em>Exemple</em>: <span class="math notranslate nohighlight">\((X,Y)\)</span> són les coordenades d’arribada d’un dard llançat
en un tauler de radi <span class="math notranslate nohighlight">\(r &gt;0\)</span>.</p>
<p>Si suposem que sóm uniformement dolents llançant dards, podem caracteritzar la
f.d.p com una uniforme en el cercle de radi <span class="math notranslate nohighlight">\(r\)</span>:</p>
<p><span class="math notranslate nohighlight">\(f(x,y) = \left\{\begin{array}{cc}\frac{1}{\pi}&amp;\mbox{ si } x^2 + y^2 \leq 1 \\ 0  &amp;\mbox{ altrament } \end{array}\right\}\)</span></p>
<p><em>Exercici</em>: Calculeu <span class="math notranslate nohighlight">\(P((X, Y) \in A)\)</span> per <span class="math notranslate nohighlight">\(A = \left\{x, y: t \leq x^2 + y^2 \leq 1\right\}\)</span></p>



<div class="slide-no">72</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="distribucions-marginals">

<h3>Distribucions marginals</h3>

<p>A vegades voldrem caracteritzar només un dels components d’un vector aleatori <span class="math notranslate nohighlight">\((X, Y)\)</span>.</p>
<p>Per exemple, si volem calcular <span class="math notranslate nohighlight">\(P(X \in A_x)\)</span>.</p>
<p>Per fer-ho, farem servir el que s’anomena la f.d.p <em>marginal</em> d’X:</p>
<p><span class="math notranslate nohighlight">\(f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy\)</span> (cas continu)</p>
<p>o:</p>
<p><span class="math notranslate nohighlight">\(p_X(x) = \sum_{k} f_{X,Y}(x, k) dy\)</span> (cas discret)</p>
<p><strong>Exemple:</strong> Quina és la probabilitat que el dard de l’exemple anterior caigui
en la regió <span class="math notranslate nohighlight">\(-1\leq y \leq 1\)</span> (<span class="math notranslate nohighlight">\(r&gt;1\)</span>).</p>
<p class="note">És important recordar que les marginals no contenen tota la informació que hi ha en la conjunta!</p>



<div class="slide-no">73</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="exemple-de-variable-multivariada-discreta-multinomial">

<h3>Exemple de variable multivariada discreta: multinomial</h3>

<p>Considerem ara un exemple d’una v.m. discreta per <span class="math notranslate nohighlight">\(K \geq 2\)</span>:</p>
<ul class="simple">
<li>Suposeu un experiment en que cada realització pren un entre <span class="math notranslate nohighlight">\(K\)</span> valors discret, amb probabilitats <span class="math notranslate nohighlight">\(p_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\cdots,K\)</span>, <span class="math notranslate nohighlight">\(\sum_i p_i = 1\)</span>.</li>
<li>Repetim l’experiment <span class="math notranslate nohighlight">\(N\)</span> vegades, cada realització és mutuament independent amb les altres</li>
<li>Definim la v.m. <span class="math notranslate nohighlight">\(\mathbf{X}=\left[X_1, \cdots, X_K\right]\)</span> com un vector on cada element <span class="math notranslate nohighlight">\(X_i\)</span> conta el nombre de vegades que hem observat el valor <span class="math notranslate nohighlight">\(i\)</span></li>
</ul>
<p>Aleshores la v.m <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> segueix una distribució multinomial [Casella &amp; Berger 4.6.2]</p>
<p><span class="math notranslate nohighlight">\(p_{X_1, \cdots, X_K}\left(x_1, \cdots, x_K\right) = N!\Pi_{i=1}^{K}\frac{p_i^{x_i}}{x_i!}\)</span></p>



<div class="slide-no">74</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id40">

<h3>Exemple de variable multivariada discreta: multinomial (2)</h3>

<p>És obvi que <span class="math notranslate nohighlight">\(p_{X_1, \cdots, X_K}\left(x_1, \cdots, x_K\right)\geq 0\)</span>. Per demostrar que això és efectivament una f.m.p,
haurem d’aplicar el Teorema Binomial [Casella &amp; Berger 4.6.4], que diu que</p>
<p><span class="math notranslate nohighlight">\(\left(p_1 + \cdots + p_N\right)^N = N!\Pi_{i=1}^{K}\frac{p_i^{x_i}}{x_i!}\)</span></p>
<p>i això és igual a 1 ja que <span class="math notranslate nohighlight">\(p_1 + \cdots + p_N=1\)</span>.</p>
<p>Fixeu-vos que:</p>
<ol class="arabic simple">
<li>Si K=2, <span class="math notranslate nohighlight">\(X_1\)</span> segueix una distribució binomial amb paràmetres <span class="math notranslate nohighlight">\(p_1, N\)</span> (i <span class="math notranslate nohighlight">\(X_2\)</span> també!)</li>
<li>La f.m.p. de qualsevol <span class="math notranslate nohighlight">\(X_i\)</span>, és també una binomial (Exercici!)</li>
</ol>



<div class="slide-no">75</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id41">

<h3>Exemple d’aplicació: modelització de documents de text</h3>

<div class="figure align-center" id="id6">
<a class="reference internal image-reference" href="../_images/multinomial_exemple.png"><img alt="../_images/multinomial_exemple.png" src="../_images/multinomial_exemple.png" style="height: 500px;" /></a>
<p class="caption"><span class="caption-text">Això es coneix com el model “Bag-of-Words” d’un document</span></p>
</div>



<div class="slide-no">76</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="funcio-de-distribucio-cumulativa-conjunta">

<h3>Funció de distribució cumulativa conjunta</h3>

<p>Som seria d’esperar, en el cas multivariat també podem definir una f.d.c conjunta. Per exemple,
per <span class="math notranslate nohighlight">\(N=2\)</span> i una v.m. <span class="math notranslate nohighlight">\((X, Y)\)</span>:</p>
<p><span class="math notranslate nohighlight">\(F_{X,Y}(x, y) = P(X \leq x, Y \leq y)\)</span></p>
<p>Cosa que en el cas continu, i en el cas que <span class="math notranslate nohighlight">\(f_{X,Y}\)</span> existeixi, implica:</p>
<p><span class="math notranslate nohighlight">\(F_{X,Y}(x, y) = \int^x_{-\infty}\int^y_{-\infty} f_{X,Y}(x',y')dx'dy'\)</span></p>
<p class="note">Les f.d.c són una mica menys útils en el cas multivariat ja que “només” ens serveixen
per calcular probabilitats d’esdeveniments “rectangulars”</p>



<div class="slide-no">77</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="funcio-de-distribucio-de-probabilitat-i-massa-condicionals">

<h3>Funció de distribució de probabilitat i massa condicionals</h3>

<p>De manera anàloga al que vam definir per a probabilitats d’esdeveniments,
podem definir f.d.p.’s o f.m.p’s condicionals.</p>
<p>Considero només el cas continu (pel cas discret la definició
és la mateixa però intercanviat <span class="math notranslate nohighlight">\(f\)</span> per <span class="math notranslate nohighlight">\(p\)</span>). Per qualsevol
<span class="math notranslate nohighlight">\(y\)</span> t.q. <span class="math notranslate nohighlight">\(f_Y(y) &gt; 0\)</span> definim la f.d.p condicional donat
<span class="math notranslate nohighlight">\(Y=y\)</span> com [Casella &amp; Berger 4.2.1 i 4.2.3]:</p>
<p><span class="math notranslate nohighlight">\(f_{X|Y=y}(x) = \frac{f_{X,Y}(x,y)}{f_Y(y)}\)</span></p>
<p>Si <span class="math notranslate nohighlight">\(X\)</span> segueix la distribució <span class="math notranslate nohighlight">\(f_{X|Y=y}(x)\)</span> direm que
<span class="math notranslate nohighlight">\(X | Y=y \sim f_{X|Y=y}\)</span> (sovint omitirem <span class="math notranslate nohighlight">\(y\)</span>)</p>
<p class="note">En realitat fixeu-vos que <span class="math notranslate nohighlight">\(f_{X|Y}(x)\)</span> és una família de distribucions:
per cada possible valor d’<span class="math notranslate nohighlight">\(Y\)</span> tenim una <span class="math notranslate nohighlight">\(f_{X|Y}(x)\)</span> diferent.</p>



<div class="slide-no">78</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="llei-de-la-probabilitat-total">

<h3>Llei de la probabilitat total</h3>

<p>La f.d.p condicional ens permet desenvolupar una expressió equivalent a
la llei de la probabilitat total que vem veure per esdeveniments [Casella &amp; BErger 1.2.11],
[diapo 9, punt (1)].</p>
<p>L’idea és expressar una marginal en funció de la condicional:</p>
<p><span class="math notranslate nohighlight">\(f_{X}(x) = \int^{\infty}_{-\infty} f_{X,Y}(x,y)dy = \int^{\infty}_{-\infty} f_{X|Y=y}(x)f_Y(y)dy\)</span></p>
<p>(una expressió similar es pot obtenir pel cas discret, remplaçant les integrals per sumes i
<span class="math notranslate nohighlight">\(f\)</span> per <span class="math notranslate nohighlight">\(p\)</span>)</p>
<p>Aquesta expressió és molt útil per caracteritzar models jeràrquics, com veurem
a continuació.</p>



<div class="slide-no">79</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="exemple-de-model-jerarquic-poisson-binomial">

<h3>Exemple de model jeràrquic: Poisson-Binomial</h3>

<div class="figure align-center" id="id7">
<a class="reference internal image-reference" href="../_images/poisson_binomial.png"><img alt="../_images/poisson_binomial.png" src="../_images/poisson_binomial.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-text">Exemple de model jeràrquic amb dos nivells: Modelem les entrades de peatons en una tenda, com un model poisson-binomial. Quina és la distribució d’<span class="math notranslate nohighlight">\(X\)</span>?</span></p>
</div>



<div class="slide-no">80</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id42">

<h3>Exemple de model jeràrquic: Poisson-Binomial (2)</h3>

<p>Gràcies a la “lei de la probabilitat total” que acabem de veure,
podem derivar la distribució d’<span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}p_{X}(k) &amp; = \sum_{n=0}^\infty p_N(n) p_{X|N=n}(k) \\
         &amp; = \sum_{n=k}^\infty \frac{\lambda^n e^{-\lambda}}{n!} {n \choose k}p^{k}\left(1 - p\right)^{n-k}\\
         &amp; = \frac{\left(\lambda p\right)^k}{k!}e^{-\lambda p}\end{split}\]</div>
<p>Per tant <span class="math notranslate nohighlight">\(X \sim \mbox{Poisson}\left(\lambdap\right)\)</span></p>
<p><strong>Exercici</strong>: demostrar com es passa de la 2a la 3a igualtat.</p>



<div class="slide-no">81</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="esperanca-condicional">

<h3>Esperança condicional</h3>

<p>Gràcies a la f.d.p. i la f.m.p. condicionals, podem definir
l’eperança condicional, que juga un rol important en estadística
com veurem tot seguit.</p>
<p><span class="math notranslate nohighlight">\(E(X|Y=y) = \int x f_{X|Y=y}(x) dx\)</span></p>
<p class="note">Noteu que a diferència de l’esperança “normal”, l’esperança condicional
és una variable aleatòria, ja que és una funció d’<span class="math notranslate nohighlight">\(Y\)</span>!</p>
<p>De fet, podem utilitzar l’esperança condicional per calcular l’esperança d’<span class="math notranslate nohighlight">\(X\)</span>,
gràcies al que a vegades s’anomena la “llei de l’esperança total” (en referència
a la llei de la probabilitat total) [Casella &amp; Berger 4.4.3]:</p>
<p><span class="math notranslate nohighlight">\(E(X) = E(E(X|Y))\)</span></p>
<p>Adoneu-vos que l’esperança “exterior” és respecte <span class="math notranslate nohighlight">\(Y\)</span>! Demostració com a exercici.</p>



<div class="slide-no">82</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>
<article class="slide level-3" id="id43">

<h3>Esperança condicional (2)</h3>

<p>Aquesta última fórmula es pot fer servir per calcular esperances que altrament
serien molt complicades. Per exemple, considereu el següent model probabilístic:</p>
<ul class="simple">
<li>Tenim <span class="math notranslate nohighlight">\(X_i\)</span> tals que <span class="math notranslate nohighlight">\(E(X_i) = \mu\)</span>, <span class="math notranslate nohighlight">\(i=1, \cdots, N\)</span></li>
<li><span class="math notranslate nohighlight">\(N\)</span> és una v.a. independent d’<span class="math notranslate nohighlight">\(X_i\)</span>, amb <span class="math notranslate nohighlight">\(E(N) = \nu\)</span></li>
<li>Volem caracteritzar <span class="math notranslate nohighlight">\(T = \sum_{i=1}^N X_i\)</span></li>
</ul>
<p>(Per exemple, <span class="math notranslate nohighlight">\(X_i\)</span> podria referir-se al gasto d’un client i <span class="math notranslate nohighlight">\(N\)</span> al
nombre de clients que entren a una web. <span class="math notranslate nohighlight">\(T\)</span> seria els ingressos totals.)</p>
<p>Gràcies a l’expressió <span class="math notranslate nohighlight">\(E(T) = E(E(T|N))\)</span>, i fent servir una propietat
que veurem tot seguit sobre v.a.’s independents, tenim:</p>
<p><span class="math notranslate nohighlight">\(E(T) = E(N)E(X) = \mu \nu\)</span>.</p>
<p>I això sense saber res d’<span class="math notranslate nohighlight">\(f_{X_1, \cdots, X_N, N}\)</span>!</p>



<div class="slide-no">83</div>


<div class="slide-footer">104392 - Modelització i Inferència - 2020</div>

</article>

</section>

<section id="slide_notes">

</section>

  </body>
</html>