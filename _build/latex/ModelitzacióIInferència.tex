%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



\title{104392 - Modelització i Inferència}
\date{Jan 15, 2021}
\release{2020.12.28}
\author{Arnau Tibau Puig}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Tema 0: Intro al curs}
\label{\detokenize{0_Intro/0_0_Intro_curs:tema-0-intro-al-curs}}\label{\detokenize{0_Intro/0_0_Intro_curs::doc}}

\section{Introducció al curs}
\label{\detokenize{0_Intro/0_0_Intro_curs:introduccio-al-curs}}

\subsection{Em presento}
\label{\detokenize{0_Intro/0_0_Intro_curs:em-presento}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{arnau_pic}.jpg}
\caption{\sphinxhref{mailto:arnau.tibau@uab.cat}{arnau.tibau@uab.cat} | \sphinxhref{https://twitter.com/ArnauTibau}{@ArnauTibau}}\label{\detokenize{0_Intro/0_0_Intro_curs:id1}}\end{figure}
\begin{itemize}
\item {} 
PhD en EECS, U of Michigan, Ann Arbor

\item {} 
Cap de Data Science a \sphinxhref{https://www.letgo.com/}{letgo}

\item {} 
10 anys experiència com a Científic de Dades (\sphinxstyleemphasis{Data Scientist})

\item {} 
Qualsevol pregunta: contacteu\sphinxhyphen{}me per email o pel Campus Virtual!

\item {} 
Horaris tutoria: Dilluns i Dimecres de 15h a 17h (\sphinxstylestrong{Envieu\sphinxhyphen{}me una invitació \textasciigrave{}Calendar\textasciigrave{}!})

\end{itemize}


\subsection{Què és l’inferència estadística?}
\label{\detokenize{0_Intro/0_0_Intro_curs:que-es-l-inferencia-estadistica}}
\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=280\sphinxpxdimen]{{stat_inference}.png}
\end{figure}

“Població” aquí es refereix al concepte estadístic, que definirem en breu. Per
inferència entenem:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Inferir propietats d’un objecte (la població)

\item {} 
Deduïr validesa d’una premisa

\end{enumerate}

a partir de d’un nombre \sphinxstylestrong{finit} de mostres


\subsection{Exemples d’aplicació}
\label{\detokenize{0_Intro/0_0_Intro_curs:exemples-d-aplicacio}}
\sphinxstylestrong{Exemple 1}: Caracterització de plantes (\sphinxhref{https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x}{R. A. Fisher 1936})

Mesurem la longitud i amplada dels sèpals i els pètals de 50 exemplars de tres espècies diferents del gènere \sphinxstyleemphasis{Iris}

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=600\sphinxpxdimen]{{iris_pics}.png}\hspace*{\fill}}

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=500\sphinxpxdimen]{{iris_dataset_sample}.png}\hspace*{\fill}}
\begin{itemize}
\item {} 
Quan solen mesurar els pètals de les \sphinxstyleemphasis{Iris}?

\item {} 
Hi ha alguna relació entre la morfologia i cada espècie d’\sphinxstyleemphasis{Iris}?

\item {} 
Donades les mesures d’un especímen nou, el podríem classificar en l’espècie correcta?

\end{itemize}

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=500\sphinxpxdimen]{{iris_dataset_sample}.png}\hspace*{\fill}}
\begin{itemize}
\item {} 
Quan solen mesurar els pètals de les Iris? \(\rightarrow\) \sphinxstylestrong{Estimació}

\item {} 
Hi ha alguna relació entre la morfologia i cada espècie d’\sphinxstyleemphasis{Iris}? \(\rightarrow\) \sphinxstylestrong{Estimació}

\item {} 
Donades les mesures d’un especímen nou, el podríem classificar en l’espècie correcta? \(\rightarrow\)  \sphinxstylestrong{Predicció}

\end{itemize}

\sphinxstylestrong{Exemple 2}: Eficacitat d’un tractament profilàctic per als contactes de casos COVID\sphinxhyphen{}19
(\sphinxhref{https://www.medrxiv.org/content/10.1101/2020.07.20.20157651v1}{O. Mitjà et al. 2020})

Seleccionem \sphinxstylestrong{aleatòriament} (en realitat l’experiment fa
\sphinxhref{https://en.wikipedia.org/wiki/Cluster\_randomised\_controlled\_trial}{cluster\sphinxhyphen{}randomization})
dos grups de pacients de COVID\sphinxhyphen{}19:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=325\sphinxpxdimen]{{hcq_example}.png}
\end{figure}

Al cap de 14 dies, contem quants individus en cada grup  tenen símptomes \sphinxstylestrong{i} dónen
positiu en una prova PCR.
\begin{itemize}
\item {} 
Quants pacients hem de seleccionar per prendre una decisió sobre la població general? \(\rightarrow\) \sphinxstylestrong{Mostreig}

\item {} 
Com sabem si hem seleccionat els grups adequadament? \(\rightarrow\)  \sphinxstylestrong{Mostreig, Estimació}

\item {} 
Com determinem si el tractament funciona? \(\rightarrow\)  \sphinxstylestrong{Tests d’hipòtesi}, \sphinxstylestrong{Intervals de confiança}

\end{itemize}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{mitja_et_al_resultats}.png}
\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=325\sphinxpxdimen]{{abc_hcq}.png}
\caption{Un assaig clínic busca falsificar l’hipòtesi nul.la: que el tractament no té efecte. No trobar evidència contra la mateixa no és un fracàs, és progrés científic! \sphinxhref{https://www.abc.es/espana/catalunya/abci-fracasa-ensayo-oriol-mitja-hidroxicloroquina-no-previene-coronavirus-202006121016\_noticia.html}{Font}}\label{\detokenize{0_Intro/0_0_Intro_curs:id2}}\end{figure}

\sphinxstylestrong{Exemple 3}: Tenim una plataforma de vídeo en streaming i volem millorar les nostres recomanacions.
En particular volem saber si, donat un usuari i un producte, a l’usuari li agradarà.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{movielens}.png}
\end{figure}

Disposem d’un històric amb les \sphinxhref{http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html}{següents dades}:
\begin{itemize}
\item {} 
Les evaluacions (0\sphinxhyphen{}5 estrelles) que cada usuari ha fet de les películes que ja ha vist

\item {} 
Informació sobre cada película (ex: gènere, actors)

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{install.packages}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{recommenderlab\PYGZsq{}}\PYG{p}{)}
\PYG{n+nf}{library}\PYG{p}{(}\PYG{n}{recommenderlab}\PYG{p}{)}
\PYG{n+nf}{data}\PYG{p}{(}\PYG{n}{MovieLense}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Veure primeres 3 evaluacions d\PYGZsq{}un dels usuaris}
\PYG{n+nf}{head}\PYG{p}{(}\PYG{n+nf}{as}\PYG{p}{(}\PYG{n}{MovieLense}\PYG{n}{[1}\PYG{p}{,}\PYG{n}{]}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{list\PYGZdq{}}\PYG{p}{)}\PYG{n}{[}\PYG{n}{[1}\PYG{n}{]}\PYG{n}{]}\PYG{p}{,} \PYG{l+m}{3}\PYG{p}{)}
\PYG{o}{\PYGZgt{}} \PYG{n}{Toy} \PYG{n+nf}{Story }\PYG{p}{(}\PYG{l+m}{1995}\PYG{p}{)}  \PYG{n+nf}{GoldenEye }\PYG{p}{(}\PYG{l+m}{1995}\PYG{p}{)} \PYG{n}{Four} \PYG{n+nf}{Rooms }\PYG{p}{(}\PYG{l+m}{1995}\PYG{p}{)}
\PYG{o}{\PYGZgt{}}        \PYG{l+m}{5}                 \PYG{l+m}{3}                 \PYG{l+m}{4}
\PYG{n}{MovieLenseMeta}\PYG{n}{[MovieLenseMeta}\PYG{o}{\PYGZdl{}}\PYG{n}{title}\PYG{o}{==}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Toy Story (1995)\PYGZdq{}}\PYG{p}{,} \PYG{n}{]}
\PYG{o}{\PYGZgt{}}      \PYG{n}{title}         \PYG{n}{year}                \PYG{n}{url}                                       \PYG{n}{unknown} \PYG{n}{Action} \PYG{n}{Adventure} \PYG{n}{....}
\PYG{o}{\PYGZgt{}} \PYG{l+m}{1} \PYG{n}{Toy} \PYG{n+nf}{Story }\PYG{p}{(}\PYG{l+m}{1995}\PYG{p}{)} \PYG{l+m}{1995} \PYG{n}{http}\PYG{o}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{us.imdb.com}\PYG{o}{/}\PYG{n}{M}\PYG{o}{/}\PYG{n}{title}\PYG{o}{\PYGZhy{}}\PYG{n}{exact}\PYG{o}{?}\PYG{n}{Toy}\PYG{o}{\PYGZpc{}20Story\PYGZpc{}}\PYG{l+m}{20}\PYG{p}{(}\PYG{l+m}{1995}\PYG{p}{)}       \PYG{l+m}{0}      \PYG{l+m}{0}         \PYG{l+m}{0}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
Donada una película que l’usuari encara no ha vist, podem determinar si li agradarà o no? \(\rightarrow\)  \sphinxstylestrong{Regressió}

\item {} 
Si implementem un model estadístic de recomanacions de pel.lícules, com podem saber si millora l’experiència dels nostres usuaris o no? \(\rightarrow\)  \sphinxstylestrong{Mostreig, Tests d’hipòtesi, Estimació}

\end{itemize}


\subsection{Organització del curs}
\label{\detokenize{0_Intro/0_0_Intro_curs:organitzacio-del-curs}}
\sphinxstylestrong{Classes de teoria}: Estadística matemàtica
\begin{itemize}
\item {} 
Rigor matemàtic: el just

\item {} 
Centrades en l’aplicació i l’aspecte computacional

\item {} 
Després de cada classe, penjaré les transparències/notes de classe al Campus Virtual

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
Es recomana la lectura \sphinxstylestrong{prèvia} dels temes a tractar abans de cada llicó
\end{sphinxadmonition}

\sphinxstylestrong{Classes de problemes}: Resolució de problemes proposats per l’estudi autònom
\begin{itemize}
\item {} 
Complement a les classes teòriques

\item {} 
Molt difícil (veure impossible) resoldre els problemes sense seguir les classes

\item {} 
Molt difícil aprovar sense treballar els problemes

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
Part de la teoria s’exposarà en els problemes
\end{sphinxadmonition}

\sphinxstylestrong{Classes pràctiques} amb programari (R/Python)
\begin{itemize}
\item {} 
Complement a les classes teòriques

\item {} 
Implementarem/experimentarem amb els mètodes descrits a classe

\item {} 
Instal.leu\sphinxhyphen{}vos \sphinxhref{https://rstudio.com/products/rstudio/}{Rstudio} i/o \sphinxhref{https://www.python.org/downloads/}{Python 3+} (si feu servir Python, us recomano que instal.leu també \sphinxhref{https://docs.conda.io/en/latest/miniconda.html}{Miniconda} per gestionar\sphinxhyphen{}ne els paquets)

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
Per ser evaluat, el programari entregat haurà de córrer sense modificacions al meu ordinador.
\end{sphinxadmonition}
\begin{itemize}
\item {} 
Com ja sabeu, hem de mantenir l’ocupació de l’aula a 32

\item {} 
Mantindrem l’alternança entre Grup 1, Grup 2 i virtual (\sphinxhref{https://www.uab.cat/doc/hor-grau-MatCAD-20-21}{calendari})

\item {} 
Si m’ajudeu podem mirar de gravar les classes per penjar\sphinxhyphen{}les al Campus Virtual

\item {} 
Haurem de ser una mica flexibles

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=250\sphinxpxdimen]{{flexibilitat}.jpg}
\caption{\sphinxhref{https://www.centralyoga.com.au/blog/beware-the-allure-of-extreme-poses}{Font de l’imatge}}\label{\detokenize{0_Intro/0_0_Intro_curs:id3}}\end{figure}

Hem estructurat el curs en 5 parts:
\begin{itemize}
\item {} 
Tema 1. Preliminars (Setmanes 1 i 2)

\item {} 
Tema 2. Introducció a l’Inferència Estadística (Setmana 2, 3 i 4)

\item {} 
Tema 3. Estimació (Octubre)

\item {} 
Tema 4. Tests d’hipòtesi (Novembre)

\item {} 
Tema 5. Regressió i predicció (Desembre)

\end{itemize}

Per més detalls sobre el contingut de cada tema, consulteu la \sphinxhref{https://e-aules.uab.cat/2020-21/course/view.php?id=7622\#section-0}{guia docent penajda al Campus Virtual}


\subsection{Bibliografia recomanada}
\label{\detokenize{0_Intro/0_0_Intro_curs:bibliografia-recomanada}}
Pel desenvolupament teòric, seguiré majoritàriament \sphinxstyleemphasis{{[}Casella \& Berger{]}},
excepte pel \sphinxtitleref{Tema 4. Tests d’hipòtesi}, on seguiré més aviat el desenvolupament
de \sphinxstyleemphasis{{[}Rice{]}}:
\begin{itemize}
\item {} 
\sphinxstyleemphasis{{[}Casella \& Berger{]}} \sphinxhref{https://cataleg.uab.cat/iii/encore/record/C\_\_Rb1522633\_\_Sstatistical\%20inference\_\_Orightresult\_\_U\_\_X7?lang=cat\&suite=def}{Statistical Inference, 2nd Edition}

\item {} 
\sphinxstyleemphasis{{[}Rice{]}} \sphinxhref{https://cataleg.uab.cat/iii/encore/record/C\_\_Rb1953551\_\_SMathematical\%20Statistics\%20and\%20Data\%20Analysis\_\_Orightresult\_\_U\_\_X7?lang=cat\&suite=def}{Mathematical Statistics and Data Analysis, J. Rice, 3rd edition}

\end{itemize}

Bibliografia complementària:
\begin{itemize}
\item {} 
{[}Efron \& Hastie{]} \sphinxhref{https://web.stanford.edu/~hastie/CASI/index.html}{Computer Age Statistical Inference}

\end{itemize}


\subsection{Avaluació curs}
\label{\detokenize{0_Intro/0_0_Intro_curs:avaluacio-curs}}
Avaluació continuada:
\begin{itemize}
\item {} 
\(C\): Nota mitjana del control de problemes

\item {} 
\(P\): Control de pràctiques

\end{itemize}

Exàmens:
\begin{itemize}
\item {} 
\(E_1\): Nota examen final

\item {} 
\(E_2\): Nota examen de recuperació

\end{itemize}

Si l’alumne es presenta a un dels dos examens:

\(N = 0.50 \times \max\left(E_1, E_2\right) + 0.20  \times C + 0.30 × P\)

(\(C, P, E_1, E_2, N \in \left[0, 10\right]\)) i aprova si \(N \geq 5\).

Si l’alumne no es presenta a cap dels dos examens:

\(N = \mbox{No Presentat}\) (independentment de \(C\) i \(P\))
\begin{itemize}
\item {} 
Alguns dels problemes els proposaré durant les classes de teoria (exemple: completar una demostració).

\item {} 
En cualsevol cas, miraré de donar sempre un mínim de 5 dies entre l’enunciat dels problemes i l’entrega/correcció de sol.lucions.

\item {} 
Alguns dels problemes els corregirem a classe (després d’entregar\sphinxhyphen{}los).

\item {} 
Ho sento però per qüestions logístiques no acceptaré entregues amb retard.

\end{itemize}

Ho repeteixo, perquè quedi clar:
\begin{itemize}
\item {} 
Tant les sessions de problemes com les de pràctiques són complementàries a les classes teòriques

\item {} 
/!\textbackslash{} part de la teoria s’exposarà en els problemes i les pràctiques

\item {} 
Molt difícil (veure impossible) resoldre els problemes sense seguir les classes

\item {} 
Molt difícil aprovar (\sphinxtitleref{aprendre!}) sense treballar els problemes

\item {} 
\sphinxstylestrong{IMPORTANT}: Per ser evaluat, el programari de cada pràctica haurà de córrer sense modificacions al meu ordinador.

\end{itemize}


\subsection{Algunes coses a tenir en compte}
\label{\detokenize{0_Intro/0_0_Intro_curs:algunes-coses-a-tenir-en-compte}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{we_need_you}.jpg}
\caption{Aquest curs és la primera vegada que el preparo!}\label{\detokenize{0_Intro/0_0_Intro_curs:id4}}
\begin{sphinxlegend}\begin{itemize}
\item {} 
Pot haver\sphinxhyphen{}hi alguna errada a les slides/apunts

\item {} 
En particular en la traducció de termes en anglès \textless{}\textendash{}\textgreater{} català

\item {} 
Qualsevol problema durant o després de la classe: \sphinxhref{mailto:arnau.tibau@uab.cat}{arnau.tibau@uab.cat}

\end{itemize}
\end{sphinxlegend}
\end{figure}


\subsection{Qüestionari}
\label{\detokenize{0_Intro/0_0_Intro_curs:questionari}}
Durant el curs faré servir qüestionaris i exercicis per ajudar\sphinxhyphen{}vos a reflexionar i recordar
el material que anem desenvolupant.

Comencem amb el primer :)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Calcular el nombre de pacients necessaris per un assaig clínic és un problema de \_\_\_\_\_\_\_\_\_.

\item {} 
Es podràn resoldre els problemes sense assistir a les classes de teoria?

\item {} 
Per determinar si un tractament clínic funciona, normalment es fa servir la tècnica dels \_\_\_\_\_\_\_\_\_\_\_\_\_.

\item {} 
Si trec un 10 en l’avaluació de problemes i pràctiques i no em presento a l’exàmen final, puc aprovar?

\item {} 
Perquè una pràctica s’evalui, m’he d’assegurar que el meu codi \_\_\_\_\_\_\_\_\_\_\_.

\end{enumerate}


\chapter{Tema 1: Repàs de probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:tema-1-repas-de-probabilitat}}\label{\detokenize{0_Intro/0_1_Repas_probabilitat::doc}}

\section{Espais i mesures de Probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:espais-i-mesures-de-probabilitat}}

\subsection{Espai de Probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:espai-de-probabilitat}}
Durant el Tema 1 haurem d’anar una mica ràpid. És impossible fer un curs de probabilitat
en 2 setmanes, però per sort ja n’heu fet un!

Un \sphinxstylestrong{espai de probabilitat} és un model matemàtic del resultat d’un \sphinxstylestrong{experiment aleatori}.

Consisteix en un triplet \(\left(\Omega, \mathcal{A}, P\right)\):
\begin{itemize}
\item {} 
\(\Omega\): l’\sphinxstylestrong{espai mostral}, conjunt de resultats possibles d’un experiment

\item {} 
\(\mathcal{A} \subseteq 2^{\Omega}\): el conjunt d’\sphinxstylestrong{esdeveniments}, una família de subconjunts d’\(\Omega\)

\item {} 
\(P\): una \sphinxstylestrong{mesura de probabilitat}, una funció \(\mathcal{A} \rightarrow \left[0, 1\right]\)

\end{itemize}

\sphinxstyleemphasis{Recordatori}: \(2^{\Omega}\) és el conjunt de tots els sub\sphinxhyphen{}conjunts d’\(\Omega\), incloent\sphinxhyphen{}hi \(\emptyset\) i \(\Omega\).


\subsection{Mesura de probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:mesura-de-probabilitat}}
Una \sphinxstylestrong{mesura de probabilitat} \(P: \mathcal{A} \rightarrow \left[0, 1\right]\)
ha de satisfer els següents axiomes (de Kolmogorov):
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(P\left(\Omega\right)=1\)

\item {} 
\(\forall A\in\mathcal{A}\), \(P\left(A\right)\geq 0\)

\item {} 
Per \(A_1,A_2,A_3, \cdots \in \mathcal{A}\) disjunts, \(P\left(\cup_i A_i\right) = \sum_i P\left(A_i\right)\)

\end{enumerate}

Fixeu\sphinxhyphen{}vos que tenim llibertat a l’hora de definir \(\mathcal{A}\) pels esdeveniments que ens
interessen (sempre i quan formin una \(\sigma\)\sphinxhyphen{}àlgebra.)

Això és una construcció axiomàtica de Probabilitat, formalitzada per Andrey Kolmogorov.

Noteu que no hem associat cap interpretació al significat físic dels valors de \(P\). Dues interpretacions típiques:
\begin{itemize}
\item {} 
\sphinxstylestrong{Frequentista}: \(P\left(A\right)\) representa la frequència amb que observariem l’esdeveniment \sphinxtitleref{A} si realitzéssim un gran nombre d’experiments

\item {} 
\sphinxstylestrong{Bayesiana}: \(P\left(A\right)\) representa la nostra certesa sobre l’ocurrència de l’esdeveniment \sphinxtitleref{A}

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
Les dues interpretacions no són completament ortogonals, però són l’orígen d’un munt de
discussions filosòfiques i a vegades dogmàtiques. Si us interessa el tema us recomano
\sphinxhref{https://projecteuclid.org/euclid.ba/1340370429}{Objections to Bayesian statistics}.
\end{sphinxadmonition}

Aquest no és un curs de probabilitat, per tant amagarem “detalls” important sota l’alfombra:
\begin{itemize}
\item {} 
\(\mathcal{A}\) en realitat ha de ser una \(\sigma\)\sphinxhyphen{}àlgebra (conté \(\emptyset\), tancat per unió contable i complement)

\item {} 
Per a conjunts \(\Omega\) contables, podem tirar milles considerant \(\mathcal{A} = 2^{\Omega}\)

\item {} 
La cosa es complica quan \(\Omega\) no és discret (exemples: l’alçada d’una població, el nivell d’expressió d’un gen)

\end{itemize}

\sphinxstylestrong{Recomano} donar una ullada al {[}Casella \& Berger{]} o a una altra de les referències
bibliogràfiques per una intro no tècnica a les \(\sigma\)\sphinxhyphen{}àlgebres


\subsection{Algunes propietats de les mesures de probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:algunes-propietats-de-les-mesures-de-probabilitat}}
\sphinxstylestrong{Teorema {[}Casella \& Berger 1.2.8 i 1.2.9{]}} Per una mesura de probabilitat \(P\) i
qualsevol esdeveniments \(A, B \in \mathcal{A}\), tenim:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(P\left(\emptyset\right)=0\)

\item {} 
\(P\left(A\right) \leq 1\)

\item {} 
\(P\left(A^c\right) = 1 - P\left(A\right)\)

\item {} 
\(P\left(B \cap A^c\right) = P\left(B\right) - P\left(A \cap B\right)\)

\item {} 
\(P\left(A \cup B\right) = P\left(A\right) + P\left(B\right) - P\left(A \cap B\right)\)

\item {} 
Si \(A \subseteq B\), aleshores \(P\left(A\right) \leq P\left(B\right)\)

\end{enumerate}

\sphinxstylestrong{Demostració}: Punts (1), (2), (3), exercici :) (recomano començar pel 3er punt).
Punts (4)\sphinxhyphen{}(6) tot seguit.

Pel punt (4), només cal observar que \(B = \left(B \cap A\right) \cup \left(B \cap A^c\right)\) (exercici).
D’aquesta identitat i tenint en compte que \(B \cap A\) i \(B \cap A^c\) son disjunts,
s’en dedueix l’expressió usant el 3er axioma de Kolmogorov.

Pel punt (5), utilitzem la següent identitat \(A \cup B = A \cup \left(B \cap A^c\right)\) i apliquem el punt (4).

Finalment el punt (6) el demostrem observant que si \(A \subseteq B\) aleshores \(A \cap B = A\)
i que \(0 \leq P\left(B \cap A^c\right) = P\left(B\right) - P\left(A\right)\).

Els següents són propietats interessants relatives a col.leccions de conjunts:

\sphinxstylestrong{Teorema {[}Casella \& Berger 1.2.11{]}} Si \(P\) és una mesura de probabilitat:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Per cualsevol partició \(C_1, \cdots, C_N\) d’ \(\Omega\), \(P\left(A\right) = \sum_i P\left(A \cap C_i \right)\)

\item {} 
\(A_1, A_2 \cdots, \in \mathcal{A}\), \(P\left(\cup_i A_i\right) \leq \sum_i P\left(A_i \right)\) (desigualtat de Boole)

\end{enumerate}

\sphinxstylestrong{Demostració}: (1) tot seguit, (2) exercici.

Demostració punt (1): Recordem que una partició \(C_1, \cdots, C_N\) d’ \(\Omega\)
és una col.lecció de conjunts tal que \(\cup_i C_i = \Omega\) i \(C_i \cap C_j = \emptyset, \forall i\neq j\).

Tenim doncs la següent cadena d’identitats:
\begin{equation*}
\begin{split}A &= A \cap \Omega \\
A & = A \cap \cup_i C_i \\
A & = \cup_i \left( A \cap C_i \right)\\
P\left(A\right) & = P\left(\cup_i \left( A \cap C_i\right)\right)\end{split}
\end{equation*}
i com que \(A \cap C_i\) i \(A \cap C_j\) son disjunts, el resultat
s’obté considerant el 3er axioma de Kolmogorov.


\subsection{Exemples d’espais de probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:exemples-d-espais-de-probabilitat}}
\sphinxstylestrong{Experiment 1}: Modelar el resultat de llançar un dau de 6 cares
\begin{itemize}
\item {} 
\(\Omega = \left\{1, 2, 3, 4, 5, 6\right\}\)

\item {} 
\(\mathcal{A} = \left\{ \left\{1\right\}, \left\{2\right\}, \cdots, \left\{1, 2\right\}, \cdots, \emptyset, \Omega \right\}\)

\item {} 
\(P\left(x\right) = \frac{1}{6}, x \in \Omega\)

\end{itemize}

\sphinxstylestrong{Exercici}: Com definirieu \(P\left(A\right)\) per a qualsevol \(A \in \mathcal{A}\)?
\begin{itemize}
\item {} 
Resposta: \(P\left(A\right) = \sum_{x \in A} P\left(x\right)\). Podeu comprovar que aquesta construcció satisfà els axiomes.

\end{itemize}

\sphinxstylestrong{Experiment 2}: Escollir 100 persones i fer\sphinxhyphen{}els\sphinxhyphen{}hi una prova d’anticossos per SARS\sphinxhyphen{}COV\sphinxhyphen{}2
\begin{itemize}
\item {} 
\(\Omega = \left\{+, -\right\}^{100}\)

\item {} 
\(\mathcal{A} = ?\)

\item {} 
\(P\left(A\right) = ?\)

\end{itemize}

\sphinxstylestrong{Experiment 3}: Escollir aleatòriament un estudiant d’aquesta classe i mesurar\sphinxhyphen{}ne la seva alçada
\begin{itemize}
\item {} 
\(\Omega = \left[0, \infty \right)\)

\item {} 
\(\mathcal{A} = ?\)

\item {} 
\(P\left(A\right) = ?\)

\end{itemize}


\subsection{Qüestionari de repàs}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:questionari-de-repas}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Un espai de probabilitat és el triplet d’un \_\_\_\_\_\_\_\_\_\_\_\_\_\_, un \_\_\_\_\_\_\_\_\_\_\_\_\_\_ i una \_\_\_\_\_\_\_\_\_\_\_\_\_.

\item {} 
Quina dels següents assercions \sphinxstylestrong{no} és un axioma de Kolmogorov:

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\alph}{enumi}{enumii}{}{.}%
\item {} 
Si \(A \cap B = \emptyset\), \(P\left(A \cup B \right) = P\left(A \right) + P\left( B \right)\)

\item {} 
\(P\left(A\right) \leq 1, \forall A \in \mathcal{A}\)

\item {} 
\(P\left(A\right) \geq 0, \forall A \in \mathcal{A}\)

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
Quin és l’\(\Omega\) i l’\(\mathcal{A}\) del següent experiment: \sphinxstyleemphasis{mesurar la vida útil en dies dels ordinadors Macbook Pro.}

\item {} 
Quin és l’\(\Omega\) i l’\(\mathcal{A}\) de l’experiment: \sphinxstyleemphasis{llençar un dau fins que treiem un 6.}

\end{enumerate}


\section{Independència i probabilitat condicional}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:independencia-i-probabilitat-condicional}}

\subsection{Probabilitat condicional}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:probabilitat-condicional}}
Donats \(A, B \in \mathcal{A}\), amb \(P\left(B\right) > 0\),
\(P\left(A|B\right) = \frac{P\left(A \cap B\right)}{P\left(B\right)}\) (aquesta construcció satisfà els axiomes de Kolmogorov)

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{proba_condicional}.png}
\end{figure}

\(P\left(\cdot|B\right)\) és la restricció de \(P\) al subconjunt d’esdeveniments B. Alguns preguntes/petits exercicis interessants:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Tindria sentit definir \(P\left(A|B\right)\) si \(P\left(B\right) = 0\)?

\item {} 
Si \(A \cap B = \emptyset\), \(P\left(A|B\right)\)?

\item {} 
Com podem interpretar si \(P\left(A|B\right) =P\left(A\right)\)? Podeu donar un exemple “físic”?

\item {} 
Si \(A \subseteq B\), quina relació hi ha entre \(P\left(A|B\right)\) i \(P\left(A\right)\)?

\end{enumerate}


\subsection{Esdeveniments independents}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:esdeveniments-independents}}
Diem que \(A, B \in \mathcal{A}\), són independents si:

\(P\left(A \cap B\right) =P\left(A\right)P\left(B\right)\)

Això és equivalent a \(P\left(A|B\right) =P\left(A\right)\) si \(P\left(B\right) > 0\).

Algunes preguntes {[}Casella \& Berger Teorema 1.3.9{]} (mirem de respondre per intució primer i matemàticament després):
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Si \(A, B \in \mathcal{A}\) son independents, què podem dir de \(A, B^c\)?

\item {} 
Si \(A, B \in \mathcal{A}\) son independents, què podem dir de \(A^c, B^c\)?

\end{enumerate}

Per exemple, l’independència conjunta no implica independència de parells:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{ex_independencia}.png}
\end{figure}

Calculem \(P\left(A \cap B \cap C\right)\) i \(P\left(B \cap C\right)\)…

\sphinxstyleemphasis{Nota:} l’independència de parells tampoc implica independència mútua (veure Problema)

Per resoldre aquests problemes, fa falta una definició molt més estricta
de la noció d’independència en conjunts d’esdeveniments:

\sphinxstylestrong{Definició} \(A_1, A_2 \cdots, \in \mathcal{A}\) són mutualment independents si per cualsevol
subcol.lecció \(A_{i_1}, A_{i_2} \cdots, \in \mathcal{A}\), tenim que \(P\left(\cap_j A_{i_j}\right) = \Pi_j P\left(A_{i_j}\right)\)

(En aquest curs, quan parlem de mostres independents, estarem assumint independència mútua)


\section{Variables aleatòries i funcions de distribució}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:variables-aleatories-i-funcions-de-distribucio}}

\subsection{Variable aleatòria}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:variable-aleatoria}}
\sphinxstylestrong{Definició} Una variable aleatòria (\sphinxstyleemphasis{v.a.} pels amics) és una funció \(X : \Omega \to \mathcal{X} \subseteq \mathbb{R}\).

Podem doncs definir una funció de probabilitat {[}Casella \& Berger 1.4.2{]}:

\(P_X\left(X \in A\right) = P\left(\left\{s\in \Omega: X\left(s\right) \in A \right\}\right)\)

que satisfà els axiomes de Kolmogorov. Aquesta definició es pot especialitzar
quan \(\Omega, \mathcal{X}\) són contables:

\(P_X\left(X \in A\right) = \sum_{s\in \Omega: X\left(s\right) \in A } P\left(s\right)\)

Enlloc de treballar amb \(P_X\left(X \in A\right)\), en general caracteritzarem les v.a. a través de les seves funcions de distribució, de massa o de densitat.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{v.a}.png}
\caption{Diagrama explicatiu de la identitat \(P_X\left(X \in A\right) = P\left(\left\{s\in \Omega: X\left(s\right) \in A \right\}\right)\).
Podem caracteritzar l’esdeveniment \(X \in A\) relatiu a una v.a. \(X\) en funció de l’esdeveniment \(\left\{s\in \Omega: X\left(s\right) \in A \right\}\)
en l’espai mostral d’orígen. En aquest curs no ho tindrem en compte, però en realitat
no totes les funcions \(X : \Omega \to \mathcal{X} \subseteq \mathbb{R}\) són admissibles,
només les \sphinxhref{https://en.wikipedia.org/wiki/Measurable\_function}{mesurables}.}\label{\detokenize{0_Intro/0_1_Repas_probabilitat:id4}}\end{figure}

Per entendre un concepte, sempre va bé intentar reflexionar primer sobre
els casos més extremadament simples.
\begin{itemize}
\item {} 
Q:\sphinxstyleemphasis{Quina seria la v.a. més simple?}

\item {} 
R: La v.a. constant, definida com \(X : \Omega \to 0\)

\item {} 
Q: \sphinxstyleemphasis{I la 2a més simple?}

\item {} 
R: La v.a. de Bernouilli, definida com \(X : \Omega \to \left\{0, 1\right\}\)

\end{itemize}

Aplicant la definició anterior, tenim que la v.a. de Bernouilli està completament
caracterizada per un sol paràmetre \(p = P\left(\left\{s\in \Omega: X\left(s\right) = 1\right\}\right)\)

Revisitem l’\sphinxstylestrong{Experiment 2} anterior (escollim 100 persones i fem una prova d’anticossos per SARS\sphinxhyphen{}COV\sphinxhyphen{}2)
\begin{itemize}
\item {} 
Teniem que \(\Omega = \left\{+, -\right\}^{100}\)

\item {} 
Definim v.a. \(X : \left\{+, -\right\}^{100} \to \mbox{Nombre de +} \in \left[0, 100\right]\)

\end{itemize}

\sphinxstylestrong{Exercici}: Fent servir l’identitat \(P_X\left(X \in A\right) = \sum_{s\in \Omega: X\left(s\right) \in A } P\left(s\right)\), derivem \(P_X\left(X=k\right)\).

Primer determinem el conjunt \(\left\{s\in \Omega: X\left(s\right) \in A\right\}\) sobre el qual haurem de sumar:
\begin{equation*}
\begin{split}\left\{s\in \Omega: X\left(s\right) \in A\right\} &= \left\{s\in \Omega: X\left(s\right)= k\right\}\\
&= \mbox{Totes les seqüencies amb exactament k +}\end{split}
\end{equation*}
Fixeu\sphinxhyphen{}vos que hi ha \({n \choose k}\) seqüencies amb \(k\) “+” d’entre \(n=100\) individus. Per altra banda,
si assumim que cada individu és + de manera independent, tenim que cada seqüència
succeeix amb probabilitat \(p^k\left(1-p\right)^{n-k}\).

Per tant deduïm que \(P_X\left(X=k\right) = {n \choose k}p^k\left(1-p\right)^{n-k}\) (distribució binomial)

Què passa si alguns individus són membres d’una mateixa família?
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Donat un espai mostral \(\Omega\), quin seria el conjunt d’esdeveniments més “petit”?

\item {} 
Si \(A \cap B = \emptyset\), vol dir que A, B són esdeveniments independents?

\item {} 
Quin és l’espai mostral d’una v.a. \(X: \Omega \to \mathcal{X}\)?

\item {} 
Quina és la probabilitat de la seqüència {[}+,\sphinxhyphen{},+,\sphinxhyphen{}{]} si \(P\left(+\right)=0.3\) i cada esdeveniment +/\sphinxhyphen{} és mutualment independent?

\end{enumerate}

Revisitem l’\sphinxstylestrong{Experiment 3}. Escollim un estudiant d’aquesta classe i aquest cop mesurem la raó alçada/pes:
\begin{itemize}
\item {} 
\(\Omega = \left(0, \infty \right) \times \left(0, \infty \right)\)

\item {} 
\(Z: (x, y) \in \Omega \to \frac{x}{y} \in \left(0, \infty \right)\)

\item {} 
Com calculariem \(P_Z\left(Z \in A\right)\)? \sphinxstyleemphasis{Necessitarem de fer alguna suposició addicional sobre les v.a. X i Y}

\end{itemize}

En la gran majoria de problemes haurem de fer una hipòtesi sobre el model aleatori de les observacions (hipòtesi que després haurem de validar comprovant la \sphinxstyleemphasis{bondat de l’ajust})


\subsection{Funció de distribució}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcio-de-distribucio}}
\sphinxstylestrong{Definició} La funció de distribució cumulativa (f.d.c.) d’una v.a. es defineix com \(F\left(x\right) = P\left(X \leq x\right)\).

De fet qualsevol funció pot ser una f.d.c si compleix {[}Casella \& Berger Teorema 1.5.3{]}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(\lim_{x\to -\infty} F(x) = 0\) i \(\lim_{x\to \infty} F(x) = 1\)

\item {} 
\(F(x)\) és no\sphinxhyphen{}decreixent

\item {} 
\(F(x)\) és contínua per la dreta (\(\lim_{x\to x_0^+} F(x) = x_0\))

\end{enumerate}

El més important es que la f.d.c caracteritza únicament una variable aleatòria: si \(F_X = F_Y\), aleshores \(X\) i \(Y\) són idènticament distribuïdes {[}Casella \& Berger 1.5.8 i 1.5.10{]}


\subsection{Funció de massa o densitat de probabilitat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcio-de-massa-o-densitat-de-probabilitat}}
A voltes ens serà més pràctic treballar amb un altre objecte, la funció de massa de probabilitat (f.m.p.) \(p_X\) o de densitat de probabilitat (f.d.p) \(f_X\).
\begin{itemize}
\item {} 
\sphinxstylestrong{Cas discret}: \(p_X\left(k\right) = P_X\left(X=k\right)\) (noteu que \(F_X\left(x\right) = \sum_{k=-\infty}^{x}p_X\left(k\right))\))

\item {} 
\sphinxstylestrong{Cas “continu”}: La funció \(f_X\) tal que \(F_X\left(x\right) = \int_{-\infty}^x f_X\left(t\right)dt\)

\item {} 
\sphinxstylestrong{Cas “mixte”}:  No les podrem caracteritzar amb una f.m.p o una f.d.p, però recordeu que existeixen v.a. que no són discretes ni contínues!

\end{itemize}

Aquí ens desviem una mica de la notació de {[}Casella \& Berger{]} al fer servir \(p_X\) enlloc de \(f_X\) per la f.m.p.

\begin{sphinxadmonition}{warning}{Warning:}
Estem ometent molts “detalls” tècnics importants… Hi ha variables contínues per les que \(f_X\) no existeix.
\end{sphinxadmonition}

Tal i com hem fist per la f.d.c, tenim un resultat similar per la f.d.p o la f.m.p: \(f_X\left(x\right)\) (\(p_X\left(k\right)\))
és una f.d.p (f.m.p) si i només si {[}Casella \& Berger 1.6.5{]}:
\begin{enumerate}
\sphinxsetlistlabels{\alph}{enumi}{enumii}{}{.}%
\item {} 
\(f_X\left(x\right) \geq 0, \forall x\) (\(p_X\left(k\right) \geq 0, \forall k\))

\item {} 
\(\int_{\infty}^{-\infty} f_X\left(x\right)dx = 1\) (\(\sum_{\infty}^{\infty} p_X\left(k\right) = 1\))

\end{enumerate}

Per tant podem construir una f.d.p. a partir de qualsevol funció \(h\left(x\right)\) no\sphinxhyphen{}negativa, definint:

\(K = \int_{-\infty}^{\infty} h\left(x\right)dx\) (també coneguda com \sphinxstyleemphasis{funció de partició})

i \(f_X\left(x\right) = \frac{h\left(x\right)}{K}\). Això es fa servir per exemple
en uns objectes anomentats \sphinxhref{https://en.wikipedia.org/wiki/Graphical\_model}{Models Gràfics Probabilístics}.


\subsection{Exemple: funció de distribució i massa d’una v.a. geomètrica}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:exemple-funcio-de-distribucio-i-massa-d-una-v-a-geometrica}}
Considerem la variable aleatòria corresponent a l’experiment de
llançar una moneda fins que surti cara.
\begin{itemize}
\item {} 
L’espai mostral és: \(\Omega = \left\{C, XC, XXC, \cdots \right\}\)

\item {} 
Definim la v.a. \(X\) com el nombre de creus que obtenim abans de la primera cara.

\end{itemize}

Si suposem que:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Cada llançament és independent de l’altre (pregunta: podeu imaginar una situació en que no ho fos)

\item {} 
La probabilitat d’obtenir cara és \(p\)

\end{enumerate}

Podem calcular \(p_X\left(k\right)=?\)

La f.m.p és la distribució geomètrica:

\(p_X\left(k\right) = P\left(\mbox{X}\right)^{k-1}P\left(\mbox{C}\right) = \left(1-p\right)^{k-1}p\)

A partir de la qual podem calcular la f.d.c:

\(F_X\left(x\right) = \sum_{k=1}^x p_X\left(k\right) = \sum_{k=1}^x \left(1-p\right)^{k-1}p\)

utilitzant l’identitat \(\sum_{k=1}^x \rho^{x-1}=\frac{1-\rho^x}{1-\rho}\), podem arribar a:

\(F_X\left(x\right) = 1 - \left(1-p\right)^x\)

Seria interessant que comprovéssiu que \(F_X\left(x\right)\) compleix les condicions per
ser una f.d.c.

Una v.a. \(X\) és \sphinxstyleemphasis{memoryless} si:

\(P\left(X > m+n | X > m\right) = P\left(X > n \right)\)

\sphinxstyleemphasis{Exercici:} Comprovem que aquesta propietat es verifica per la \(p_X\left(k\right)\) geomètrica.
\begin{itemize}
\item {} 
L’interpretació de la propietat és interessant, per exemple, en el contexte de la loteria: No haver guanyat després de jugar 10 cops no incrementa la probabilitat que guanyem en els següents 10 cops…

\item {} 
Aquesta propietat no és tant freqüent com podria semblar.

\item {} 
Aquesta f.m.p és interessant per modelar problemes de \sphinxstyleemphasis{temps de vida}, per exemple: fallada d’un component electrònic, on la probabilitat de que falli \sphinxstylestrong{no canvia amb el temps}.

\end{itemize}


\subsection{Altres v.a. discretes}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:altres-v-a-discretes}}
A través dels exemples, fins ara ja hem vist 4 tipus de variables aleatòries discretes:
\begin{itemize}
\item {} 
Uniforme, \(X \in \left\{0, \cdots, k-1\right\}\), \(P\left(X = c \right)=\frac{1}{k}\)

\item {} 
Bernouilli, \(X \in \left\{0, 1\right\}\), \(P\left(X = 1; p \right)=p\)

\item {} 
Binomial, \(X \in \left\{0, \cdots, n\right\}\), \(P\left(X = k; p, n \right)={n\choose k}p^k\left(1-p\right)^{n-k}\)

\item {} 
Geomètrica, \(X \in \left\{1, \cdots,\right\}\), \(P\left(X = k; p \right)= p\left(1-p\right)^{k-1}\)

\end{itemize}

Exercici: podeu trobar un experiment “físic” que es correspongui a cada una de les v.a. anteriors?

Us recomano donar un cop d’ull pel vostre compte a dues distribucions famoses més,
la \sphinxstylestrong{hipergeomètrica} i la \sphinxstylestrong{binomial negativa}. Ara donarem una ullada a la de Poisson.

La distribució de Poisson es pot motivar físicament amb el següent exemple. Suposeu volem modelar \# de clients que arriben en un interval T:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{poisson_motivation}.png}
\end{figure}
\begin{itemize}
\item {} 
els intervals de temps \(\delta t_i = \frac{T}{N}, N \gg 1\), aleshores \(B_i\) és aproximadament Bernouilli(p)

\item {} 
els esdeveniments \(B_i\) són independents

\end{itemize}

Per tant \(X\) és aproximadament \(\mbox{Binomial}\left(N, p\right)\) on N és el nombre d’intervals en el periòde.

La distribució de Poisson apareix quan tenim que \(p \to 0\) i \(N \to \infty\)
mantenint el nombre mig d’arribades per interval de temps fixe, que anomenarem \(\lambda = Np\).

La f.m.p de  \(X\) és aleshores, quan \(n \to \infty\):
\begin{equation*}
\begin{split}P\left(X = k; \lambda \right) & =\frac{n!}{k!\left(n-k\right)!}\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k} \\
                     & =\frac{\lambda^k}{k!}\frac{n!}{\left(n-k\right)!}\frac{1}{n^k}\left(1 - \frac{\lambda}{n}\right)^n\left(1-\frac{\lambda}{n}\right)^{-k} \\
                     & \to \frac{\lambda^k}{k!}e^{-\lambda}\end{split}
\end{equation*}
\sphinxstyleemphasis{Exercici}: Justificar l’últim pas!


\subsection{Incís sobre les v.a. contínues}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:incis-sobre-les-v-a-continues}}
Hem vist que una variable aleatòria contínua es caracteritza per una funció
de densitat de probabilitat \(f_X\) tal que:

\(F_X\left(x\right) = \int_{-\infty}^x f_X\left(t\right)dt\)

per tant tenim que

\(P\left(a < X \leq b\right) = \int_{a}^b f_X\left(t\right)dt\)

Una conseqüència d’aquesta definició quan \(b \to a\) és el que pot semblar paradoxal:

\(P\left(X = x\right) = 0\)

Pel 3er axioma de Kolmogorov això sembla implicar que
\(P\left(a < X < b\right)\), sent la unió de tots els punts entre a i b, hauria
de ser també 0. La paradoxa es resol si recordem que el 3er axioma només contempla unions contables!


\subsection{La distribució uniforme}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:la-distribucio-uniforme}}
La f.d.p més simple es correspon amb la variable aleatòria contínua més simple, escollir
un nombre aleatori dins d’un interval \(\left[a, b\right]\):

\(f_X\left(x; a, b\right) = \left\{\begin{array}{cc} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \mbox{altrament} \end{array}\right.\)

\sphinxstylestrong{Exercicis}:
\begin{itemize}
\item {} 
Calculem la f.d.c d’una variable uniforme.

\item {} 
Doneu un exemple d’un experiment on l’uniforme és un bon model?

\item {} 
Com generarieu una variable uniforme amb un ordinador?

\end{itemize}

Irònicament, i potser contraintuïtivament, l’aleatorietat és molt difícil de generar!


\subsection{La família Gamma}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:la-familia-gamma}}
Recordeu que podem definir una f.d.p tot normalitzant qualsevol funció no\sphinxhyphen{}negativa.

Considerem la següent família de funcions, parameteritzades per \(\alpha, \beta\) doncs:

\(h\left(t\right) = \frac{t^{\alpha-1}}{\beta^{\alpha}} e^{-\frac{t}{\beta}}\)

definides per \(t\in \left[0, \infty\right)\). Es pot demostrar que per \(\alpha, \beta > 0\),

\(\Gamma\left(\alpha\right)=\int_{0}^{\infty}h\left(t\right)dt\) existeix.

Per tant definim la família distribucions \(\mbox{gamma}\left(\alpha, \beta\right)\) com:

\(f_X\left(x;\alpha, \beta\right) = \frac{1}{\Gamma\left(\alpha\right)}\frac{x^{\alpha-1}}{\beta^{\alpha}} e^{-\frac{x}{\beta}}\)

La família Gamma és important perquè permet modelar una gran varietat d’experiments i està intimament
relacionada amb altres distribucions:
\begin{itemize}
\item {} 
Distribució exponencial (si fixem \(\alpha=1\)): la cosina contínua de la f.m.p geomètrica que hem vist abans

\item {} 
Distribució de \(\chi^2_p\) (si fixem \(\alpha=p/2\) i \(\beta=2\))

\end{itemize}

La f.d.p de la Gamma per diversos valors de \(\alpha\) (k a l’imatge) i \(\beta\) (\(\theta\) a la figura) \sphinxhref{https://en.wikipedia.org/wiki/Gamma\_distribution}{{[}Font{]}}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=250\sphinxpxdimen]{{650px-Gamma_distribution_pdf.svg}.png}
\end{figure}


\subsection{La família “Normal”}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:la-familia-normal}}
\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=150\sphinxpxdimen]{{440px-Normal_Distribution_PDF.svg}.png}
\end{figure}

\(f_X\left(x ; \mu, \sigma\right) = \frac{1}{\sqrt{2\pi}} e^{-\frac{\left(x - \mu\right)^2}{\sigma^2}}\)

La distribució Normal o Gaussiana és fonamental en estadística, per múltiples raons:
\begin{itemize}
\item {} 
Apareix “naturalment” quan sumem/calculem el promig d’un gran nombre de mostres

\item {} 
És simètrica i parameteritzada per 2 paràmetres intuitius (\(\mu\) i \(\sigma\))

\item {} 
Malgrat la seva aparença intimidant, és tractable analíticament

\end{itemize}


\subsection{I què te a veure tot això amb l’estadística?}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:i-que-te-a-veure-tot-aixo-amb-l-estadistica}}
\begin{sphinxadmonition}{note}{Note:}
Com vem comentar a l’introducció al curs, l’inferència estadística és la ciència d’establir propietats
d’una població mitjantçant mostres de la mateixa.

Els models probabilístics com els que hem vist darrerament són una de les eines que farem servir
per fer aquesta feina d’inferència.
\end{sphinxadmonition}

Vegem un exemple pràctic, el de l’\sphinxstylestrong{Experiment 2} (proves d’anticossos).
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Com hem dit, l’estadística comença amb la \sphinxstylestrong{recollida de mostres} (dades), en aquest cas, realizar tests d’anticossos a 100 persones a l’atzar i anotar\sphinxhyphen{}ne el resultat

\item {} 
El segon pas en \sphinxstylestrong{estadística paramètrica} és la definició d’un model probabilístic que caracteritzi les observacions. Com hem vist abans, un model raonable és que cada una de les 100 mostres és una v.a. de Bernouilli.

\item {} 
Ara tenim una col.lecció de mostres, \(\left\{x_1, \cdots, x_{100}\right\}\), on cada \(x_i\in \left\{0, 1\right\}\), i un model: \(P_X\left(X_i=1\right) = p\). L’únic que ens falta per poder fer inferència és trobar el valor de \(p\) que millor descriu les observacions (Tema 2). Per exemple un estimador raonable seria la mitjana aritmètica \(\hat{p}=\frac{1}{100}\sum_i x_i= \frac{\mbox{N. de +}}{100}\). Posem que \(\hat{p}=0.1\).

\end{enumerate}

Amb aquest estimador, obtingut \sphinxstylestrong{només a partir de 100 mostres}, i gràcies als resultats que
veurem en els Temes 1 i 2, ja podríem deduïr propietats de la població en general:
\begin{itemize}
\item {} 
Veurem que \(\hat{p}\) és un estimador “sense biaix” de \(p\)

\item {} 
Però també veurem que la variabilitat (ex: variança) de \(\hat{p}\) decreix amb el nombre de mostres, i potser 100 són massa poques…

\item {} 
També veurem com, a partir de \(\hat{p}\), podem donar un interval de confiança sobre \(p\) (ja hem vist que \(\sum_i x_i \sim \mbox{Binomial}\left(p, 100 \right)\)…)

\end{itemize}

Però per tot això primer hem d’aprofundir més en alguns altres conceptes de probabilitat: les transformacions
de v.a., l’esperança, les distribucions conjuntes i algunes desigualtats.


\section{Funcions de variables aleatòries}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcions-de-variables-aleatories}}

\subsection{Transformacions afins}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:transformacions-afins}}
Sovint ens trobarem que el nostre experiment es pot modelar més fàcilment
com la transformació d’una v.a. \(X: \Omega \to \mathcal{X}\) mitjantçant una funció
\(g: \mathcal{X}\to\mathcal{Y}\): \(Y=g\left(X\right)\)

\begin{sphinxadmonition}{note}{Note:}
Recordem que \(\mathcal{X}\) i \(\mathcal{Y}\) denoten l’espai mostral d’\(X\)
i \(Y\), respectivament.
\end{sphinxadmonition}

Per exemple, una transformació senzilla és l’afí: \(Y = a + b X\)

En aquest cas, podem expressar la f.d.c \(F_Y\) en funció de \(F_X\):
\begin{equation*}
\begin{split}F_Y\left(y\right) &= P\left( a + b X \leq y \right) \\
                  &= P\left( X \leq \frac{y - a}{b} \right) \\
                  &= F_X\left(\frac{y - a}{b} \right)\end{split}
\end{equation*}
Exercici: Fent servir aquesta identitat, demostreu que si \(X \sim \mathcal{N}\left(\mu, \sigma\right)\), \(Y = \frac{X - \mu}{\sigma} \sim \mathcal{N}\left(0, 1\right)\)


\subsection{Cas genèric}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:cas-generic}}
Per una funció genèrica, \(g: \mathcal{X}\to\mathcal{Y}\),
no serà tan senzill caracteritzar la f.d.c de \(Y\) en funció de la d’\(X\).

Sota unes condicions tècniques relativament generals, podrem definir una funció de
probabilitat el conjunt d’esdeveniments associat a l’espai mostral \(\mathcal{Y}\) com segueix:
\begin{equation*}
\begin{split}P\left(Y \in A\right) & = P\left(\left\{x \in \mathcal{X}: g\left(x\right) \in A \right\}\right) \\
                      & = P\left(X \in g^{-1}\left(A\right)\right)\end{split}
\end{equation*}
on definim el mapa invers \(g^{-1}\left(A\right) = \left\{ x\in \mathcal{X}: g(x) \in A\right\}\) {[}Casella \& Berger 2.1.1{]}

Vegem la relació entre els tres espais mostrals mitjantçant un diagrama:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=280\sphinxpxdimen]{{transformation}.png}
\caption{\(P\left(Y \in A\right) = P\left(X \in g^{-1}\left(A\right)\right) = P\left(\left\{ s \in \Omega: X(s) \in g^{-1}\left(A\right) \right\}\right)\)}\label{\detokenize{0_Intro/0_1_Repas_probabilitat:id5}}\end{figure}

Sortosament, normalment no haurem de raonar directament sobre \(\Omega\), ja que en molts casos
podrem caracteritzar \(Y\) en base a la f.d.c d’\(X\).


\subsection{F.d.c i transformacions monòtones}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:f-d-c-i-transformacions-monotones}}
En general, la f.d.c. d’\(Y\) vé donada per l’expressió {[}Casella \& Berger 2.1.4{]}:
\begin{equation*}
\begin{split}F_Y\left(y\right) &= P\left( g\left(X\right) \leq y \right) \\
                  & = P_X\left(\left\{x \in \mathcal{X}: g\left(x\right) \leq y \right\}\right)\end{split}
\end{equation*}
En el cas que la funció \(g: \mathcal{X}\to\mathcal{Y}\) sigui monòtona stricta (creixent o decreixent),
tindrem que és injectiva i surjectiva, i per tant podem definir \(g^{-1}: \mathcal{Y}\to\mathcal{X}\)
associant un únic x a cada y. Per exemple, en el cas monòton creixent:
\begin{equation*}
\begin{split}\left\{x \in \mathcal{X}: g\left(x\right) \leq y \right\} & =  \left\{x \in \mathcal{X}: x \leq g^{-1}\left(y\right) \right\} \\\end{split}
\end{equation*}
Per tant podem simplicar l’expressió {[}Casella \& Berger 2.1.3{]}: \(F_Y\left(y\right) =F_x\left(g^{-1}\left(y\right)\right)\)


\subsection{Transformacions monòtones i diferenciables}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:transformacions-monotones-i-diferenciables}}
Si ens restringim a v.a’s contínues i a transformacions estrictament monòtones diferenciables:
\begin{equation*}
\begin{split}F_Y\left(y\right) &= F_x\left(g^{-1}\left(y\right)\right) \mbox{(g creixent)} \\
F_Y\left(y\right) &= 1 - F_x\left(g^{-1}\left(y\right)\right) \mbox{(g decreixent)}\\\end{split}
\end{equation*}
són diferenciables, i aplicant la regla de la cadena arribem al famós resultat de la “transformació per Jacobià” {[}Casella \& Berger 2.1.5{]}:
\begin{equation*}
\begin{split}f_Y\left(y\right) &= f_x\left(g^{-1}\left(y\right)\right)\left|\frac{d g^{-1}\left(y\right)}{dy} \right| \\\end{split}
\end{equation*}
\(\forall y \in \mathcal{Y} = \left\{y : \exists x\in \mathcal{X},  g(x)=y \right\}\)
on \(\mathcal{X} = \left\{x : f_X\left(x\right) > 0 \right\}\).

Veure {[}Casella \& Berger 2.1.8{]} per una extensió on la funció \(g\) és monòtona només sobre alguns intervals!

Veiem aquí un exemple de com el resultat anterior es pot extendre a transformacions
no monòtones interessants en estadística. Considerarem la distribució de la transformació (contínua i diferenciable)

\(Y = X^2\)

quan \(X \sim \mathcal{N}\left(0, 1\right)\). Observem que:
\begin{equation*}
\begin{split}F_Y\left(y\right) &= P_X\left(-\sqrt{y} \leq X \leq \sqrt{y}\right) \\
                  &= P_X\left(X \leq \sqrt{y}\right) - P_X\left(X \leq -\sqrt{y}\right) \\
                  &= F_X\left(\sqrt{y}\right) - F_X\left(-\sqrt{y}\right)\end{split}
\end{equation*}
Diferenciant i fent servir la simetria de \(F_X\left(x\right)\) respecte 0, obtenim:

\(f_y\left(y\right) = y^{-\frac{1}{2}} f_X\left(\sqrt{y}\right) = \frac{y^{-\frac{1}{2}}}{\sqrt{2\pi}} e^{-\frac{y}{2}}\),
que podem identificar amb la Gamma si fixem \(\alpha=1/2\) i \(\beta=2\), que és
la \(\chi^2_1\).


\subsection{Transformació integral}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:transformacio-integral}}
L’última transformació que veurem inspirarà un
algoritme per generar mostres de v.a. contínues amb distribucions
arbitràries (ho veurem a la primera pràctica).

{[}Casella \& Berger 2.1.10{]} Sigui \(X\) una v.a. contínua caracteritzada per \(F_X\). Aleshores
la v.a. \(Y = F_X\left(X\right)\) és uniforme entre \(\left[0, 1\right]\)

La demostració passa per la definició de la funció:

\(F_X^{-1}\left(y\right) = \left\{\begin{array}{cc} \inf \left\{x : F\left(x\right) \geq y \right\} & y \in \left(0, 1\right) \\ \infty & y=1 \\ -\infty & y = 0 \end{array}\right.\)

I observant que (compte amb el segon “=”!):

\(P\left(Y \leq y \right) = P\left(F_X\left(X\right) \leq y\right) = P\left(X \leq F_X^{-1}\left(y\right)\right) = y\)


\subsection{Qüestionari de repàs}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:id1}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Quines condicions ha de verificar una f.d.p o una f.m.p?

\item {} 
Si compro un dècim de loteria de nadal cada any, i hi ha 170M de dècims, quina és la probabilitat que no em toqui en tota la vida?

\item {} 
Les v.a. són contínues o discretes segons si la seva \(F_X\) és contínua o discreta: cert o fals?

\item {} 
Per què el fet que una v.a. contínua satisfaci \(P(X=x)=0\) no implica que \(P(a < X \leq b)=0\)?

\item {} 
La distribució de Poisson és un bona aproximació de la binomial quan \_\_\_\_\_\_ i \_\_\_\_\_\_\_\_.

\item {} 
Per demostrar el teorema de la transformació integral, hem definit \(F_X^{-1}\left(y\right) = \inf \left\{x : F\left(x\right) \geq y \right\}, y \in (0, 1)\). Perquè l’infimum i perquè el \textgreater{}=?

\item {} 
En quins 4 casos podem caracteritzar fàcilment una variable \(Y=g(X)\) en funció de la distribució de X?

\end{enumerate}


\section{Esperança i moments}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:esperanca-i-moments}}

\subsection{Esperança}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:esperanca}}
L’esperança o mitja d’una v.a. \(g\left(X\right)\) es defineix com:
\begin{itemize}
\item {} 
Cas continu: \(E\left(g\left(X\right)\right) = \int_{-\infty}^{\infty} g\left(x\right)f_X\left(x\right)dx\)

\item {} 
Cas discret: \(E\left(g\left(X\right)\right) = \sum_{k} g\left(k\right)p_X\left(k\right)\)

\end{itemize}

Com ja sabeu, l’esperança pot ser un indicador de “localització” però depèn de la dispersió
(ex: variança) de la distribució en questió…

Aquesta definició es coneix com la \sphinxhref{https://en.wikipedia.org/wiki/Law\_of\_the\_unconscious\_statistician}{Llei de l’Estadístic Inconscient},
perquè en realitat l’existència de \(E\left(g\left(X\right)\right)\) s’hauria de provar formalment.
Recordeu doncs que l’esperança no té perquè existir! L’exemple clàssic és \(g(x)=x\) i amb una distribució de Cauchy.

\sphinxstylestrong{Exercici}: Podeu imaginar una distribució on l’esperança ens pot donar una idea equivocada?

L’esperança i la mitja aritmètica són cosines germanes. La següent és una interpretació
de la mitja que pot ser útil:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=330\sphinxpxdimen]{{mitja_Esperança}.png}
\end{figure}

Per tant \(\bar{X} = \sum_{i} c_i \frac{\mbox{card}\left\{x=c_i\right\}}{N} = \sum_{i} c_i \hat{P}\left(X = c_i\right)\) (S’assembla a l’esperança, no?)

El càlcul l’esperança sol dependre una mica de la forma de la f.d.p o f.m.p
de la v.a. en questió.

Vegem com ho fariem per la f.m.p de Poisson:

\(P\left(X = k; \lambda \right) = \frac{\lambda^k}{k!}e^{-\lambda}\)

Aplicant la definició:
\begin{equation*}
\begin{split}E(X) & = \sum_{k=0}^{\infty} k \frac{\lambda^k}{k!}e^{-\lambda} \\
     & = \lambda e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{\left(k-1\right)!} \\
     & = \lambda\end{split}
\end{equation*}
On hem fet servir l’identitat \(\sum_{j=0}^{\infty} \frac{\lambda^{j}}{j!}=e^{\lambda}\)

Suposem \(X \sim U[0, 1]\) i \(Y=-\log(X)\):
\begin{itemize}
\item {} 
Podem calcular \(E(Y)\) adonant\sphinxhyphen{}nos que \(F_Y(y)=P(Y \leq y) = 1 - e^{-y}\), per tant Y es exponencial amb paràmetre \(\lambda=1\)

\item {} 
Podem calcular \(E(Y) = \int_{0}^1 -\log(x)dx=x - \log(x)|^1_0=1\)

\end{itemize}

En alguns casos podem escollir entre fer servir la definició amb \(g(x)\)
o bé calcular la f.d.p de \(Y=g(X)\) per calcular\sphinxhyphen{}ne \(E(Y)\).

La majoria de propietats de l’esperança provenen de la
linearitat de l’operador integració/suma {[}Casella \& Berger 2.2.5{]}*:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(E\left(a X + b Y + c\right) = aE\left( X\right) + b E\left( Y\right) + c\)

\item {} 
Si \(X \geq Y\), aleshores \(E\left(X\right)\geq E\left(Y\right)\)

\item {} 
Si \(a\geq X \geq b\), aleshores \(a \geq E\left(X\right)\geq b\)

\end{enumerate}

\sphinxstyleemphasis{Demostració (1) i (2) a la pissarra, (3) com exercici}

{\color{red}\bfseries{}*}Nota: En realitat, a {[}Casella \& Berger 2.2.5{]} contemplen només el cas
\(X=g_1(x)\) i \(Y=g_2(X)\). Aquí farem una mica de trampa i farem
servir ja les distribucions marginals d’\(X\) i \(Y\) que encara no hem definit
però definirem més endavant.

Tot i que podria semblar un resultat trivial, la linearitat de l’esperança és una propietat
molt útil a la pràctica. Vegem per exemple la seva aplicació en el següent problema:

{[}Rice 4.1.2 Exemple B{]} Tenim \(n\) tipus diferents de cupons, i cada cop que comprem cereals ens en donen
un dels \(n\) a l’atzar. Quans cereals haurem de comprar per aconseguir\sphinxhyphen{}los tots?
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Definim \(X_i\) nombre de compres fins que aconseguim el cupó \sphinxstyleemphasis{i}

\item {} 
El nombre total de compres \(Y=\sum_{i=1}^n X_i\)

\item {} 
Noteu que \(X_i\) es una v.a. geomètrica, amb paràmetre \(p_i = \frac{n -i + 1}{n}\)

\item {} 
Recordem que en aquest cas \(E(X_i) = \frac{1}{p_i}\)

\item {} 
Per linearitat de l’esperança \(E(Y)=\sum_{i=1}^n E(X_i) = \frac{n}{n} + \frac{n}{n-1} + \cdots + n = n\sum_{i=1}^n\frac{1}{i}\)

\end{enumerate}

{[}Rice 4.1.2 Exemple E{]} Suposeu que tenim una cartera d’inversió amb dues accions A i B de borsa
amb retorns representats per v.a.’s \(R_A\) i \(R_B\).
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Si invertim una fracció \(\pi\) del nostre capital a A, i \(1-\pi\) a B, tindrem que el retorn final serà: \(R = \pi R_1 + \left(1 - \pi\right)R_2\)

\item {} 
Per linearitat de l’esperança, \(E(R) = \pi E(R_A) + \left(1 - \pi\right)E(R_B)\)

\item {} 
Per tant l’estratègia òptima d’inversió seria \(\pi=1\) si \(E(R_A)>E(R_B)\) i \(\pi=0\) en cas contrari

\end{enumerate}

Fixeu\sphinxhyphen{}vos que una possible correlació entre \(R_A\) i \(R_B\) és irrellevant…

Clarament la gestió de carteres és més complicada que això… què creieu que falla en el nostre model?


\subsection{Moments i moments centrals}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:moments-i-moments-centrals}}
A partir de l’esperança podem definir altres quantitats caracteritzant
una v.a. {[}Casella \& Berger 2.3.1{]}. Per tot enter \(n\), definim:
\begin{itemize}
\item {} 
El moment d’ordre \(n\) com: \(\mu_n' = E(X^n)\)

\item {} 
El moment \sphinxstyleemphasis{central} d’ordre \(n\) com: \(\mu_n = E\left(\left(X - E(X)\right)^n\right)\)

\end{itemize}

(que recordem no tenen perquè existir!)

Exemples:
\begin{itemize}
\item {} 
\(\mu_0'=1\), \(\mu_1=0\)

\item {} 
La variança, \(\mbox{Var}\left(X\right) = \mu_2\), que indica la desviació d’X respecte la seva mitja

\item {} 
L’asimetria (\sphinxstyleemphasis{skewness}) \(\mu_3\) que ens indica si la cua de la f.d.p està a l’esquerra (\textless{}0) o a la dreta (\textgreater{}0) de la mitja

\end{itemize}


\subsection{L’esperança com a predictor de mínim error quadrat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:l-esperanca-com-a-predictor-de-minim-error-quadrat}}
Amb aquesta última definició podem establir una propietat
fonamental de l’esperança. Considerem el següent problema de predicció
d’una v.a. X tal que minitzem l’error de predicció {[}Casella \& Berger 2.2.6{]}:

Trobar \(\theta\) tal que \(\min_{\theta} E\left(X - \theta \right)^2\).

Observem:
\begin{equation*}
\begin{split}E\left(X - \theta \right)^2 &= E\left(X - E(X) + \left(E(X) - \theta\right)\right)^2 \\
                            &= E\left(X - E(X)\right)^2 + E\left(E(X) - \theta\right)^2 + \\
                            & + 2E\left(X - E(X)\right)E\left(E(X) - \theta\right) \\
                            &= \mbox{Var}\left(X\right) + E\left(E(X) - \theta\right)^2 \geq \mbox{Var}\left(X\right)\end{split}
\end{equation*}
(pel tercer “=”, \(E\left(X - E(X)\right)=0\).) Per tant \(\theta^*=E(X)\)!


\subsection{Funció generatriu de moments}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcio-generatriu-de-moments}}
Per una v.a. X, la funció generatriu de moments (\sphinxstyleemphasis{f.g.m}) (\sphinxstyleemphasis{Moment\sphinxhyphen{}Generating Function}) es defineix com:

\(M_X\left(t\right)=E\left(e^{tX}\right)\)

suposant que existeix per \(t\in [-\epsilon, \epsilon]\) amb \(\epsilon>0\).

En estadística, la f.g.m es fa servir majoritàriament per tres raons:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Calcular els moments d’una distribució que altrament serien molt difícils de calcular, veure {[}Casella \& Berger 2.3.7 i 2.3.8{]}, per exemple els moments d’una Gamma.

\item {} 
Per calcular distribucions de transformacions afins de v.a. {[}Casella \& Berger 2.3.15{]}

\item {} 
Per calcular la distribució de la suma de v.a. independents {[}Casella \& Berger 4.2.12{]}

\end{enumerate}

En aquest curs la farem servir més endavant per l’objectiu (3).

Per ara, només mencionar que aquesta utilitat es deriva d’un resultat
fonamental per les f.g.m’s, que és que sota algunes condicions, la f.g.m
caracteritza inequívocament una f.d.c:

{[}Casella \& Berger 2.3.11{]} Siguin \(F_X\), \(F_Y\) f.d.c’s per les quals
tots els moments existeixen. Aleshores \(M_X\left(t\right)=M_Y\left(t\right)\)
per \(t\in [-\epsilon, \epsilon]\) amb \(\epsilon>0\) implica que \(F_X = F_Y\).

És aquest resultat el que ens permetrà més endavant de calcular
una f.g.m i a partir d’aquesta desfer el camí i obtenir\sphinxhyphen{}be la f.d.c. corresponent.


\section{Desigualtats}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:desigualtats}}

\subsection{Desigualtat de Markov}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:desigualtat-de-markov}}
La desigualta de Markov estableix que, per qualsevol v.a. \(X \geq 0\),
tal que \(E(X)\) existeix, podem acotar la probabilitat de que \(X\)
excedeixi un cert valor \(t > 0\) per:

\(P(X \geq t) \leq \frac{E(X)}{t}\)

Per exemple, si fixem \(t = kE(X)\), podem acotar la proba que \(X\)
excedeixi la seva mitja per un factor k:

\(P(X \geq k E(X)) \leq \frac{1}{k}\)

\sphinxstyleemphasis{Demostració}:

Descomposar \(E(X)=\int_{-\infty}^t x f_X(x)dx + \int_t^{\infty} x f_X(x)dx\)
i observar que \(\int_{-\infty}^t x f_X(x)dx \geq 0\) i \(\int_t^{\infty} x f_X(x)dx \geq t P(X\geq t)\)


\subsection{Desigualtat de Txebitxev}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:desigualtat-de-txebitxev}}
La desigualtat de Markov és molt laxa perquè només fa servir informació sobre \(E(X)\).
La seva extensió (desigualtat de Txebitxev) ens permetrà establir cotes una mica més
útils. Sigui v.a. \(g(X) \geq 0\), tal que \(E(g(X))\) existeix, per qualsevol \(t > 0\) tenim:

\(P(g(X) \geq t) \leq \frac{E(g(X))}{t}\)

A priori això sembla calcat a la de Markov, però vegem\sphinxhyphen{}ne una aplicació:

Sigui \(X\) una v.a. amb mitja \(\mu\) i variança \(\sigma^2\). Aleshores
\(P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}\)

\sphinxstyleemphasis{Demostració}: Per demostrar Txebitxev, es segueix la mateixa idea que per Markov.
Per demostrar\sphinxhyphen{}ne aquesta aplicació, n’hi ha prou amb aplicar Txebitxev amb \(g(x) = \left(\frac{x - \mu}{\sigma}\right)^2\)


\subsection{Desigualtat de Jensen}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:desigualtat-de-jensen}}
L’última desigualtat que considerarem és la de Jensen, que sota
la seva simplicitat amaga moltíssim poder.

Sigui \(g(x)\) una funció convexa i \(X\) una v.a. tal que \(E(g(x))\)
existeix. Aleshores:

\(E(g(x)) \geq g(E(X))\)

\sphinxstyleemphasis{Aplicacions}: Moltíssimes, però dues d’immediates són les d’obtenir cotes, per exemple:
\begin{itemize}
\item {} 
\(E(X^2) \geq (E(X))^2\)

\item {} 
\(E(\frac{1}{X}) \geq \frac{1}{E(X)}\)

\end{itemize}

\sphinxstyleemphasis{Demostració}: A la pissarra


\section{Vectors aleatoris / Variables multivariades}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:vectors-aleatoris-variables-multivariades}}

\subsection{Variables multivariades}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:variables-multivariades}}
Una variable aleatòria multivariada és una extensió d’una v.a.
a múltiples dimensions:

\sphinxstylestrong{Definició} Una variable aleatòria multivariada (\sphinxstyleemphasis{v.m.} pels amics)
és una funció \(\mathbf{X} : \Omega \to \mathcal{X} \subseteq \mathbb{R}^K\).

Exemples:
\begin{itemize}
\item {} 
\sphinxstylestrong{V.m. contínua}: Mesurem l’alçada, pes i perímetre cranial dels nadons al néixer. Cada mostra es pot interpretar com una v.m. contínua de dimensió 3, que pren valors en \(\mathbb{R}^3\).

\item {} 
\sphinxstylestrong{V.m. discreta}: Agafem un document de text i en contem el nombre de vegades que apareixen \(K\) paraules d’un diccionari. El resultat és un vector discret que pren valors a \(\mathbb{N}^K\)

\end{itemize}


\subsection{Funció de densitat o de massa de probabilitat conjuntes: Cas bivariat}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcio-de-densitat-o-de-massa-de-probabilitat-conjuntes-cas-bivariat}}
De manera anàloga al que hem vist per v.a.’s, ver una v.m \((X, Y)\) també
podem definir:
\begin{itemize}
\item {} 
Una \sphinxstyleemphasis{funció de massa de probabilitat conjunta}: \(p_{X,Y}(x, y)\) si \(\mathcal{X}\) contable

\item {} 
Una \sphinxstyleemphasis{funció de distribució de probabilitat conjunta}: \(f_{X,Y}(x, y)\) si \(\mathcal{X}\) incontable

\end{itemize}

Per tal de ser vàlides aquestes funcions han de verificar:
\begin{itemize}
\item {} 
\(p_{X,Y}(x, y) \geq 0\), \(\sum_{x, y \in \mathcal{X}}p_{X,Y}(x, y)=1\)

\item {} 
\(f_{X,Y}(x, y) \geq 0\), \(\int\int_{x, y \in \mathcal{X}}f_{X,Y}(x, y)=1\)

\end{itemize}

Durant aquest repàs, presentarem els conceptes només pel cas bivariat (\(N=2\)),
l’extensió a \(K>2\) és gairebé immediata {[}Casella \& Berger 4.6{]}

La f.m.p. o la f.d.c. es poden utilitzar per caracteritzar la probabilitat
d’un esdeveniment \(A\) (omitirem el cas discret):

\(P((X, Y) \in A) = \int\int_{x, y \in A} f_{X,Y}(x,y)dx dy\)

\sphinxstyleemphasis{Exemple}: \((X,Y)\) són les coordenades d’arribada d’un dard llançat
en un tauler de radi \(r >0\).

Si suposem que sóm uniformement dolents llançant dards, podem caracteritzar la
f.d.p com una uniforme en el cercle de radi \(r\):

\(f(x,y) = \left\{\begin{array}{cc}\frac{1}{\pi}&\mbox{ si } x^2 + y^2 \leq 1 \\ 0  &\mbox{ altrament } \end{array}\right\}\)

\sphinxstyleemphasis{Exercici}: Calculeu \(P((X, Y) \in A)\) per \(A = \left\{x, y: t \leq x^2 + y^2 \leq 1\right\}\)


\subsection{Distribucions marginals}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:distribucions-marginals}}
A vegades voldrem caracteritzar només un dels components d’un vector aleatori \((X, Y)\).

Per exemple, si volem calcular \(P(X \in A_x)\).

Per fer\sphinxhyphen{}ho, farem servir el que s’anomena la f.d.p \sphinxstyleemphasis{marginal} d’X:

\(f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy\) (cas continu)

o:

\(p_X(x) = \sum_{k} f_{X,Y}(x, k) dy\) (cas discret)

\sphinxstylestrong{Exemple:} Quina és la probabilitat que el dard de l’exemple anterior caigui
en la regió \(-1\leq y \leq 1\) (\(r>1\)).

És important recordar que les marginals no contenen tota la informació que hi ha en la conjunta!


\subsection{Exemple de variable multivariada discreta: multinomial}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:exemple-de-variable-multivariada-discreta-multinomial}}
Considerem ara un exemple d’una v.m. discreta per \(K \geq 2\):
\begin{itemize}
\item {} 
Suposeu un experiment en que cada realització pren un entre \(K\) valors discret, amb probabilitats \(p_i\), \(i=1,\cdots,K\), \(\sum_i p_i = 1\).

\item {} 
Repetim l’experiment \(N\) vegades, cada realització és mutuament independent amb les altres

\item {} 
Definim la v.m. \(\mathbf{X}=\left[X_1, \cdots, X_K\right]\) com un vector on cada element \(X_i\) conta el nombre de vegades que hem observat el valor \(i\)

\end{itemize}

Aleshores la v.m \(\mathbf{X}\) segueix una distribució multinomial {[}Casella \& Berger 4.6.2{]}

\(p_{X_1, \cdots, X_K}\left(x_1, \cdots, x_K\right) = N!\Pi_{i=1}^{K}\frac{p_i^{x_i}}{x_i!}\)

És obvi que \(p_{X_1, \cdots, X_K}\left(x_1, \cdots, x_K\right)\geq 0\). Per demostrar que això és efectivament una f.m.p,
haurem d’aplicar el Teorema Binomial {[}Casella \& Berger 4.6.4{]}, que diu que

\(\left(p_1 + \cdots + p_N\right)^N = N!\Pi_{i=1}^{K}\frac{p_i^{x_i}}{x_i!}\)

i això és igual a 1 ja que \(p_1 + \cdots + p_N=1\).

Fixeu\sphinxhyphen{}vos que:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Si K=2, \(X_1\) segueix una distribució binomial amb paràmetres \(p_1, N\) (i \(X_2\) també!)

\item {} 
La f.m.p. de qualsevol \(X_i\), és també una binomial (Exercici!)

\end{enumerate}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=500\sphinxpxdimen]{{multinomial_exemple}.png}
\caption{Això es coneix com el model “Bag\sphinxhyphen{}of\sphinxhyphen{}Words” d’un document}\label{\detokenize{0_Intro/0_1_Repas_probabilitat:id6}}\end{figure}


\subsection{Funció de distribució cumulativa conjunta}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcio-de-distribucio-cumulativa-conjunta}}
Som seria d’esperar, en el cas multivariat també podem definir una f.d.c conjunta. Per exemple,
per \(N=2\) i una v.m. \((X, Y)\):

\(F_{X,Y}(x, y) = P(X \leq x, Y \leq y)\)

Cosa que en el cas continu, i en el cas que \(f_{X,Y}\) existeixi, implica:

\(F_{X,Y}(x, y) = \int^x_{-\infty}\int^y_{-\infty} f_{X,Y}(x',y')dx'dy'\)

Les f.d.c són una mica menys útils en el cas multivariat ja que “només” ens serveixen
per calcular probabilitats d’esdeveniments “rectangulars”


\subsection{Funció de distribució de probabilitat i massa condicionals}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:funcio-de-distribucio-de-probabilitat-i-massa-condicionals}}
De manera anàloga al que vam definir per a probabilitats d’esdeveniments,
podem definir f.d.p.’s o f.m.p’s condicionals.

Considero només el cas continu (pel cas discret la definició
és la mateixa però intercanviat \(f\) per \(p\)). Per qualsevol
\(y\) t.q. \(f_Y(y) > 0\) definim la f.d.p condicional donat
\(Y=y\) com {[}Casella \& Berger 4.2.1 i 4.2.3{]}:

\(f_{X|Y=y}(x) = \frac{f_{X,Y}(x,y)}{f_Y(y)}\)

Si \(X\) segueix la distribució \(f_{X|Y=y}(x)\) direm que
\(X | Y=y \sim f_{X|Y=y}\) (sovint omitirem \(y\))

En realitat fixeu\sphinxhyphen{}vos que \(f_{X|Y}(x)\) és una família de distribucions:
per cada possible valor d’\(Y\) tenim una \(f_{X|Y}(x)\) diferent.


\subsection{Llei de la probabilitat total}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:llei-de-la-probabilitat-total}}
La f.d.p condicional ens permet desenvolupar una expressió equivalent a
la llei de la probabilitat total que vem veure per esdeveniments {[}Casella \& BErger 1.2.11{]},
{[}diapo 9, punt (1){]}.

L’idea és expressar una marginal en funció de la condicional:

\(f_{X}(x) = \int^{\infty}_{-\infty} f_{X,Y}(x,y)dy = \int^{\infty}_{-\infty} f_{X|Y=y}(x)f_Y(y)dy\)

(una expressió similar es pot obtenir pel cas discret, remplaçant les integrals per sumes i
\(f\) per \(p\))

Aquesta expressió és molt útil per caracteritzar models jeràrquics, com veurem
a continuació.


\subsection{Exemple de model jeràrquic: Poisson\sphinxhyphen{}Binomial}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:exemple-de-model-jerarquic-poisson-binomial}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{poisson_binomial}.png}
\caption{Exemple de model jeràrquic amb dos nivells: Modelem les entrades de peatons en una tenda, com un model poisson\sphinxhyphen{}binomial. Quina és la distribució d’\(X\)?}\label{\detokenize{0_Intro/0_1_Repas_probabilitat:id7}}\end{figure}

Gràcies a la “lei de la probabilitat total” que acabem de veure,
podem derivar la distribució d’\(X\)
\begin{equation*}
\begin{split}p_{X}(k) & = \sum_{n=0}^\infty p_N(n) p_{X|N=n}(k) \\
         & = \sum_{n=k}^\infty \frac{\lambda^n e^{-\lambda}}{n!} {n \choose k}p^{k}\left(1 - p\right)^{n-k}\\
         & = \frac{\left(\lambda p\right)^k}{k!}e^{-\lambda p}\end{split}
\end{equation*}
Per tant \(X \sim \mbox{Poisson}\left(\lambda p\right)\)

\sphinxstylestrong{Exercici}: demostrar com es passa de la 2a la 3a igualtat.


\subsection{Esperança condicional}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:esperanca-condicional}}
Gràcies a la f.d.p. i la f.m.p. condicionals, podem definir
l’eperança condicional, que juga un rol important en estadística
com veurem tot seguit.

\(E(X|Y=y) = \int x f_{X|Y=y}(x) dx\)

Noteu que a diferència de l’esperança “normal”, l’esperança condicional
és una variable aleatòria, ja que és una funció d’\(Y\)!

De fet, podem utilitzar l’esperança condicional per calcular l’esperança d’\(X\),
gràcies al que a vegades s’anomena la “llei de l’esperança total” (en referència
a la llei de la probabilitat total) {[}Casella \& Berger 4.4.3{]}:

\(E(X) = E(E(X|Y))\)

Adoneu\sphinxhyphen{}vos que l’esperança “exterior” és respecte \(Y\)! Demostració com a exercici.

Aquesta última fórmula es pot fer servir per calcular esperances que altrament
serien molt complicades. Per exemple, considereu el següent model probabilístic:
\begin{itemize}
\item {} 
Tenim \(X_i\) tals que \(E(X_i) = \mu\), \(i=1, \cdots, N\)

\item {} 
\(N\) és una v.a. independent d’\(X_i\), amb \(E(N) = \nu\)

\item {} 
Volem caracteritzar \(T = \sum_{i=1}^N X_i\)

\end{itemize}

(Per exemple, \(X_i\) podria referir\sphinxhyphen{}se al gasto d’un client i \(N\) al
nombre de clients que entren a una web. \(T\) seria els ingressos totals.)

Gràcies a l’expressió \(E(T) = E(E(T|N))\), i fent servir una propietat
que veurem tot seguit sobre v.a.’s independents, tenim:

\(E(T) = E(N)E(X) = \mu \nu\).

I això sense saber res d’\(f_{X_1, \cdots, X_N, N}\)!


\subsection{Esperança condicional i predicció}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:esperanca-condicional-i-prediccio}}
L’esperança condicional juga un rol molt
important en els problemes de predicció.

Vem veure a la diapo 62, que l’esperança d’una v.a. \(X\) era el predictor de
mínim error quadrat:

\(E(Y) = \arg\min_{\theta} E(Y - \theta)^2\)

En molts problemes, el que voldrem és predir \(Y\) en funció
d’un covariat (\sphinxstyleemphasis{covariate} o \sphinxstyleemphasis{feature}) \(X\).

Per exemple:
\begin{itemize}
\item {} 
Predir \(Y\): la nota de l’exàmen final

\item {} 
en funció de \(X\) el nombre d’hores d’estudi

\end{itemize}

En aquest cas, el problem és de trobar una funció \(h(X)\) tal que
minimitzi l’Error Quadràtic Mitjà:

\(\min_{h(x)} E(Y - h(X))^2\) (\sphinxstyleemphasis{l’esperança és sobre :math:\textasciigrave{}X,Y\textasciigrave{}!})

Gràcies a la llei de l’esperança total, podem escriure:

\(E(Y - h(X)\theta)^2 =  E(E(\left(Y - h(X)\right)^2 | X))^2\)

I fent servir el resultat esmentat, veiem que aquesta quantitat es minimitza
per \(h(x) = E(Y | X=x)\)!

El millor predictor d’\(Y\) donat \(X\) és \(E(Y | X=x)\)!
Malauradament, aquest predictor requereix un coneixement de \(f_{XY}\)
per ser implementat. Durant el curs veurem altres predictors més útils,
per exemple els predictors linears, on \(h(X) = a + b X\).


\subsection{Variables aleatòries independents}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:variables-aleatories-independents}}
El concepte d’esdeveniments independents:

\(A, B: P(A\cap B) = P(A)P(B)\)

es pot extendre a v.a.’s (o a les components d’un v.m.). Pel cas bivariat, tenim:

{[}Casella \& Berger 4.2.5{]} si \(X, Y \sim f_{X,Y}\) i \(X\sim f_{X}\),
\(Y\sim f_{Y}\) compleixen que \(f_{X,Y}(x,y) = f_{X}(x)f_Y(y)\),
aleshores diem que \(X, Y\) són v.a.’s independents.
\begin{itemize}
\item {} 
Podeu verificar que aquesta definició implica que per qualsevol \(A, B\), els esdeveniments \(X\in A, Y \in B\) són independents.

\item {} 
Un resultat més sorprenent és que el recíproc també és cert, si existeixen \(g(x), h(y)\) tals que \(f_{X,Y}(x,y) = h(x)g(y)\), aleshores \(X, Y\) són independents {[}Casella \& Berger 4.2.7{]}

\item {} 
El resultat s’extén de manera immediata a les components d’un v.m. amb \(N>2\) {[}Casella \& Berger 4.6.5{]}

\end{itemize}


\subsection{Correlació i covariança}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:correlacio-i-covarianca}}\begin{itemize}
\item {} 
Fins ara hem vist una caracterització dicotòmica de les relacions entre variables aleatòries: \sphinxstylestrong{independents} o \sphinxstylestrong{no independents}

\item {} 
Quan les v.a.’s no són independents, sovint és útil caracteritzar\sphinxhyphen{}ne el grau d’associació

\end{itemize}

La covariança i la correlació ens serveixen per quantificar el grau d’associació \sphinxstylestrong{linear}
entre dues v.a.:

{[}Casella \& Berger 4.5.1, 4.5.2{]} Considerem \(X, Y\) tals que
\(E(X)=\mu_X\), \(E(Y)=\mu_Y\), \(\mbox{Var}(X)=\sigma^2_X\),
\(\mbox{Var}(Y)=\sigma^2_Y\). Definim la covariança com \(\mbox{Cov}(X,Y) = E((X - \mu_X)(Y - \mu_Y))\)
i la correlació com \(\rho_{X,Y} = \frac{\mbox{Cov}(X,Y)}{\sigma_X \sigma_Y}\)

\sphinxstyleemphasis{Exercici}: Demostrar que \(-1 \leq \rho_{X,Y} \leq 1\)

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=500\sphinxpxdimen]{{covariance}.png}
\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=450\sphinxpxdimen]{{simpson}.png}
\caption{Exemple: \(X\) és la dosis d’un medicament i \(Y\) és la supervivència.
\(W\) podria ser per exemple, l’ètnia del pacient.}\label{\detokenize{0_Intro/0_1_Repas_probabilitat:id8}}\end{figure}


\subsection{Esperança de funcions v.a.’s independents}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:esperanca-de-funcions-v-a-s-independents}}
L’última propietat que estudiarem abans d’avançar al Tema 2 és la relació entre
v.a.’s independents i l’esperança:

{[}Casella \& Berger 4.2.10{]} Considereu \(g(X)\) i \(h(Y)\) per dues funcions
i v.a.’s arbitràries. Aleshores, si \(X, Y\) són independents, \(E(g(X)h(Y)) = E(g(X))E(h(Y))\)

\sphinxstyleemphasis{Demostració}: Aplicació immediata de la definició d’independència de v.a.’s.

Aquest resultat, com tots els relacionats amb l’esperança sembla trivial però
té conseqüencies importants:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Si \(X, Y\) són independents, aleshores \(\mbox{Cov}(X,Y)=\rho_{X,Y} = 0\) (Pregunta: creieu que el recíproc és cert?)

\item {} 
La funció generatriu de moments d’una suma de variables aleatòries independents és la multiplicació de f.g.m’s

\end{enumerate}


\subsection{Qüestionari repàs}
\label{\detokenize{0_Intro/0_1_Repas_probabilitat:questionari-repas}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Si X, Y son tals que \(\mbox{cov}(X, Y) = 0\), quines de les següents afirmacions són certes: (a) X, Y són incorrelades, (b) X, Y no tenen perquè ser independents, (c) si X, Y son gaussianes, aleshores són independents, (d) Totes les anteriors son certes.

\item {} 
Tenim una f.d.p conjunta \(f(x,y,z) = \frac{1}{K} e^{- (x + y + z)}\) amb \(K\) t.q. \(\int \int \int f(x,y,z) = 1\). Podem dir que X, Y, Z són mutualment independents?

\item {} 
Volem predir una v.a. Y en funció d’X. Quin és el predictor que en minimitza l’error quadràtic mitjà? Com el podem estimar?

\item {} 
Si \(X\) és tal que \(\mbox{var}(X)=\sigma^2\) i \(Y = a + b X\), quina és \(\mbox{var}(Y)\)?

\item {} 
Quina és la marginal per \(X\) de la conjunta \(f(x,y)=\left\{\begin{array}{cc}\lambda^2 e^{- \lambda y} & o \leq x \leq y \\ 0 & \mbox{altrament} \end{array}\right.\)

\end{enumerate}


\chapter{Tema 2: Introducció a l’inferència estadística}
\label{\detokenize{0_Intro/0_2_Intro_stats:tema-2-introduccio-a-l-inferencia-estadistica}}\label{\detokenize{0_Intro/0_2_Intro_stats::doc}}

\section{Mostreig}
\label{\detokenize{0_Intro/0_2_Intro_stats:mostreig}}

\subsection{El perquè de tot plegat}
\label{\detokenize{0_Intro/0_2_Intro_stats:el-perque-de-tot-plegat}}
En aquesta Tema 2 desenvoluparem la teoria necessària per respondre a les següents preguntes:
\begin{itemize}
\item {} 
Com podem caracteritzar estadísticament una \sphinxstyleemphasis{població} o un \sphinxstyleemphasis{experiment aleatori} a partir d’una \sphinxstyleemphasis{mostra} finita?

\item {} 
Com de fiables són aquestes caracteritzacions?

\end{itemize}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{mostreig}.png}
\end{figure}


\subsection{Exemple: estimació preu mig de venta immobiliària de Barcelona}
\label{\detokenize{0_Intro/0_2_Intro_stats:exemple-estimacio-preu-mig-de-venta-immobiliaria-de-barcelona}}
Som inversors immobiliaris a Barcelona i volem tenir una estimació del preu mig de
venta dels habitatges sense esperar a que l’ajuntament publiqui les dades de les
compra\sphinxhyphen{}ventes registrades en l’últim exercici.

Tenim:
\begin{itemize}
\item {} 
\(\mu\): preu mig (\sphinxstyleemphasis{paràmetre}) calculat sobre totes les ventes (\sphinxstyleemphasis{població})

\item {} 
\(\bar{x}\): preu mig calculat sobre una \sphinxstyleemphasis{mostra} aleatòria de \(N=30\) mostres (\sphinxstyleemphasis{estadístic})

\end{itemize}

A l’\sphinxhref{https://e-aules.uab.cat/2020-21/mod/resource/view.php?id=156704}{exemple complet (Campus Virtual)}
hem vist que podem utilitzar \(\bar{x}\) per estimar \(\mu\), amb un error de \textless{} 11\%.


\subsection{Població estadística}
\label{\detokenize{0_Intro/0_2_Intro_stats:poblacio-estadistica}}
Quan parlem de \sphinxstyleemphasis{població} ens podem referir a:
\begin{itemize}
\item {} 
Un conjunt \sphinxstyleemphasis{finit} i \sphinxstyleemphasis{“petit”} d’elements dels quals mesurem una quantitat real. \sphinxstyleemphasis{Exemple}: les alçades de les persones d’aquesta classe.

\item {} 
Un conjunt \sphinxstyleemphasis{finit} però \sphinxstyleemphasis{“gran”} d’elements dels quals mesurem una quantitat real. \sphinxstyleemphasis{Exemple}: les mesures dels sèpals i pètals de les diferents espècies d’\sphinxstyleemphasis{Iris}

\item {} 
Una \sphinxstyleemphasis{abstracció} representant el fet de mesurar una o diverses variables aleatòries caracteritzades per una funció de distribució de probabilitat. \sphinxstyleemphasis{Exemple}: obtenir 4 asos al repartir 4 cartes d’una baralla de 52 cartes

\end{itemize}

A efectes pràctics, l’únic que ens interessa és si l’acte de mostrejar impacta la distribució de la mostra o no


\subsection{Mostreig}
\label{\detokenize{0_Intro/0_2_Intro_stats:id1}}
El mostreig és l’acció de sel.leccionar i mesurar un \sphinxstylestrong{subconjunt de la població}.

El mostreig més simple és l’aleatori: escollim un membre de la població a l’atzar, i en mesurem l’atribut.

En \sphinxstylestrong{poblacions infinites} (o finites però suficientment grans), l’acte de mostrejar no afecta la distribució de les següents mostres.

En \sphinxstylestrong{poblacions finites}, un mostreig aleatori \sphinxstyleemphasis{sense remplaçament} sí que impacta la distribució de les mostres
subsegüents (vegeu exemple a continuació)


\subsection{Mostra \sphinxstyleemphasis{iid}: Definició}
\label{\detokenize{0_Intro/0_2_Intro_stats:mostra-iid-definicio}}
En la resta del curs, ens centrarem en \sphinxstylestrong{mostres aleatòries de poblacions infinites}. És una prou bona aproximació
en la majoria de casos pràctics (exercici)

\sphinxstylestrong{Definició}: Les variables aleatòries \(\left\{X_1, \cdots, X_N\right\}\) són una mostra aleatòria d’una població
caracteritzada per una \sphinxstyleemphasis{fdp} \(f_X(x)\), si \(\left\{X_1, \cdots, X_N\right\}\) són \sphinxstylestrong{mutualment independents} i \(X_i \sim f_X(x)\).

Alternativament \(\left\{X_1, \cdots, X_N\right\}\) s’anomenen \sphinxstylestrong{independents i idènticament distribuïdes} (abreviat \sphinxstylestrong{iid})

Una mostra \sphinxstyleemphasis{iid} és més fàcil de modelar probabilísticament, ja que la f.d.p. (o la f.m.p.) conjunta:

\(f_{X_1, \cdots, X_{N}}(x_1, \cdots, x_N) = \Pi_{i=1}^N f_X(x_i)\)

{[}Casella \& Berger Exemple: 5.1.2{]} Volem caracteritzar la vida útil (en anys) \(X_i\) d’\(n\) circuits electrònics. Si
\(X_i \sim \mathbf{exponential}\left(\beta\right)\), quina és la probabilitat que tots els circuits mesurats durin més de 2 anys?

Els esdeveniments \(\left\{X_1 \leq x_1, \cdots, X_{N} \leq x_{N}\right\}\) són mutualment independents (el fet de mesurar un subconjunt de circuits no té cap impacte en la mesura dels altres)

Per la propietat \sphinxstyleemphasis{iid}, la \sphinxstyleemphasis{fdp} conjunta de \(\left\{X_1, \cdots, X_{N}\right\}\) és \(f\left(x_1, \cdots, x_{N}\right) = \Pi_{i=1}^{N}f\left(x_i\right) = \frac{1}{\beta^n} e^{\frac{-\sum_i x_i}{\beta}}\)
\begin{equation*}
\begin{split}P\left(X_1 \geq 2, \cdots, X_N \geq 2\right) &= \int_{2}^{\infty}\cdots\int_{2}^{\infty}\frac{1}{\beta^n}e^{\frac{-\sum_i x_i}{\beta}} dx_1\cdots dx_N \\
                                             &= e^{-\frac{2n}{\beta}}\end{split}
\end{equation*}
Tenim una població de n=30 persones, representades per la seva alçada: \(\left\{x_1, \cdots, x_n\right\}, x_i\in \left(0, \infty\right)\)

\(X_i\): alçada de la \(i\)\sphinxhyphen{}ena persona. Si \(x_i\neq x_j, \forall i\neq j\), tenim:
\begin{equation*}
\begin{split}P\left(X_1 = x_k\right)             &= \frac{1}{n} \\
P\left(X_2 = x_l | X_1 = x_k\right) &= \left\{\begin{array}{cc} 0 & l=k \\ \frac{1}{n-1} &l\neq k \end{array}\right.\end{split}
\end{equation*}
\(P\left(X_2 | X_1 = x_k\right) \neq P\left(X_2\right) \neq P\left(X_1\right) \Rightarrow\) Les mostres \(X_1, X_2\) no són ni independents, ni idènticament distribuïdes

\sphinxstylestrong{Exercicis}:
\sphinxhyphen{} Què passa si mostregem cada persona \sphinxstyleemphasis{amb remplaçament}?
\sphinxhyphen{} Què passa si els \(x_i\) no són únics?


\subsection{Exemple de mostra \sphinxstylestrong{no\sphinxhyphen{}iid}: Diferents biaixos de sel.lecció}
\label{\detokenize{0_Intro/0_2_Intro_stats:exemple-de-mostra-no-iid-diferents-biaixos-de-sel-leccio}}

\section{Estadístics i estimadors}
\label{\detokenize{0_Intro/0_2_Intro_stats:estadistics-i-estimadors}}

\subsection{Estadístics: mitja i variança}
\label{\detokenize{0_Intro/0_2_Intro_stats:estadistics-mitja-i-varianca}}
Si ens donen un conjunt de dades i volem descriure’l abreviadament, el més comú es calcular\sphinxhyphen{}ne:
\begin{itemize}
\item {} 
La mitjana: \(\bar{X} = \frac{1}{N}\sum_{i=1}^N X_i\)

\item {} 
La variança mostral o variança empírica: \(S^2_X = \frac{1}{N-1}\sum_{i=1}^N \left(X_i -  \bar{X}\right)^2\)

\end{itemize}

Amb aquestes dues quantitats ja podem fer inferència:
\begin{itemize}
\item {} 
Podem utilitzar \(\bar{X}\) com a predictor d’una nova observació

\item {} 
Podem utilitzar \(\bar{X} \pm 2 \sqrt{S^2_X}\) per evaluar com de rara és una nova observació

\end{itemize}

La mitja i la variança són dos exemples d’un \sphinxstylestrong{estadístic}.


\subsection{Estadístics: definició}
\label{\detokenize{0_Intro/0_2_Intro_stats:estadistics-definicio}}
\sphinxstylestrong{Definició:} Donada una mostra iid \(\left\{X_1, \cdots, X_N\right\}\) d’un espai mostral \(\Omega\),
un estadístic és una funció \(T: \Omega^N \rightarrow \mathbb{R}^p\), amb \(p \geq 1\).

\sphinxstyleemphasis{Exemples}:
\begin{itemize}
\item {} 
La mitja i la variança d’una mostra reals són estadístics amb \(p=1\).

\item {} 
La matriu de covariança \(\mathbf{S}\) d’una mostra multivariada de dimensió \(d\) és un estadístic amb \(p=d + d(d-1)/2\) (graus de llibertat d’una matriu simètrica):

\end{itemize}

\(\mathbf{S} = \frac{1}{N-1} \sum_{i=1}^N (\mathbf{X}_i - \bar{\mathbf{X}})(\mathbf{X}_i - \bar{\mathbf{X}})^T\)

Com que un estadístic \(T\) és una funció de variables o vectors aleatòries,
\(T\) és també una variable o vector aleatori

\sphinxstylestrong{Exercici}: Altres exemples d’estadístics?

\sphinxstylestrong{Per una sola mostra} \(\left\{X_1, \cdots, X_N\right\}\):
\begin{itemize}
\item {} 
\sphinxstyleemphasis{Mediana}: el valor tal que 50\% dels elements de la mostra són més petits

\item {} 
\sphinxstyleemphasis{Percentil} \(p\): el valor tal que \(p\) \%\% dels elements de la mostra són més petits

\item {} 
\sphinxstyleemphasis{Max/min (extrems)}: el màxim i mínim de la mostra

\item {} 
\sphinxstyleemphasis{Histograma}

\end{itemize}

\sphinxstylestrong{Per dues mostres} \(\left\{X_1, \cdots, X_N\right\}\), \(\left\{Y_1, \cdots, Y_N\right\}\):
\begin{itemize}
\item {} 
\sphinxstyleemphasis{Coeficient de correlació}:  \(R = \frac{\sum_{i}X_i Y_i}{\sqrt{S^2_X S^2_Y}}\)

\item {} 
\sphinxstyleemphasis{Risc relatiu}: \(\frac{\bar{X}}{\bar{Y}}\) (per mostres de Bernouilli)

\end{itemize}


\subsection{Estimadors i paràmetres: Exemple en població \sphinxstylestrong{finita}}
\label{\detokenize{0_Intro/0_2_Intro_stats:estimadors-i-parametres-exemple-en-poblacio-finita}}
Tornem a un exemple similar al que vem veure a la \sphinxhref{https://e-aules.uab.cat/2020-21/mod/resource/view.php?id=156704}{primera classe del Tema 2 (Campus Virtual)}.

Tenim una \sphinxstylestrong{població} de \(n=47 \times 10^6\) persones, representades per la seva alçada:
\(\left\{x_1, \cdots, x_n\right\}, x_i\in \left(0, \infty\right)\)

Definim dos \sphinxstylestrong{paràmetres} que caracteritzen la població, per exemple:
\begin{itemize}
\item {} 
L’alçada mitja: \(\mu = \frac{1}{n}\sum_{i=1}^n x_i\)

\item {} 
La desviació estàndar de l’alçada: \(\sigma = \sqrt{ \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2}\)

\end{itemize}

Aquestes dues són quantitats \sphinxstylestrong{deterministes}, però calcular\sphinxhyphen{}les requereix mesurar l’alçada de 47M de persones.
(Això sense tenir en compte que l’alçada de les persones canvia en el temps…)

Com hem vist a la primera classe, enlloc de fer passar 47M de persones
pels Centres d’Atenció Primària, podem construïr \sphinxstylestrong{estimadors} d’aquests \sphinxstylestrong{paràmetres},
a partir d’una mostra finita amb N=1000, per exemple:

\(\hat{\mu} = \bar{X}\)

\(\hat{\sigma} = \sqrt{S^2_X}\)

Noteu que \(X_i\) representen la v.a. corresponent a mostrejar la població
\(\left\{x_i\right\}\) (\sphinxstylestrong{amb remplaçament}) i per tant són una mostra \sphinxstylestrong{iid}.

Questions d’inferència:
\begin{itemize}
\item {} 
Com de bé aproximen \(\hat{\mu}\) a \(\mu\) i \(\hat{\sigma}\) a \(\sigma\)?

\item {} 
\(\hat{\mu}\), \(\hat{\sigma}\) són v.a.’s… quina distribució tenen?

\end{itemize}

En el cas d’una població infinita, normalment \sphinxstylestrong{assumim} que la mateixa
està caracteritzada per una distribució de probabilitat parameteritzada per
una sèrie de paràmetres (a vegades als paràmetres genèrics s’els denota per
un vector \(\mathbf{\theta}\))

Exemples:
\begin{itemize}
\item {} 
La vida útil d’un circuit estava carateritzada per una \sphinxstylestrong{població exponencial} amb paràmetre \(\beta\)

\item {} 
Una mostra iid d’una \sphinxstylestrong{població normal} està caracteritzada per la mitja (\(\mu\)) i la variança (\(\sigma^2\)). Direm que \(\mathbf{\theta}=\left[\mu, \sigma^2 \right]\)

\item {} 
El nombre de cares al llençar una moneda N vegades és una mostra d’una \sphinxstylestrong{població binomial} amb paràmetres \(p, N\)

\end{itemize}


\subsection{(Breu parèntesis: Estimadors i estadístics)}
\label{\detokenize{0_Intro/0_2_Intro_stats:breu-parentesis-estimadors-i-estadistics}}\begin{itemize}
\item {} 
En general tots els \sphinxstylestrong{estimadors} són \sphinxstylestrong{estadístics}

\item {} 
Quan parlem d’estimadors, ens referim a un \sphinxstylestrong{paràmetre de la població}

\item {} 
Fins ara em vist estimadors que són identitats d’estadístics (mitja, variança)

\item {} 
Més endavant veurem com construïr estimadors que són funcions més complexes de les dades

\end{itemize}

Ara ens interessarem en la caracterització probabilística dels estadístics que hem vist fins ara.


\subsection{Qüestionari de recapitulació}
\label{\detokenize{0_Intro/0_2_Intro_stats:questionari-de-recapitulacio}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Donada una mostra \(\left\{x_1, \cdots, x_N\right\}\), és \(T = \max_i x_i - \min_i x_i\) un estadístic?

\item {} 
Quina és la població corresponent a l’experiment de sel.leccionar 4 cartes de pòker d´una baralla de 48?

\item {} 
Quan es fa una enquesta d’intenció de vot, quin tipus de població i mostra tenim?

\item {} 
Quan es fa una enquesta d’intenció de vot, perquè no és vàlid agafar una mostra només d’una zona geogràfica determinada?

\end{enumerate}


\section{Caracterització probabilística d’estadístics}
\label{\detokenize{0_Intro/0_2_Intro_stats:caracteritzacio-probabilistica-d-estadistics}}

\subsection{Caracterització d’un estadístic}
\label{\detokenize{0_Intro/0_2_Intro_stats:caracteritzacio-d-un-estadistic}}
La definició genèrica d´un estadístic (funció de variables aleatòries i.i.d)
no ens aporta masses pistes sobre com caracteritzar\sphinxhyphen{}lo probabilísticament

Haurem de fer doncs asssumpcions addicionals:
\begin{itemize}
\item {} 
\sphinxstylestrong{tipus de funció} (ex: mitja, variança, funció contínua i diferenciable)

\item {} 
\sphinxstylestrong{distribució de la mostra} (ex: mostra normals)

\item {} 
\sphinxstylestrong{comportament asimptòtic} (quan el tamany de la mostra tendeix a l’infinit)

\end{itemize}

o bé utilitzar eines computacionals (bootstrap, simulació) que veureu amb més detall en un altre curs.

Comencem doncs pels casos més senzills: la mitjana aritmètica i la variança


\subsection{Caracterització dels estadístics mitjana i variança}
\label{\detokenize{0_Intro/0_2_Intro_stats:caracteritzacio-dels-estadistics-mitjana-i-varianca}}
Comencem per un resultat auxiliar important:

\sphinxstylestrong{Lemma 5.2.5:} Donada una mostra iid \(\left\{X_1, \cdots, X_N\right\}\) amb esperança finita, i una funció
arbitrària \(g\) tenim que:
\begin{itemize}
\item {} 
\(E\left(\sum_{i=1}^N g\left(X_i\right)\right) = N E\left(g\left(X\right)\right)\)

\item {} 
\(\mbox{Var}\left(\sum_{i=1}^N g\left(X_i\right)\right) = N \mbox{Var}\left(g\left(X\right)\right)\)

\end{itemize}

Demostració (feta a la “pissarra”). Recordeu:
\begin{itemize}
\item {} 
Linearitat de l’esperança

\item {} 
Covariança de v.a. independents

\end{itemize}

Com a corolari del darrer Lemma, tenim:

\sphinxstylestrong{Teorema 5.2.6:} Donada una mostra iid \(\left\{X_1, \cdots, X_N\right\}\) amb esperança \(\mu\) i variança \(\sigma^2\)
tenim:
\begin{itemize}
\item {} 
\(E\left(\bar{X}\right) = \mu\)

\item {} 
\(\mbox{Var}\left(\bar{X}\right) = \frac{1}{N}\sigma^2\)

\item {} 
\(E\left(S^2_X\right) = \sigma^2\)

\end{itemize}

Demostració (exercici). Aplicació quasi directa del Lemma anterior. Per calcular
\(E\left(S^2_X\right)\) convé fer servir la identitat \(S^2_X = \frac{1}{N -1}(\sum X_i^2 - \bar{X}^2)\)

Observacions:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Els estadístics \(\bar{X}\) i \(S^2\) son \sphinxstylestrong{estimadors sense biaix} de \(\mu\) i \(\sigma^2\)

\item {} 
Només hem assumit l’existència de moments d’ordre 2!

\item {} 
Sense el factor \(\frac{1}{N-1}\), \(S^2\) tindria biaix

\end{enumerate}

Només amb aquest resultat, podem aplicar la desigualtat de Txebixev:
\begin{equation*}
\begin{split}P\left(\frac{\left(\bar{X} - \mu\right)^2}{\sigma^2} \geq r \right) & \leq \frac{E \left(\frac{\left(\bar{X} - \mu\right)^2}{\sigma^2} \right)}{r} \\
                                                                    & =  \frac{\mbox{Var}\left(\bar{X}\right)}{\sigma^2 r} \\
                                                                    & = \frac{1}{ N r}\end{split}
\end{equation*}
Per tant \(\lim_{N \to \infty} P\left(\left|\bar{X} - \mu\right| \geq r \right) = 0\) (convergència en probabilitat)

Acabem de demostrar la \sphinxstylestrong{Llei Feble dels Grans Nombres}: \(\bar{X}\) convergeix a \(\mu\) quan \(N\) tendeix a infinit.


\subsection{Caracterització de \protect\(\bar{X}\protect\) mitjantçant la funció generatriu de moments}
\label{\detokenize{0_Intro/0_2_Intro_stats:caracteritzacio-de-bar-x-mitjantcant-la-funcio-generatriu-de-moments}}
En alguns casos, podem anar encara més enllà en la caracterització de la
distribució de \(\bar{X}\). El primer cas que tractarem és a través
de la funció generatriu de moments:

\sphinxstylestrong{Teorema 5.2.7:} Donada una mostra iid \(\left\{X_1, \cdots, X_N\right\}\) amb funció
generatriu de moments \(M_X(t)\). La f.g.m de \(\bar{X}\) és: \(M_{\bar{X}} = \left(M_{X}(\frac{t}{N})\right)^N\)

\sphinxstyleemphasis{Demostració}: Aplicació directa de la propietat que hem vist al Tema 1 per l’esperança del producte de v.a.’s independents.

Aquest resultat ens permet caracteritzar facilment la mitja de poblacions
amb f.g.m coneguda, per exemple:
\begin{itemize}
\item {} 
normal

\item {} 
gamma

\end{itemize}

Apliquem\sphinxhyphen{}ho a calcular la f.d.p. d’\(\bar{X}\) per una mostra iid Gaussiana.

La f.g.m d’una Gaussiana \(\mathcal{N}(\mu, \sigma^2)\) és \(M_X(t) = e^{\mu t + \frac{\sigma^2t^2}{2}}\)

Per aplicació directa del resultat anterior, tenim que en aquest cas:

\(M_{\bar{X}} = M_{X}(\frac{t}{N})^N = e^{\mu t + \frac{\sigma^2t^2}{2N}}\)

que podem identificar amb la f.g.m d’una Gaussiana \(\mathcal{N}(\mu, \frac{\sigma^2}{N})\).

Per tant \(\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{N})\)


\subsection{Caracterització de \protect\(\bar{X}\protect\) a través de la convolució}
\label{\detokenize{0_Intro/0_2_Intro_stats:caracteritzacio-de-bar-x-a-traves-de-la-convolucio}}
Quan la f.g.m no existeix o no es correspon amb la f.g.m d’una distribució coneguda,
només ens queda una eina teòrica per caracteritzar \(\bar{X}\),
i és la caracterització pel Jacobià d’una transformació, que citarem però no demostrarem:

\sphinxstylestrong{Teorema 5.2.9:} Si X, Y són v.a.’s independents amb f.d.p \(f_X\) i \(f_Y\), respectivament,
aleshores la f.d.p de \(Z = X + Y\) ve donada per \(f_Z(x) = (f_X \ast f_Y)(x)\)

\sphinxstylestrong{Esboç demostració}: Definir la transformació \((X, Y) \to (X, Z + Y)\), calcular\sphinxhyphen{}ne el Jacobià i
aplicar la fórmula 4.3.2 de Casela \& Berger.

\sphinxstylestrong{Exemple d’aplicació}: Caracteritzar \(\bar{X}\) per una població de Cauchy, {[}Casella \& Berger 5.2.10{]}.


\subsection{Mostra iid d’una població Normal}
\label{\detokenize{0_Intro/0_2_Intro_stats:mostra-iid-d-una-poblacio-normal}}
Acabem de veure que per una mostra iid d’una població \(\mathcal{N}(\mu, \sigma^2)\),
\(\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{N})\). De fet, podem anar una mica més lluny
i caracteritzar també la distribució de \(S_X^2\):

\sphinxstylestrong{{[}Casella \& Berger 5.3.1{]}} per una mostra iid d’una població \(\mathcal{N}(\mu, \sigma^2)\), tenim:
\begin{enumerate}
\sphinxsetlistlabels{\alph}{enumi}{enumii}{}{.}%
\item {} 
\(\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{N})\)

\item {} 
\(\bar{X}\) i  \(S_X^2\) són independents

\item {} 
\(\frac{N-1}{\sigma^2}S_X^2 \sim \chi^2_{N-1}\)

\end{enumerate}

\sphinxstylestrong{Esboç demostració}: El punt (a) ja l’hem vist. Els (b), (c), a la “pisarra”.

Observacions:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
La variança de \(\bar{X}\) és inversament proporcional al tamany de la mostra, com ja haviem vist en el Teorema 5.2.6 pel cas genèric

\item {} 
Per calcular \(S_X^2\) es fa servir \(\bar{X}\)… com pot ser que siguin v.a.’s independents!

\item {} 
Fixeu\sphinxhyphen{}vos que l’aplicació pràctica tal qual és limitada: només podem caracteritzar la distribució de \(\bar{X}\), \(S_X^2\) si coneixem els paràmetres  \(\mu, \sigma^2\)… que és el que volem estimar!

\end{enumerate}

Més endavant veurem un parell de sol.lucions per aquestes mancances


\subsection{Distribució de \protect\(\bar{X}\protect\) per mostres grans: Teorema del Límit Central}
\label{\detokenize{0_Intro/0_2_Intro_stats:distribucio-de-bar-x-per-mostres-grans-teorema-del-limit-central}}
Fins ara hem vist com caracteritzar \(\bar{X}\) i \(S_X^2\) més enllà dels seus moments
és bastant complicat a no ser que fem suposicions bastant fortes sobre
la distribució de la població.

Per sort, en el règim asimptòtic quan el tamany de la mostra \(N \to \infty\),
tenim un resultat absolutament sorprenent sobre la distribució de \(\bar{X}\)
\sphinxstylestrong{sense cap suposició sobre la distribució d’}\(X_i\)!

\sphinxstylestrong{{[}Teorema del Límit Central, Casella \& Berger 5.5.15{]}} La mitjana \(\bar{X}\) de mostres iid
amb \(E(X_i) =\mu\) i \(\mbox{Var}(X_i)=\sigma^2\) és tal que quan \(N \to \infty\),
\(\sqrt{N}\left(\frac{\bar{X} - \mu}{\sigma}\right) \Rightarrow \mathcal{N}(0, 1)\)

(el símbol \(\Rightarrow\) denota convergència en distribució. Ho interpretarem
com que \(\sqrt{N}\left(\frac{\bar{X} - \mu}{\sigma}\right)\) es comporta com \(\mathcal{N}(0, 1)\)
a mesura que \(N \to \infty\))


\subsection{Juguem amb el Teorema del Límit Central!}
\label{\detokenize{0_Intro/0_2_Intro_stats:juguem-amb-el-teorema-del-limit-central}}
A l’\sphinxhref{https://e-aules.uab.cat/2020-21/mod/resource/view.php?id=156704}{exemple de l’inici del Tema 2}
ja vem acabar veient que la distribució de \(\bar{X}\), al re\sphinxhyphen{}mostrejar amb remplaçament
\(N=1000\) vegades la nostra població, s’assemblava bastant a una normal:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{clt}.png}
\end{figure}

Experimentem una mica:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Generem N=1000 mostres de \(\bar{X}_n\) per mostres de tamany \(n=10,100,500,1000\) d’una distribució amb variança finita. Podeu provar distribucions discretes (binomial negativa, poisson) i contínues (exponencial).

\item {} 
Ara repetim l’experiment però amb alguna distribució amb variança no finita (per exemple, Cauchy)

\end{enumerate}

Què observeu? Com de ràpid respecte a \(n\) s’assembla la distribució de \(\bar{X}\) a \(\mathcal{N}(\mu, \frac{\sigma^2}{n})\)?

Similar al que passava amb el resultat de {[}Casella \& Berger 5.3.1{]} per una població normal,
l’utilitat pràctica del Teorema del Límit Central requereix a priori el coneixement
de \(\sigma^2\)…

Per sort, hi ha un resultat (Teorema de Slutsky, {[}Casella \& Berger 5.5.17{]}) que ens permet
remplaçar \(\sigma^2\) per \(S^2_X\):

\(\sqrt{N}\left(\frac{\bar{X} - \mu}{\sqrt{S^2_X}}\right) \Rightarrow \mathcal{N}(0, 1)\)

cosa que ens permet fer inferència aproximada sobre \(\mu\)
a partir de \(\bar{X}\) i \(\sqrt{S^2_X}\), com veurem tot seguit:

Identificant \(\mbox{Var}(\bar{X}) = \frac{\sigma^2}{N}\), sabem que, per \(N\) suficientment gran,

\(\frac{\bar{X} - \mu}{\sqrt{\mbox{Var}(\bar{X})}} \sim \mathcal{N}(0, 1)\)

podem trobar un interval \([-z_{\alpha}, z_{\alpha}]\) tal que, per qualsevol \(\alpha\),

\(P(-z_{\alpha} \leq \frac{\bar{X} - \mu}{\sqrt{\mbox{Var}(\bar{X})}} \leq z_{\alpha}) = 1 - \alpha\)

\sphinxstyleemphasis{Exercicis:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Trobem un interval \sphinxstylestrong{aleatori} \([\hat{\mu}_1, \hat{\mu}_2]\) calculat a partir de la mostra tal que \(P(\mu \in [\hat{\mu}_1, \hat{\mu}_2]) = 0.95\) (noteu que aquí la quantitat aleatòria és l’interval, no \(\mu\)!)

\item {} 
Si tenim una idea del ratio \(\frac{\mu}{\sigma}\) (el que se sol anomenar relació senyal\sphinxhyphen{}soroll en tractament estadístic del senyal), per quin \(N\) tindrem que l’error d’estimació és inferior a 5\% amb probabilitat 95\%?

\end{enumerate}

Solució Exercici (1):  Escollint \(\alpha=0.05\), i fent servir
l’aproximació del TLC amb l’aproximació \(\mbox{Var}\left(\bar{X}\right) \to \frac{S^2_X}{N}\)

\(\frac{\bar{X} - \mu}{\sqrt{\frac{S^2_X}{N}}} \sim \mathcal{N}(0, 1)\)

tenim que \(P(-z_{\alpha} \leq \frac{\bar{X} - \mu}{\sqrt{\frac{S^2_X}{N}}} \leq z_{\alpha}) = 0.95\)

per \(z_{\alpha} = \phi^{-1}\left(1 - \alpha/2\right) = 1.96\) (on \(\phi^{-1}\) és la
f.d.c inversa d’una normal estàndard). Re\sphinxhyphen{}organitzant els termes:

\(P(\bar{X} -1.96\sqrt{\frac{S^2_X}{N}} \leq  \mu \leq \bar{X} + 1.96\sqrt{\frac{S^2_X}{N}}) \approx 0.95\)

o, el que és el mateix:

\(P(\mu \in \left[\bar{X} -1.96\sqrt{\frac{S^2_X}{N}}, \bar{X} + 1.96\sqrt{\frac{S^2_X}{N}}\right]) \approx 0.95\)

Solució Exercici (2): Com en la sol.lució anterior, fem servir l’aproximació del TLC per obtenir:

\(P(-1.96 \leq \frac{\bar{X} - \mu}{\sqrt{\mbox{Var}(\bar{X})}} \leq 1.96) \approx 0.95\)

Dividint cada element per \(\mu\), remplaçant \(\mbox{Var}(\bar{X}) = \frac{\sigma^2}{N}\), i reordenant:

\(P( \left|\frac{\bar{X} - \mu}{\mu}\right| \leq 1.96 \frac{\sigma}{\mu\sqrt{N}}) \approx 0.95\)

Per tant si escollim \(N \geq \left(\frac{1.96}{0.05}\frac{\sigma}{\mu}\right)^2 \approx 1536 \times \left(\frac{\sigma}{\mu}\right)^2\) tindrem la precisió desitjada amb una confiança del 95\%.

Per exemple si creiem que la desviació estàndard és de l’ordre de la meitat que la mitja,
això ens donaria \(N \approx 400\).


\section{Estadístics d’Ordre}
\label{\detokenize{0_Intro/0_2_Intro_stats:estadistics-d-ordre}}

\subsection{Estadístics d’Ordre: definició i exemples}
\label{\detokenize{0_Intro/0_2_Intro_stats:estadistics-d-ordre-definicio-i-exemples}}\begin{itemize}
\item {} 
A vegades no ens interessa caracteritzar el comportament “típic” d’una població, sino el \sphinxstylestrong{comportament extrem}, per exemple: saber quin es el cabal màxim d’un riu és molt més útil a l’hora de dimensionar un pont que saber\sphinxhyphen{}ne el cabal mitjà.

\item {} 
D’altres vegades la mitjana pot ser massa sensible a valors grans per distribucions amb “cues” llargues i donar una impressió equivocada, sent més interessant utilitzar els percentils o mitjana. Per exemple: el retorn mig d’una inversió de capital risc pot ser molt gran si vens un “unicorni”, mentres que la mediana molt petita.

\end{itemize}

{[}casella \& Berger 5.4.1{]} Els estadístics d’ordre d’una mostra \(\left\{X_1, \cdots, X_N\right\}\)
són els valors de la mostra ordenats en ordre ascendent, que identificarem per \(\left\{X_{(1)}, \cdots, X_{(N)}\right\}\)

Per exemple:
\begin{itemize}
\item {} 
El mínim/màxim mostral: \(X_{(1)}, X_{(N)}\)

\item {} 
El rang mostral: \(R = X_{(N)} - X_{(1)}\)

\item {} 
La mediana: \(M = X_{(N+1)/2}\) si \(N\) és senar, \(M = \frac{1}{2}(X_{N/2} + X_{N/2 + 1})\) si parell.

\item {} 
Els quartils…

\end{itemize}

Com hem vist abans per la mitjana i la variança empírica, els estadístics d’ordre son també funcions de variables aleatòries i per tant una v.a. en sí mateixos


\subsection{Distribució dels extrems}
\label{\detokenize{0_Intro/0_2_Intro_stats:distribucio-dels-extrems}}
Sorprenentment, la distribució dels extrems (\(X_{(1)}, X_{(N)}\))
d’una mostra iid és bastant fàcil d’obtenir a través de la f.d.c.

Denotem per \(U = \min_i X_i = X_{(1)}\), amb \(X_i \sim F_X\). Aleshores:
\begin{equation*}
\begin{split}F_U(u) = P(U \leq u) &= P(\min_i X_i \leq u) \\
                     &= 1 - P(\min_i X_i > u) \\
                     &= 1 - P(\cap_i X_i > u) \\
                     &= 1 - \Pi_i P(X_i > u) \\
                     &= 1 - \left(1 - F_X(u)\right)^N\end{split}
\end{equation*}
Per tant, si \(X_i\) son v.a. contínues amb f.d.p \(f_X\):
\begin{equation*}
\begin{split}f_U(u) = \frac{d F_X(u)}{du} = N\left(1 - F_X(u)\right)^{N-1}f_X(u)\end{split}
\end{equation*}
Un raonament similar ens permet obtenir la f.d.p del màxim \(V = \max_i X_i = X_{(N)}\):
\begin{equation*}
\begin{split}f_V(u) =  N\left(F_X(u)\right)^{N-1}f_X(u)\end{split}
\end{equation*}
\sphinxstyleemphasis{Exemple d’aplicació}: Màxim d’una mostra iid d’una població uniforme entre {[}0,1{]}:

La f.d.p d’una uniforme és \(f_X(x) = 1\) per \(0 \leq x \leq 1\). Per tant \(F_X(x) = x\) per \(0 \leq x \leq 1\).

Aplicant la fórmula anterior, trobem que la distribució del màxim entre N mostres és:
\begin{equation*}
\begin{split}f_{\max_i X_i}(u) = \left\{\begin{array}{cc} N x^{N-1} & 0 \leq x \leq 1 \\ 0 & \mbox{ altrament} \end{array}\right.\end{split}
\end{equation*}
\noindent{\hspace*{\fill}\sphinxincludegraphics[height=250\sphinxpxdimen]{{max_uniforme}.png}\hspace*{\fill}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{u} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{length}\PYG{o}{=}\PYG{l+m}{100}\PYG{p}{)}
\PYG{n}{Ns} \PYG{o}{=} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{10}\PYG{p}{,} \PYG{l+m}{100}\PYG{p}{,} \PYG{l+m}{1000}\PYG{p}{)}
\PYG{n}{colors} \PYG{o}{=} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{red\PYGZsq{}}\PYG{p}{,} \PYG{l+s}{\PYGZsq{}}\PYG{l+s}{green\PYGZsq{}}\PYG{p}{,} \PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{)}
\PYG{n+nf}{for }\PYG{p}{(}\PYG{n}{i} \PYG{n}{in} \PYG{l+m}{1}\PYG{o}{:}\PYG{n+nf}{length}\PYG{p}{(}\PYG{n}{Ns}\PYG{p}{)}\PYG{p}{)}\PYG{p}{\PYGZob{}}
  \PYG{n+nf}{if }\PYG{p}{(}\PYG{n}{i} \PYG{o}{==} \PYG{l+m}{1}\PYG{p}{)}\PYG{p}{\PYGZob{}}
    \PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{u}\PYG{p}{,} \PYG{n}{Ns}\PYG{n}{[i}\PYG{n}{]}\PYG{o}{*}\PYG{p}{(}\PYG{n}{u}\PYG{n+nf}{\PYGZca{}}\PYG{p}{(}\PYG{n}{Ns}\PYG{n}{[i}\PYG{n}{]}\PYG{l+m}{\PYGZhy{}1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type} \PYG{o}{=} \PYG{l+s}{\PYGZsq{}}\PYG{l+s}{l\PYGZsq{}}\PYG{p}{,} \PYG{n}{ylab} \PYG{o}{=} \PYG{l+s}{\PYGZsq{}}\PYG{l+s}{f\PYGZus{}U\PYGZsq{}}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{n}{colors}\PYG{n}{[i}\PYG{n}{]}\PYG{p}{)}
  \PYG{p}{\PYGZcb{}} \PYG{n}{else} \PYG{p}{\PYGZob{}}
    \PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{u}\PYG{p}{,} \PYG{n}{Ns}\PYG{n}{[i}\PYG{n}{]}\PYG{o}{*}\PYG{p}{(}\PYG{n}{u}\PYG{n+nf}{\PYGZca{}}\PYG{p}{(}\PYG{n}{Ns}\PYG{n}{[i}\PYG{n}{]}\PYG{l+m}{\PYGZhy{}1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab} \PYG{o}{=} \PYG{l+s}{\PYGZsq{}}\PYG{l+s}{f\PYGZus{}U\PYGZsq{}}\PYG{p}{,}  \PYG{n}{col}\PYG{o}{=}\PYG{n}{colors}\PYG{n}{[i}\PYG{n}{]}\PYG{p}{)}
  \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\PYG{n+nf}{legend}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{9}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{Ns}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{n}{colors}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Distribució dels estadístics d’ordre en el cas discret}
\label{\detokenize{0_Intro/0_2_Intro_stats:distribucio-dels-estadistics-d-ordre-en-el-cas-discret}}
Considerem \(X\) discreta prenent valors \(x_1 < x_2 < \cdots < x_k\):

\(p_X(x_i) = p_i\)

Per una mostra iid d’\(X\) de talla N, definim:

\(Y_i\): nombre de \(X_j\) tals que \(X_j \leq x_i\), per \(i>1\)

Com que la mostra és independent i \(P(X_j \geq x_i) = \sum_{k=1}^i p_i := P_i\),
tenim que:

\(Y_i \sim \mbox{Binomial}(P_i, N)\)

Ara només cal observar que l’esdeveniment \(X_{(j)} \leq x_i\)
és equivalent a que hi hagi al menys \(j\) de les observacions
més petites o iguals a \(x_i\), que és exactament la definició de
\(Y_i \geq j\), i per tant:

\(P(X_{(j)} \leq x_i) = P(Y_i \geq j)\)

és a dir:

\(F_{X_{(j)}}(x_i) = \sum_{k=j}^N {n \choose k} P_i^k(1-P_i)^{N-k}\)

que és el resultat que trobarem a {[}Casella \& Berger 5.4.3{]}. Per obtenir
\(p_{X_{(j)}}(x_i)\) “només” cal calcular \(F_{X_{(j)}}(x_i) - F_{X_{(j)}}(x_i-1)\)


\subsection{Distribució dels estadístics d’ordre en el cas continuu}
\label{\detokenize{0_Intro/0_2_Intro_stats:distribucio-dels-estadistics-d-ordre-en-el-cas-continuu}}
La derivació formal de la distribució de l’estadístic d’ordre \(X_{(i)}\)
en el cas continuu utilitza la mateixa idea que en el
cas concret però és bastant tediosa (veure {[}Casella \& Berger 5.4.4{]}).

Aquí farem un raonament heurístic: l’esdeveniment
\(x \leq X_{(i)} \leq x + dx\) per un \(dx\) petit
és equivalent a que:
\begin{itemize}
\item {} 
\(i-1\) de les mostres són estrictament més petites que \(x\)

\item {} 
1 de les mostres està entre \(x\) i \(x + dx\)

\item {} 
\(N-i\) de les mostres són estríctament més grans que \(x\)

\end{itemize}

Hi ha \(\frac{N!}{(i-1)!(N-i)!}\) maneres de que es dongui
aquesta situació, i cada manera succeeix amb probabilitat
\(f_X(x)\left(F_X(x)\right)^{i-1}\left(1 - F_X(x)\right)^{N-i}\),
per tant:

\(f_{X_{(i)}}(x) = \frac{N!}{(i-1)!(N-i)!} f_X(x)\left(F_X(x)\right)^{i-1}\left(1 - F_X(x)\right)^{N-i}\)

A diferència dels moments mostrals (ex: mitjana, variança empírica),
podeu veure que la caracterització dels estadístics d’ordre
requereix un coneixement explícit de la f.d.p (o la f.m.p) de la població,
cosa que en limita la utilitat…

\sphinxstyleemphasis{Exemple}: f.d.p de la mediana d’una mostra iid uniforme entre {[}0,1{]}:

Apliquem la formula de la diapo anterior amb \(f_X(x) = 1\) i \(F_X(x) = x\) per \(0 \leq x \leq 1\),
i suposant N és senar:

\(f_{M}(x) = \frac{N!}{(\frac{N+1}{2}-1)!(N-\frac{N+1}{2})!} x^{\frac{N+1}{2}-1}\left(1 - x\right)^{N-\frac{N+1}{2}}\)

(això és una distribució \(\mbox{Beta}(\frac{N+1}{2}, N - \frac{N+1}{2} + 1)\)

Comparem la distribució d’\(M\), la mediana empírica, amb:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
el que coneixem com la mediana de la població, que pel cas d’una uniforme entre 0 i 1 és 1/2.

\item {} 
l’histograma d’\(M\) per n=100 repeticions de mostres de tamany \(N=100\)

\end{enumerate}

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=250\sphinxpxdimen]{{uniform_median}.png}\hspace*{\fill}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{N} \PYG{o}{=} \PYG{l+m}{101}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m}{100}
\PYG{n}{median\PYGZus{}hat} \PYG{o}{=} \PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
\PYG{n+nf}{for }\PYG{p}{(}\PYG{n}{i} \PYG{n}{in} \PYG{l+m}{1}\PYG{o}{:}\PYG{n}{n}\PYG{p}{)}\PYG{p}{\PYGZob{}}
    \PYG{n}{median\PYGZus{}hat}\PYG{n}{[i}\PYG{n}{]} \PYG{o}{=} \PYG{n+nf}{median}\PYG{p}{(}\PYG{n+nf}{runif}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{median\PYGZus{}hat}\PYG{p}{,} \PYG{l+m}{50}\PYG{p}{,} \PYG{n}{prob}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{length}\PYG{o}{=}\PYG{l+m}{100}\PYG{p}{)}
\PYG{n}{f} \PYG{o}{=} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,} \PYG{p}{(}\PYG{n}{N}\PYG{l+m}{+1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m}{2}\PYG{p}{,} \PYG{n}{N} \PYG{o}{\PYGZhy{}} \PYG{p}{(}\PYG{n}{N}\PYG{l+m}{+1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m}{2} \PYG{o}{+} \PYG{l+m}{1}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{z}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Altres descriptius importants}
\label{\detokenize{0_Intro/0_2_Intro_stats:altres-descriptius-importants}}

\subsection{L’histograma}
\label{\detokenize{0_Intro/0_2_Intro_stats:l-histograma}}

\subsection{La distribució cumulativa empírica}
\label{\detokenize{0_Intro/0_2_Intro_stats:la-distribucio-cumulativa-empirica}}

\subsection{El boxplot}
\label{\detokenize{0_Intro/0_2_Intro_stats:el-boxplot}}

\subsection{L’scatterplot}
\label{\detokenize{0_Intro/0_2_Intro_stats:l-scatterplot}}

\chapter{Tema 3: Estimació}
\label{\detokenize{0_Intro/0_3_Estimacio:tema-3-estimacio}}\label{\detokenize{0_Intro/0_3_Estimacio::doc}}

\section{Estimació per Màxima Versemblança}
\label{\detokenize{0_Intro/0_3_Estimacio:estimacio-per-maxima-versemblanca}}

\subsection{Ajust de distribucions de probabilitat}
\label{\detokenize{0_Intro/0_3_Estimacio:ajust-de-distribucions-de-probabilitat}}
A l’exemple de \sphinxhref{https://e-aules.uab.cat/2020-21/pluginfile.php/695686/mod\_page/content/2/motivacio\_tema\_3.pdf}{la primera classe del Tema 3}
vem veure un exemple de modelització estadística on
partim d’un conjunt de dades que podem modelar com
una mostra i.i.d. d’una població:

\(X_i \sim f_X(x;\theta), i=1,\cdots,N\)

on:
\begin{itemize}
\item {} 
\(f_X\) és la f.d.p. d’una família de distribucions i

\item {} 
\(\theta\) són els paràmetres de la mateixa (vector o escalar), també anomenats \sphinxstyleemphasis{paràmetres de la població}.

\end{itemize}

El problema d’\sphinxstyleemphasis{estimació de paràmetres} o d’\sphinxstyleemphasis{ajust de la distribució} a partir de les dades
consisteix en estimar \(\theta\) a partir de \(X_1, \cdots, X_N\).

Normalment se segueix la següent recepta:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Explorar dades (estadístics descriptius, gràfics)

\item {} 
Hipòtesi: Escollir familia (paramètrica) \(f_X(x;\theta)\)

\item {} 
Àjustar paràmetre \(\theta\) segons algun criteri

\item {} 
Comprovar l’hipòtesi: \sphinxstylestrong{bondat d’ajust} (Tema 4)

\end{enumerate}

A la darrera classe vem veure un exemple d’aplicació d’aquesta recepta
on el criteri d’ajust era \sphinxstylestrong{heurístic}: visualitzar l’histograma conjuntament
amb \(f_X(x;\theta)\) per diversos valors de \(\theta\).

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=350\sphinxpxdimen]{{heuristic_fit}.png}
\end{figure}

Aquest procés heurístic de prova\sphinxhyphen{}i\sphinxhyphen{}error és (1) ineficient i (2) difícil de justificar
quantitativament

Un altre mètode
d’estimació molt versàtil i millor fonamentat
és el de \sphinxstylestrong{Màxima Versemblança}.

Per començar, definim la \sphinxstyleemphasis{log\sphinxhyphen{}versemblança}:
\begin{equation*}
\begin{split}L(\theta; x_1, \cdots, x_N) = \log \left(f_{X_1, \cdots, X_N}(x_1, \cdots, x_N; \theta) \right)\end{split}
\end{equation*}
on \(f_{X_1, \cdots, X_N}(x_1, \cdots, x_N; \theta)\) és la
f.d.p. conjunta de la mostra.

En el cas d’una mostra iid, la log\sphinxhyphen{}versemblança es simplifica a:
\begin{equation*}
\begin{split}L(\theta; x_1, \cdots, x_N) = \sum_{i=1}^N \log f_X(x_i;\theta)\end{split}
\end{equation*}
Per mostres de v.a. discretes, la log\sphinxhyphen{}versemblança es calcula a partir de la f.m.p. conjunta \(p_{X_1, \cdots, X_N}(x_1, \cdots, x_N; \theta)\)
enlloc de la f.d.c.

Algunes propietats de la log\sphinxhyphen{}versemblança:
\begin{itemize}
\item {} 
\(L(\theta; x_1, \cdots, x_N)\) és un funció \(\Theta \times \mathbb{R}^N \to \mathbb{R}\).

\item {} 
Donada una mostra en particular \(X_1=x_1, \cdots, X_N=x_n\), \(L(\theta; x_1, \cdots, x_N)\) és un funció de \(\theta \in \Theta \to \mathbb{R}\).

\item {} 
Com que la mostra és una v.a., \(L(\theta; X_1, \cdots, X_N)\) és una v.a. per cada valor de \(\theta\)!

\item {} 
Com vem veure a la primera classe, aquesta funció es pot interpretar com un criteri de qualitat de \(\theta\) a l’hora d’\sphinxstyleemphasis{explicar} les dades observades (quan més gran, millor explicades).

\end{itemize}

Per tant, sembla raonable definir un estimador del(s)
paràmetre(s) \(\theta\) com:
\begin{equation*}
\begin{split}\hat{\theta} = \arg \max L(\theta; x_1, \cdots, x_N)\end{split}
\end{equation*}
Aquest és el que s’anomena \sphinxstylestrong{Estimador de Màxima Versemblança} (EMV) o MLE per les
seves sigles en anglès.


\subsection{Càlcul de l’EMV}
\label{\detokenize{0_Intro/0_3_Estimacio:calcul-de-l-emv}}
En alguns casos, l’EMV es pot calcular
analíticament, resolent el problema d’optimització associat.
Per exemple en una mostra d’una família Gaussiana,
\begin{equation*}
\begin{split}L(\mu, \sigma; x_1, \cdots, x_N) = - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2} - N \log(\sqrt{2 \pi} \sigma)\end{split}
\end{equation*}
Aquesta funció és diferenciable i concava en \(\mu\) i  \(\sigma\),
per tant el seu màxim existeix i haurà de verificar la \sphinxstylestrong{condició d’optimalitat}:
\begin{equation*}
\begin{split}\nabla_{\mu, \sigma} L(\mu, \sigma; x_1, \cdots, x_N) = 0\end{split}
\end{equation*}
Això ens porta a un sistema d’equacions:
\begin{equation*}
\begin{split}- \sum_i \frac{x_i - \mu}{\sigma^2} &= 0 \\
\sum_i \frac{(x_i - \mu)^2}{\sigma^3} - \frac{N}{\sigma} = 0\end{split}
\end{equation*}
d’on podem concloure que l’EMV és:
\begin{equation*}
\begin{split}\hat{\mu} &= \bar{x} \\
\hat{\sigma} &= \frac{1}{N}\sum_i (x_i - \bar{x})^2\end{split}
\end{equation*}
(noteu que \(\hat{\sigma}\) no és igual que \(S_X^2\)!)

És important tenir en compte que:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
No sempre podrem calcular els EMV de manera analítica.

\item {} 
En alguns casos ho podrem calcular numèricament (fent servir el mètode de descens del gradient, o de Newton)

\item {} 
En alguns casos, l’EMV no serà únic (i.e. la log\sphinxhyphen{}versemblança tindrà més d’un màxim)

\item {} 
En molts casos la log\sphinxhyphen{}versemblança no serà concava, o diferenciable, per tant l’EMV pot ser computacionalment molt difícil de calcular

\end{enumerate}

Malgrat aquestes limitacions, el mètdode de la Màxima Versemblança
ens proporciona un mètode bastant genèric per trobar estimadors.


\subsection{Exemple: EMV d’una multinomial}
\label{\detokenize{0_Intro/0_3_Estimacio:exemple-emv-d-una-multinomial}}
Continuem amb un parell més d’exemples d’aplicació del EMV.

Un model molt útil en estadística és el model \sphinxstyleemphasis{multinomial}, que
s’utilitza quan tenim observacions tabulades, per exemple: un histograma,
el nombre de respostes d’un qüestionari per edat de l’entrevistat,
l’incidència d’una malaltia per regió geogràfica, etc.

En tots aquests casos, es poden resumir les N observacions d’una mostra en un
vector \(X_1, \cdots, X_M\) on \(X_i\) es correspon amb el nombre d’observacions dins la casella \(i\),
i hi ha M caselles i \(\sum_i X_i = N\).

\sphinxstylestrong{Important}: Noteu que en aquest cas \(X_i\) no és iid!

El model \sphinxstyleemphasis{multinomial} suposa que la f.d.m. conjunta de \(X_1, \cdots, X_M\) vé donada per:
\begin{equation*}
\begin{split}p(x_1, \cdots, x_m; p_1, \cdots, p_M) = \frac{N!}{\Pi_i {x_i!}}\Pi_i p_i^{x_i}\end{split}
\end{equation*}
on \(p_1, \cdots, p_M\) són els paràmetres de la població, tals que \(\sum_i p_i = 1\),
i per construcció \(\sum_i X_i = N\).

A partir d’aquesta f.d.m conjunta, i una mostra \(X_1=x_1, \cdots, X_M=x_m\)
podem calcular la log\sphinxhyphen{}versemblança:
\begin{equation*}
\begin{split}L( p_1, \cdots, p_M; x_1, \cdots, x_m) \propto - \sum_i \log (x_i!) + \sum_i x_i \log p_i\end{split}
\end{equation*}
(on ignorem els termes que no depènen d’\(x_i\) o \(p_i\).)

Com que sabem que \(\sum_i p_i = 1\) , podem imposar la restricció que \(p_M = 1 - \sum_{i=1}^{M-1} p_i\),
i tindrem:
\begin{equation*}
\begin{split}L( p_1, \cdots, p_{M-1}; x_1, \cdots, x_m) \propto&  - \sum_i \log (x_i!) + \sum_{i=1}^{M-1} x_i \log p_i \\
    & + (N - \sum_{i=1}^{M-1} x_i ) \log (1 - \sum_{i=1}^{M-1} p_i)\end{split}
\end{equation*}
Calculant\sphinxhyphen{}ne el gradient i igualant\sphinxhyphen{}lo a 0 (exercici per casa), podrem concloure que l’EMV d’una multinomial és simplement:
\begin{equation*}
\begin{split}\hat{p}_i = \frac{x_i}{N}\end{split}
\end{equation*}
En els exemples que hem vist fins ara (Gaussiana, Multinomial, Poisson..), excepte el model de precipitació a través d’una Gamma,
l’EMV es correspon amb l’estimador que hauriem escollit sense saber la teoria de Màxima Versemblança…
Val la pena complicar\sphinxhyphen{}nos la vida amb aquesta teoria!?


\subsection{Exemple: EMV amb dades censurades}
\label{\detokenize{0_Intro/0_3_Estimacio:exemple-emv-amb-dades-censurades}}
L’EMV és una metodologia molt més potent del que hem vist fins ara. D’na banda, com veurem tot seguit,
ens permet estimar paràmetres en casos on l’intuició potser ens fallaria. Per altra banda, com veurem
més endavant, els EMVs té unes propietats estadístiques interessants.

Considerem l’exemple següent: Estem interessats en modelar
la supervivència d’uns pacients sota un tractament mèdic determinat.
Considerem \(X_i\) l’edat en anys de defunció del pacient \(i\).
Durant la durada del nostre estudi, alguns dels pacients moriran
però alguns altres seguiran vius. Per tant, per aquests últims
pacients l’únic que sabrem és que \(X_i \geq e_i\) on
\(e_i\) és l’edat del pacient en el moment d’acabar l’estudi.

Per tant necessitem modelar la versemblança d’una mostra
mixta d’observacions \(X_i\) i esdeveniments \(X_j \geq e_j\),
on aquestes últimes s’anomenen “observacions censurades” (com
si algú ens hagués “censurat” les dades, en aquest cas l’univers).

Anomenem \(\mathcal{M}\) el subconjunt
de pacients morts (i que per tant dels que hem pogut observar\sphinxhyphen{}ne l’edat de defunció)
i \(\bar{\mathcal{M}}\) el subconjunt de pacients vius (dels que només sabem q
que \(X_i \geq e_i\)).

La funció de log\sphinxhyphen{}versemblança
que utilitzarem en aquest cas és:
\begin{equation*}
\begin{split}L(\theta) = \log P\left(\left( \cap_{i: \mathcal{M}}{ X_i=x_i} \right) \cap \left(\cap_{i: \bar{\mathcal{M}}}{ X_i \geq e_i}\right); \theta \right)\end{split}
\end{equation*}
Si la mostra és iid, això es simplificarà a:
\begin{equation*}
\begin{split}L(\theta) = \sum_{i: \mathcal{M}}\log p_X(x_i; \theta) + \sum_{i: \bar{\mathcal{M}}} \log(1 -  F_X(e_i; \theta))\end{split}
\end{equation*}
on \(p_X(x_i; \theta)\) és la f.m.p. del nostre model d’edat de defunció i \(F_X(e_i; \theta)\) n’és la f.d.c. corresponent.

Suposem que modelem l’edat de defunció dels pacients segons una llei geomètrica:
\begin{equation*}
\begin{split}p_X(x; \rho) = (1 - \rho)^{x - 1} \rho\end{split}
\end{equation*}
on \(\rho \in [0, 1]\) és el paràmetre de la població.
La f.d.c. és \(F_x(x ;\rho) = 1 - (1 -\rho)^x\)
i per tant podem calcular la log\sphinxhyphen{}versemblança com:
\begin{equation*}
\begin{split}L(\rho) = \left|\mathcal{M}\right| \log \rho + \sum_{i: \mathcal{M}}(x_i -1) \log(1 -\rho) + \sum_{i: \bar{\mathcal{M}}}e_i \log(1 -  \rho)\end{split}
\end{equation*}
\sphinxstylestrong{Exercici}: Acabar de calcular \(\hat{\rho} = \arg \max  L(\rho)\)


\section{Propietats asimptòtiques de l’EMV}
\label{\detokenize{0_Intro/0_3_Estimacio:propietats-asimptotiques-de-l-emv}}

\subsection{Biaix, Variança, EQM…}
\label{\detokenize{0_Intro/0_3_Estimacio:biaix-varianca-eqm}}
Recordem que la log\sphinxhyphen{}versemblança \(L(\theta; X_1, \cdots, X_N)\)
és una v.a. per cada \(\theta\) (i.e. una “funció aleatòria”)
i per tant l’EMV \(\hat{\theta}\) també és una v.a.! Per caracteritzar\sphinxhyphen{}lo
haurem de fer servir les eines que vem desenvolupar al Tema 2:
\begin{itemize}
\item {} 
\sphinxstylestrong{Biaix}: \(b(\hat{\theta}) := E(\hat{\theta} - \theta_0)\)

\item {} 
\sphinxstylestrong{Variança}: \(\mbox{Var}(\hat{\theta}) = E((\hat{\theta} - E(\hat{\theta}))^2)\)

\item {} 
\sphinxstylestrong{Error Quadràtic Mitjà}: \(\mbox{MSE}(\hat{\theta}) = E((\hat{\theta} - \theta_0)^2)\)

\item {} 
\sphinxstylestrong{La seva f.d.p.}: \(f_{\hat{\theta}}(x)\)

\end{itemize}

\sphinxstylestrong{IMPORTANT}: Tot el que segueix \sphinxstylestrong{suposa} una mostra i.i.d. generada segons un
model \(X_i \sim f_X(x;\theta_0); i=1,\cdots,N\), on \(\theta_0\)
és el valor \sphinxstylestrong{real però desconegut} del paràmetre a estimar. Per tant totes les esperances
que tractem són relatives a aquesta \(f_X(x;\theta_0)\)!

En general l’EMV no tindrà una forma analítica que es presti a
calcular\sphinxhyphen{}ne el biaix, variança o MSE, i molt menys a
caracteritzar\sphinxhyphen{}ne la distribució.

La gran avantatge dels EMV és que, asimptòticament,
es poden caracteritzar relativament fàcilment. Primer definim
què és el que volem dir per “asimptòtic”. Explicitant la dependència
de l’EMV amb el tamany de la mostra:

\(\hat{\theta}^N = \arg \max  L(\theta; X_1, \cdots, X_N)\)

el que ens interessarà és caracteritzar el biaix, variança i
f.d.p de \(\hat{\theta}^N\) a mesura que \(N \to \infty\)

Començarem aquesta caracterització amb el següent resultat,
que provarem de manera informal:

\sphinxstylestrong{Teorema 3.1}: Donada una mostra iid, i per \(f(x; \theta)\) prou “suaus”,
\(\frac{1}{N}L(\theta; X_1, \cdots, X_N) = \frac{1}{N}\sum_i \log f(X_i; \theta)\)
convergeix en probabilitat a \(E(\log f_X(x; \theta))\).

\sphinxstyleemphasis{“Demostració”}: Resulta de l’aplicació de la \sphinxhref{https://atibaup.github.io/ModInfer\_2020/slides/0\_Intro/0\_2\_Intro\_stats.html\#25}{LLei Feble dels Grans Nombres}
que vem veure al Tema 2. Per tant només hauriem de comprovar que podem aplicar\sphinxhyphen{}la, és a dir que:
\begin{itemize}
\item {} 
\(E(\log f(X_i; \theta))\) existeix i que

\item {} 
\(\mbox{Var}(\log f(X_i; \theta))\) és finita

\end{itemize}

que dóna lloc a la condició “prou “suaus”” de la proposició.


\subsection{Consistència}
\label{\detokenize{0_Intro/0_3_Estimacio:consistencia}}
\sphinxstylestrong{Definició}: Un estimador és consistent si, a mesura
que el tamany de la mostra augmenta, l’estimador
convergeix en probabilitat al paràmetre d’interès:
\begin{equation*}
\begin{split}\lim_{N\to \infty} P(|\hat{\theta}^N - \theta_0|>\epsilon) = 0\end{split}
\end{equation*}
\sphinxstylestrong{Teorema 3.2}: Sota algunes condicions de regularitat per \(f_X(x; \theta)\),
l’EMV és un estimador consistent.

\sphinxstyleemphasis{“Demostració”}: Pel \sphinxstylestrong{Teorema 3.1} hem vist que
\(\frac{1}{N}L(\theta; X_1, \cdots, X_N)  \to E(\log f_X(x; \theta))\)
en probabilitat. No podrem demostrar\sphinxhyphen{}ho en aquest curs,
però sembla raonable esperar que, sota algunes condicions,
el \(\hat{\theta}^N\) que maximitza l’expressió de l’esquerra
hauria de maximitzar l’expressió de la dreta i viceversa.

Sota aquest supòsit, només ens cal verificar que \(\theta_0\) maximitza
\(E(\log f_X(x; \theta))\) per concloure que \(\hat{\theta}^N \to \theta_0\).
Fem\sphinxhyphen{}ho:
\begin{equation*}
\begin{split}\frac{\partial}{\partial \theta} E(\log f_X(x; \theta)) & = \frac{\partial}{\partial \theta} \int_x  \log f(x; \theta) f(x; \theta_0) dx \\
& =  \int_x  \frac{\partial}{\partial \theta}\log f(x; \theta) f(x; \theta_0) dx \\
& = \int_x  \frac{1}{f(x; \theta)} \frac{\partial}{\partial \theta} f(x; \theta) f(x; \theta_0) dx\end{split}
\end{equation*}
Estem cometent bastants sacrilegis intercanviant l’ordre dels operadors
integrals i diferencials sense donar explicacions… però ens haurem de
creure que és possible per la majoria de \(f_X\) d’interès.

Noteu que per \(\theta = \theta_0\), aquesta última expressió resulta:
\begin{equation*}
\begin{split}\int_x  \frac{1}{f(x; \theta_0)} \left.\frac{\partial}{\partial \theta} f(x; \theta) \right|_{\theta=\theta_0} f(x; \theta_0) dx & =  \int_x  \left. \frac{\partial}{\partial \theta} f(x; \theta_0) \right|_{\theta=\theta_0} dx\\
& =  \frac{\partial}{\partial \theta} \int_x   f(x; \theta_0)dx \\
& = 0\end{split}
\end{equation*}
per tant \(\theta = \theta_0\) és tal que \(\frac{\partial}{\partial \theta} E(\log f_X(x; \theta))=0\)
i si \(E(\log f_X(x; \theta))\) és concava, n’és un màxim. Amb això
podem “concloure” que \(\hat{\theta}^N \to \theta_0\).


\subsection{Distribució asimptòtica de l’EMV}
\label{\detokenize{0_Intro/0_3_Estimacio:distribucio-asimptotica-de-l-emv}}
Per ara hem vist que l’EMV té una propietat bona, la consistència: quan el tamany
de la mostra augmenta, l’estimador convergeix al valor del
paràmetre de la població.

La caracterització asimptòtica de l’EMV no s’acaba aquí: de fet,
tot seguit veurem que \sphinxstylestrong{la distribució de l’EMV és Gaussiana,
centrada en el paràmetre d’interès} \(\theta_0\) \sphinxstylestrong{(asimptòticament
sense biaix!) i amb una variança que decreix amb N}.

\sphinxstylestrong{Teorema 3.3}: Sota algunes condicions de regularitat de
\(f_X\), \(\sqrt{N {I}(\theta_0)}(\hat{\theta}^N - \theta_0) \Rightarrow \mathcal{N}(0, 1)\), on
\({I}(\theta) = - E\left(\frac{\partial^2}{\partial \theta^2}\log f(X; \theta) \right)\)
és la \sphinxstylestrong{Informació de Fisher}.

Abans de donar un esboç de la prova d’aquest resultat, mirem d’entendre’l.
Aquest resultat implica:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
L’EMV és \sphinxstylestrong{asimptòticament sense biaix}, \(\lim_{N \to \infty} E(\hat{\theta}^N - \theta_0) =0\).

\item {} 
La seva \sphinxstylestrong{variança asimptòtica} és inversament proporcional a N, i per tant l’EMV és \sphinxstylestrong{consistent}

\item {} 
Al límit, i independentment de la distribució de la mostra, \sphinxstylestrong{l’EMV es comporta com una Gaussiana!}

\item {} 
La \sphinxstylestrong{variança asimptòtica} depèn d’aquesta quantitat un pèl esotèrica \({I}(\theta)\)…

\end{enumerate}

Ara comprovarem computacionalment el resultat per un cas en particular,
quan \(X \sim \mbox{Poisson}(\lambda_0)\). Tenim que
\begin{equation*}
\begin{split}\log f_X(x;\lambda) = x\log \lambda - \lambda - \log x!\end{split}
\end{equation*}
i per tant:
\begin{equation*}
\begin{split}\frac{\partial^2}{\partial \theta^2} \log f_X(x;\lambda) = -\frac{x}{\lambda^2}\end{split}
\end{equation*}
aleshores: \({I}(\lambda)= - E\left(-\frac{X}{\lambda^2} \right)=\frac{1}{\lambda}\).

Per altra banda, l’EMV d’una mostra
iid de Poisson és simplement el moment mostral (Exercici!):

\(\hat{\lambda}^N = \bar{x}\)

Per tant, asimptòticament: \(\hat{\lambda}^N \sim \mathcal{N}(\theta_0, \frac{\lambda_0}{N})\)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{N} \PYG{o}{=} \PYG{l+m}{100} \PYG{c+c1}{\PYGZsh{} Tamany de cada mostra}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m}{1000} \PYG{c+c1}{\PYGZsh{} Nombre de repeticions}
\PYG{n}{lambda} \PYG{o}{=} \PYG{l+m}{15} \PYG{c+c1}{\PYGZsh{} Paràmetre de la població}

\PYG{n}{emv\PYGZus{}poisson} \PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
\PYG{n+nf}{for }\PYG{p}{(}\PYG{n}{i} \PYG{n}{in} \PYG{l+m}{1}\PYG{o}{:}\PYG{n}{n}\PYG{p}{)}\PYG{p}{\PYGZob{}}
  \PYG{n}{sample} \PYG{o}{=} \PYG{n+nf}{rpois}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{n}{lambda}\PYG{p}{)}
  \PYG{n}{emv\PYGZus{}poisson}\PYG{n}{[i}\PYG{n}{]} \PYG{o}{=} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}

\PYG{n}{x} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{n+nf}{min}\PYG{p}{(}\PYG{n}{emv\PYGZus{}poisson}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{max}\PYG{p}{(}\PYG{n}{emv\PYGZus{}poisson}\PYG{p}{)}\PYG{p}{,} \PYG{l+m}{0.1}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{emv\PYGZus{}poisson}\PYG{p}{,} \PYG{l+m}{20}\PYG{p}{,} \PYG{n}{freq} \PYG{o}{=} \PYG{n+nb+bp}{F}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{lambda}\PYG{p}{,} \PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{n}{lambda}\PYG{o}{/}\PYG{n}{N}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{green\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=250\sphinxpxdimen]{{ex_asimptotic}.png}\hspace*{\fill}}


\subsection{Justificació de la distribució asimptòtica de l’EMV}
\label{\detokenize{0_Intro/0_3_Estimacio:justificacio-de-la-distribucio-asimptotica-de-l-emv}}
Ara procedirem a justificar el Teorema 3.3, sense arribar a provar\sphinxhyphen{}lo, cosa
que requeriria tècniques molt més avançades que les d’aquest curs.

Primer de tot, alleugerirem la notació establint, com ja hem fet servir
en exemples anteriors:

\(L(\theta) := L( \theta; x_1, \cdots, x_m)\)

Imaginem\sphinxhyphen{}nos una \(L(\theta)\) “simpàtica” al voltant de \(\theta_0\):

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=300\sphinxpxdimen]{{likelihood_ex}.png}\hspace*{\fill}}

Recordeu de càlcul que l’expansió de Taylor de segon ordre d’una funció “suau”
\(f(x)\) al voltant d’un punt \(x_0\) és:
\begin{equation*}
\begin{split}f(x) \approx f(x_0) + (x - x_0) f'(x_0)\end{split}
\end{equation*}
Aleshores suposant que \(L'(\theta)\) és “suau”, tindrem que:
\begin{equation*}
\begin{split}L'(\theta) \approx L'(\theta_0) + (x - x_0) L''(\theta_0)\end{split}
\end{equation*}
i per tant, per \(\hat{\theta}\) que maximitza \(L(\theta)\),
haurà de verificar la condició d’optimalitat:
\begin{equation*}
\begin{split}0 = L'(\hat{\theta}) \approx L'(\theta_0) + (\hat{\theta} - \theta_0) L''(\theta_0)\end{split}
\end{equation*}
Cosa que ens permet concloure que:
\begin{equation*}
\begin{split}(\theta - \theta_0) \approx \frac{L'(\theta_0)}{L''(\theta_0)}\end{split}
\end{equation*}
Gràficament:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=500\sphinxpxdimen]{{likelihood_ex_2}.png}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=500\sphinxpxdimen]{{likelihood_ex_3}.png}
\end{figure}

Abans de continuar, farem un petit escalat de la darrera equació:
\begin{equation*}
\begin{split}\sqrt{N}(\theta - \theta_0) \approx \frac{\frac{1}{\sqrt{N}} L'(\theta_0)}{ \frac{1}{N} L''(\theta_0)}\end{split}
\end{equation*}
Ara, fixem\sphinxhyphen{}nos que en el cas iid, el numerador:
\begin{equation*}
\begin{split}\frac{1}{\sqrt{N}} L'(\theta_0) =\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \frac{\partial}{\partial \theta} \log f_X(x_i;\theta_0)\end{split}
\end{equation*}
és una suma de v.a. iid (\(\log f_X(x_i;\theta_0)\)) amb mitja 0, per la raó que hem vist
en el Teorema 3.1, i variança:
\begin{equation*}
\begin{split}\mbox{Var}(\frac{1}{\sqrt{N}} L'(\theta_0)) =E( \frac{\partial}{\partial \theta} \log f_X(X;\theta_0))^2\end{split}
\end{equation*}
per propietats de la variança de la suma de v.a. iid.

Per continuar, necessitarem un resultat auxiliar:

Lema 3.4: Sota algunes condicions de regularitat de
\(f_X\), \(E( \frac{\partial}{\partial \theta} \log f_X(X;\theta))^2 = - E( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)) = I(\theta)\)

\sphinxstyleemphasis{Justificació}: Com que \(\int f_X(x;\theta)dx = 1\),
\begin{equation*}
\begin{split}0 & = \frac{\partial}{\partial \theta} \int f_X(x;\theta)dx = \int  \frac{\partial}{\partial \theta} f_X(x;\theta) dx\\
0 & = \int  (\frac{\partial}{\partial \theta} \log f_X(X;\theta))  f_X(x;\theta) dx = \frac{\partial}{\partial \theta} \int  (\frac{\partial}{\partial \theta} \log f_X(X;\theta))  f_X(x;\theta) dx \\
0 & = \int  (\frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta))  f_X(x;\theta)dx + \\
  & + \int (\frac{\partial}{\partial \theta} \log f_X(X;\theta))^2 f_X(x;\theta)dx\end{split}
\end{equation*}
Combinant aquest últim resultat amb l’aplicació del TLC, podem concloure que
\begin{equation*}
\begin{split}\frac{1}{\sqrt{N}} L'(\theta_0) \Rightarrow \mathcal{N}(0, I(\theta_0))\end{split}
\end{equation*}
Per altra banda, pel denominador tenim:
\begin{equation*}
\begin{split}\frac{1}{N} L''(\theta) = \frac{1}{N} \sum_i \frac{\partial^2}{\partial \theta^2} \log f_X(x_i;\theta)\end{split}
\end{equation*}
que, per la Llei dels Grans Nombres:
\begin{equation*}
\begin{split}\frac{1}{N} L''(\theta) \to E(\frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)) = -I(\theta)\end{split}
\end{equation*}
Combinant aquests dos resultats, veiem que
\begin{equation*}
\begin{split}\frac{\frac{1}{\sqrt{N}} L'(\theta_0)}{ \frac{1}{N} L''(\theta_0)} \Rightarrow  \mathcal{N}(0, I^{-1}(\theta_0))\end{split}
\end{equation*}
que és el resultat que buscàvem justificar.

\sphinxstylestrong{Interpretació de la Informació de Fisher} (\(I(\theta)\)):
\begin{itemize}
\item {} 
Fixeu\sphinxhyphen{}vos que  \(L''(\theta_0)\) és asimptòticament proporcional a \(I(\theta)\)

\item {} 
\(L''(\theta_0)\) mesura la corvatura de \(L(\theta)\) al voltant de \(\theta_0\)

\item {} 
Quan més corbatura, menys variança, quan més “plana” més variança.

\item {} 
=\textgreater{} La “forma” de la nostra distribució determina la variança asimptòtica de l’estimador

\end{itemize}


\subsection{Eficiència i Cota de Cramer\sphinxhyphen{}Rao}
\label{\detokenize{0_Intro/0_3_Estimacio:eficiencia-i-cota-de-cramer-rao}}
L’últim concepte teòric que considerarem en aquest tema
és el de l’eficiència.

En aquest curs no ho hem vist, però hi ha altres metodologies
per obtenir estimadors com el \sphinxhref{https://en.wikipedia.org/wiki/Method\_of\_moments\_(statistics)}{Mètode dels Moments},
els estimadors de \sphinxhref{https://en.wikipedia.org/wiki/James\%E2\%80\%93Stein\_estimator}{James\sphinxhyphen{}Stein},
o els \sphinxhref{https://en.wikipedia.org/wiki/Bayes\_estimator}{estimadors Bayesians}.

Per tant, per un mateix paràmetre, podem trobar\sphinxhyphen{}nos amb diferents
“receptes” per construir\sphinxhyphen{}ne un estimador.

La pregunta que ens ocupa és: Donats diferents estimadors d’un mateix paràmetre, com n’escollim el millor?

La resposta a aquesta pregunta és: “depèn”.

Dependrà del que volguem
fer a posteriori amb aquest estimador, però un criteri bastant acceptat
és el de comparar\sphinxhyphen{}los segons el seu Error Quadràtic Mitjà (denominat MSE per les sigles en anglès), que
com sabem es pot descomposar com la suma de la Variança de l’Estimador
i del quadrat del seu biaix:
\begin{equation*}
\begin{split}\mbox{MSE}(\hat{\theta}) & = E(\hat{\theta}^N - \theta_0)^2 \\
                         & = \mbox{Var}(\hat{\theta}) + b(\hat{\theta})^2\end{split}
\end{equation*}
Si ens restringim a estimadors sense biaix, comparar\sphinxhyphen{}ne l’MSE és
equivalent a comparar\sphinxhyphen{}ne les variànces, cosa que dona lloc a la definició
d’eficiència:

Un estimador sense biaix és \sphinxstylestrong{eficient} si té menys o igual variança que
qualsevol altre estimador

Aquesta definició no és massa constructiva: per trobar l’estimador eficient,
hauriem de construïr tots els estimadors possibles (infinits!), calcular\sphinxhyphen{}ne
la variança, i finalment escollir el que en té menys.

Per sort, un dels resultats més importants de l’estadística,
desenvolupat als anys 40 del s. XX per Harald Crámer i C.R. Rao,
ens diu:

\sphinxstylestrong{Teorema 3.5}: Sigui \(X_1, \cdots, X_N\) una mostra iid
amb \(X \sim f_X(x;\theta_0)\) i \(\tilde{\theta}\) un estimador
sense biaix de \(\theta_0\). Aleshores, sota certes condicions
de regularitat de \(f_X(x;\theta)\),
\(\mbox{Var}(\tilde{\theta}) \geq \frac{1}{N I(\theta_0)}\).

Fixeu\sphinxhyphen{}vos\sphinxhyphen{}hi que per tant, \sphinxstylestrong{l’EMV és asimptòticament eficient}
ja que \(\mbox{Var}(\hat{\theta}) \to \frac{1}{N I(\theta_0)}\).


\section{Intervals de confiança per EMVs}
\label{\detokenize{0_Intro/0_3_Estimacio:intervals-de-confianca-per-emvs}}

\subsection{Intervals de confiança}
\label{\detokenize{0_Intro/0_3_Estimacio:intervals-de-confianca}}
Durant el curs ja hem treballat diverses vegades amb Intervals de Confiança,
que vam introduïr per primer cop a la Pràctica \#2.

Un \sphinxstylestrong{interval de confiança} de nivell \(1-\alpha\) (a la Pràctica \#2
parlàvem de nivell \(\alpha\) enlloc de \(1-\alpha\) però aquesta darrera convenció
és més comuna) per un paràmetre \(\mu\) és un estadístic
(per tant una v.a. que és una funció de la mostra)
format per dos nombres \(L\) i \(U\) tals que:
\begin{equation*}
\begin{split}P([L, U] \ni \mu) = 1 - \alpha\end{split}
\end{equation*}
És important entendre que la quantitat aleatòria aquí és el conjunt \([L, U]\)
i no \(\mu\). L’interpretació d’aquesta probabilitat és que, si agaféssim M mostres
(cada una de tamany N) i calculéssim M intervals (un per cada mostra), hauriem
d’esperar que una fracció \(1 - \alpha\) dels mateixos contenen \(\mu\).


\subsection{Intervals exactes}
\label{\detokenize{0_Intro/0_3_Estimacio:intervals-exactes}}
En alguns casos específics, podrem calcular I.C’s de manera “exacta”,
és a dir, calculant la f.d.p. \(f_{\hat{\theta}}\) de l’estimador
del paràmetre d’interès, i utilitzant\sphinxhyphen{}la per calcular un I.C.

Aquest és el cas, per exemple, de l’EMV de la mitja i variança d’una mostra Gaussiana,
on, com vam veure a la \sphinxhref{https://e-aules.uab.cat/2020-21/mod/assign/view.php?id=178383}{Pràctica 2}:
\begin{equation*}
\begin{split}\frac{\sqrt{N}(\hat{\mu} - \mu)}{\hat{\sigma}} \sim t_{N-1}\end{split}
\end{equation*}
ja que l’EMV per la mitja i variança Gaussiana és
\(\hat{\mu}=\bar{x}\) i \(\hat{\sigma}^2 = \frac{N-1}{N}S_X^2\).
Per altra banda, \sphinxhref{https://atibaup.github.io/ModInfer\_2020/slides/0\_Intro/0\_2\_Intro\_stats.html\#29}{un dels resultats que vam veure al Tema 2} és que:
\begin{equation*}
\begin{split}\frac{(N-1)S_X^2}{\sigma^2} \sim \chi_{N-1}^2\end{split}
\end{equation*}
Podem fer servir aquests dos resultats per
calcular IC’s per \(\hat{\mu}, \hat{\sigma}\) com segueix:

1) Com que la distribució \(t_{N-1}\) és simètrica
al voltant de 0, i denotant per \(\phi_t(x)\)
la seva f.d.c. inversa, tindrem que:
\begin{equation*}
\begin{split}P(-\phi_t(\frac{\alpha}{2}) \leq \frac{\sqrt{N}(\hat{\mu} - \mu)}{\hat{\sigma}} \leq \phi_t(\frac{\alpha}{2}))\end{split}
\end{equation*}
d’aquí podem concloure que l’interval de confiança de nivell \(1 - \alpha\)
per \(\mu\) és:
\begin{equation*}
\begin{split}\hat{\mu} \pm \phi_t(\frac{\alpha}{2})\sqrt{\frac{\hat{\sigma}^2}{N}}\end{split}
\end{equation*}
Noteu la similitud i les diferències respecte l’interval de confiança que obtindriem pel TLC.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\setcounter{enumi}{1}
\item {} 
Per altra banda com que

\end{enumerate}
\begin{equation*}
\begin{split}\frac{N \hat{\sigma}^2}{\sigma^2} = \frac{(N-1)S_X^2}{\sigma} \sim \chi_{N-1}^2\end{split}
\end{equation*}
i denotant per \(\phi_{\chi_{N-1}^2}(x)\) la f.d.c. inversa
d’una \(\chi_{N-1}^2\), tindrem:
\begin{equation*}
\begin{split}P\left(\phi_{\chi_{N-1}^2}(1 - \frac{\alpha}{2}) \leq \frac{N \hat{\sigma}}{\sigma} \leq \phi_{\chi_{N-1}^2}(\frac{\alpha}{2})\right) = 1- \alpha\end{split}
\end{equation*}
i per tant, l’IC de nivell \(1-\alpha\):
\begin{equation*}
\begin{split}\left[\frac{N \hat{\sigma}^2}{\phi_{\chi_{N-1}^2}(\frac{\alpha}{2})}, \frac{N \hat{\sigma}^2}{\phi_{\chi_{N-1}^2}(1 - \frac{\alpha}{2})}\right]\end{split}
\end{equation*}
que com podeu observar no és simètric com en el cas anterior.


\subsection{Intervals aproximats asimptòtics}
\label{\detokenize{0_Intro/0_3_Estimacio:intervals-aproximats-asimptotics}}
En general serà difícil caracteritzar la f.d.p. dels nostres estimadors, i
per tant haurem de recórrer a aproximacions, com la que vam veure
en la teoria asimptòtica de l’EMV (Teorema 3.3 d’aquestes diapos):
\begin{equation*}
\begin{split}\sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \Rightarrow \mathcal{N}(0, 1)\end{split}
\end{equation*}
on \(I(\theta)\) és
la Informació de Fisher. Si sapiguéssim el valor de \(\theta_0\), podriem fer
servir el desenvolupament que ja hem fet servir múltiples vegades
per trobar un IC de nivell \(1  - \alpha\). Per N suficientment gran:
\begin{equation*}
\begin{split}P\left(\sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \leq \phi\left(1 - \frac{\alpha}{2}\right)\right) &\approx 1 - \frac{\alpha}{2} \\
P\left(\sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \leq \phi\left(\frac{\alpha}{2}\right)\right) & \approx \frac{\alpha}{2}\end{split}
\end{equation*}
on \(\phi(x)\) és la f.d.c. inversa (funció de quantils) d’una normal estàndard.

Per tant, sabent que \(\phi\left(1 - \frac{\alpha}{2}\right)= -  \phi\left(\frac{\alpha}{2}\right)\),
\begin{equation*}
\begin{split}P\left( \phi\left(\frac{\alpha}{2}\right) \leq \sqrt{N {I}(\theta_0)}(\hat{\theta} - \theta_0) \leq -\phi\left(\frac{\alpha}{2}\right)\right) \approx 1 - \alpha\end{split}
\end{equation*}
cosa que justificaria el següent IC aproximat per l’EMV d’una mostra iid d’una
població \(f_X(x;\theta)\):
\begin{equation*}
\begin{split}\hat{\theta} \pm \phi\left(\frac{\alpha}{2}\right) \sqrt{\frac{1}{N I(\theta_0)}}\end{split}
\end{equation*}
on \(\phi\) és la f.d.c. inversa d’una Normal estàndard i \(I(\theta)\) és
la Informació de Fisher.

Com que no coneixem \(\theta_0\), aquesta última expressió és inútil ja que depèn
de \(I(\theta_0)\).

Farem doncs servir el \sphinxstylestrong{principi de substitució} (plug\sphinxhyphen{}in principle),
que ja hem fet servir altres vegades, i substituïrem \(I(\theta_0)\) per \(I(\hat{\theta})\),
sota el precepte de que per N suficientment gran \(\hat{\theta} \to \theta_0\).

Per tant arribem a:
\begin{equation*}
\begin{split}\hat{\theta} \pm \phi\left(\frac{\alpha}{2}\right) \sqrt{\frac{1}{N I(\hat{\theta})}}\end{split}
\end{equation*}
Fixeu\sphinxhyphen{}vos en la similitud/diferències entre aquesta expressió i els Intervals de Confiança obtinguts
en el cas on podíem derivar intervals de confiança exactes.


\subsection{Unes notes sobre el càlcul de \protect\(I(\hat{\theta})\protect\)}
\label{\detokenize{0_Intro/0_3_Estimacio:unes-notes-sobre-el-calcul-de-i-hat-theta}}
A l’última entrega de problemes, he vist que hi havia una mica de confusió
respecte el càlcul de la Informació de Fisher \(I(\theta)\).

En molts casos, podrem calcular \(I(\theta)\) analíticament,
a través d’una de les seves dues definicions alternatives:
\begin{equation*}
\begin{split}I(\theta) &= E( \frac{\partial}{\partial \theta} \log f_X(X;\theta))^2 \\
I(\theta) &= - E( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta))\end{split}
\end{equation*}
(Podem escollir la que ens vagi millor per al problema.) Fixeu\sphinxhyphen{}vos que la v.a.
aleatòria de la que volem calcular l’esperança aquí és una funció de
\(\log f_X(X;\theta)\) (la seva derivada al quadrat o la seva  segona derivada), que a la vegada és una funció de \(X\).
L’esperança, per tant, es calcula respecte a \(X \sim f_X\).

En alguns casos, calcular aquesta esperança serà massa difícil (com
per exemple a l’Exercici 2 de l’entrega de Problemes), i haurem
de calcular\sphinxhyphen{}la de manera aproximada. És aquí on podem recórrer de nou a la \sphinxhref{https://atibaup.github.io/ModInfer\_2020/slides/0\_Intro/0\_2\_Intro\_stats.html\#25}{Llei dels Grans Nombres},
que ens diu que per qualsevol funció \(g(x)\) d’una variable aleatòria
\(Y\), i sota algunes condicions de regularitat,
\begin{equation*}
\begin{split}\frac{1}{N}\sum_i g(Y_i) \to E(g(Y))\end{split}
\end{equation*}
quan \(N \to \infty\). Per tant, si tenim una mostra gran
de \(X \sim f_X\) (o si la podem simular), podrem aproximar:
\begin{equation*}
\begin{split}I(\theta) &\approx \frac{1}{N}\sum_i \left(\frac{\partial}{\partial \theta} \log f_X(X_i;\theta)\right)^2 \\
I(\theta) &\approx - \frac{1}{N}\sum_i \left( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)\right)\end{split}
\end{equation*}
Fixeu\sphinxhyphen{}vos que aquesta última expressió dóna lloc a l’aproximació
que veu trobar alguns per Internet:
\begin{equation*}
\begin{split}I(\theta) \approx - \frac{1}{N}\sum_i \left( \frac{\partial^2}{\partial \theta^2} \log f_X(X;\theta)\right) = \frac{1}{N}\frac{\partial^2}{\partial \theta^2} L(\theta)\end{split}
\end{equation*}
quan la mostra és iid, on simplement hem aplicat la definició de \(L(\theta)\) i
intercanviat l’ordre de l’operador suma i derivada.


\subsection{Intervals aproximats per Bootstrap}
\label{\detokenize{0_Intro/0_3_Estimacio:intervals-aproximats-per-bootstrap}}
L’última tècnica que veurem per calcular Intervals de Confiança
és la més potent, ja que no requereix cap suposició
pel que fa a la distribució de la mostra, i funciona en el règim no\sphinxhyphen{}asimptòtic.

Fixeu\sphinxhyphen{}vos que en el desenvolupament anterior ens hem basat (implícitament)
en que coneixiem la distribució de:
\begin{equation*}
\begin{split}\Lambda = \hat{\theta} - \theta_0 \sim f_{\Lambda}(x;\theta_0)\end{split}
\end{equation*}
Si coneixem \(f_{\Lambda}\), vol dir que també coneixem la seva f.d.c.
inversa, \(\phi_{\Lambda}\) i per tant podem trobar Intervals
de Confiança ja que:
\begin{equation*}
\begin{split}P\left( \phi_{\Lambda}\left(\frac{\alpha}{2}\right)\leq \hat{\theta} - \theta_0 \leq \phi_{\Lambda}\left(1 - \frac{\alpha}{2}\right)\right) = 1 - \alpha\end{split}
\end{equation*}
En el cas asimptòtic que hem vist abans, teniem que \(\Lambda \approx \mathcal{N}\left(0, \frac{1}{N I(\hat{\theta})}\right)\).

En general, no coneixem \(f_{\Lambda}(x;\theta_0)\) i per tant
ens quedem encallats.

El mètode de Bootstrap paramètric, el que proposa és:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Generar una mostra Bootstrap de talla M: \(\Lambda_i \sim f_{\Lambda}(x;\hat{\theta}), i=1, \cdots, M\) (fixeu\sphinxhyphen{}vos que hem remplaçat \(\theta_0\) per \(\hat{\theta}\))

\item {} 
Estimar \(\phi_{\Lambda}\left(\frac{\alpha}{2}\right)\) i \(\phi_{\Lambda}\left(1 - \frac{\alpha}{2}\right)\) a partir dels quantils de la mostra \(\Lambda_1, \cdots, \Lambda_M\)

\end{enumerate}

per trobar un interval de confiança aproximat:
\begin{equation*}
\begin{split}P\left( \hat{\phi}_{\Lambda}\left(\frac{\alpha}{2}\right)\leq \hat{\theta} - \theta_0 \leq \hat{\phi}_{\Lambda}\left(1 - \frac{\alpha}{2}\right)\right) \approx 1 - \alpha\end{split}
\end{equation*}
Ens queda aclarir com generar \(\Lambda_i \sim f_{\Lambda}(x;\hat{\theta})\)…

Per generar \(\Lambda \sim f_{\Lambda}(x;\hat{\theta})\), ho farem
de manera indirecta, ja que com hem dit en general no coneixem
\(f_{\Lambda}\). El que sí que coneixem, en principi, és \(f_X(x;\theta)\),
la f.d.p. o f.m.p de la nostra mostra.

Per tant generarem \(\Lambda\) indirectament. Per fer\sphinxhyphen{}ho, repetirem \(M\) vegades
el següent procés:

1. Generar \sphinxstylestrong{una} mostra bootstrap de \(X \sim f_X(x;\hat{\theta})\) de talla N, segons el valor
\(\hat{\theta}\) que hem trobat a partir de les nostres dades.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
Calcularem l’EMV sobre aquesta mostra bootstrap, i l’anomenarem \(\hat{\theta}^*\)

\item {} 
Calcularem \(\Lambda = \hat{\theta}^* - \hat{\theta}\)

\end{enumerate}

Al final d’aquest procés haurem obtingut la mostra Bootstrap de \(\Lambda\) i
podrem calcular\sphinxhyphen{}ne els quantils, per calcular l’IC aproximat que en vist en l’anterior diapo.


\chapter{Tema 4: Tests d’hipòtesis}
\label{\detokenize{0_Intro/0_4_Tests:tema-4-tests-d-hipotesis}}\label{\detokenize{0_Intro/0_4_Tests::doc}}

\section{Raó de Versemblances i Neyman\sphinxhyphen{}Pearson}
\label{\detokenize{0_Intro/0_4_Tests:rao-de-versemblances-i-neyman-pearson}}

\subsection{Exemple de motivació}
\label{\detokenize{0_Intro/0_4_Tests:exemple-de-motivacio}}
Començarem considerant un dels exemples que
vam veure al Tema 2:
\begin{itemize}
\item {} 
Tenim un circuit integrat per K components en sèrie

\item {} 
El fabricant dels components ens assegura que cada component té una proba diària de fallar \(\rho_X < 0.01\)

\end{itemize}

Durant els nostres experiments de control de qualitat, observem
que el circuit falla després  de 57, 49, 8, 234, 4, 181, 153,22,  91,  11 dies.

Està respectant el fabricant de components la garantia?

Al Tema 2, vem aprendre que \(Y\), el temps de vida del circuit,
seguia una distribució geomètrica amb paràmetre \(\rho_Y = 1 - (1-\rho_X)^K\),
i vem derivar\sphinxhyphen{}ne l’EMV:
\begin{equation*}
\begin{split}\hat{\rho}_Y = \frac{N}{\sum_i y_i}\end{split}
\end{equation*}
Apartir d’això vam estimar:
\begin{equation*}
\begin{split}\hat{\rho}_X = 1 - (1 - \hat{\rho}_Y)^\frac{1}{K} \approx 0.001\end{split}
\end{equation*}
i vam concloure que el fabricant estava respectant la garantia, ja que \(\hat{\rho}_X \ll 0.01\).

Però què decidirieu si ens hagués sortit \(\hat{\rho}_X \approx 0.01\)? O \(\hat{\rho}_X \approx 0.008\)?

De la teoria del Tema 2, sabem que l’EMV \(\hat{\rho}_Y\) és
una quantitat aleatòria, i per tant \(\hat{\rho}_X\) també. Per
exemple, vegem la distribució de \(\hat{\rho}_X\) quan \(\rho_X=0.008\):

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=250\sphinxpxdimen]{{geom_null}.png}\hspace*{\fill}}

Quan \(\rho_X = 0.008\), \(P(\hat{\rho}_X > 0.01; \rho_X = 0.008) \approx 37\%\)! Si
\(\rho_X = 0.0001\), aquesta probabilitat és més petita, però no és nula. Això és
degut a l’error d’estimació (l’EMV és consistent asimptòticament, però estem treballant amb mostres finites!).
Això s’anomena un \sphinxstylestrong{Fals Positiu}.

Per altra banda, suposem que \(\rho_X = 0.02\) (2x més gran que 1\%).
Degut a l’aleatorietat de \(\hat{\rho}_X\), també
tindrem una probabilitat no nula de que \(\hat{\rho}_X < 0.01\):

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=250\sphinxpxdimen]{{geom_H1}.png}\hspace*{\fill}}

Decidir que, en base a les observacions, \(\rho_X < 0.01\) quan en realitat \(\rho_X > 0.01\) s’anomena un \sphinxstylestrong{Fals Negatiu}.

Malgrat aquestes dues observacions, la intuició ens diu que si
\(\hat{\rho}_X\) és molt més petit que \(\rho_c = 0.01\),
o de manera equivalent, \(T := \hat{\rho}_X - \rho_c\) és prou
petit, hauriem de poder afirmar que \(\rho_X\) és efectivament
més petit que 0.01 i per tant el fabricant està complint la garantia.

Ja podem definir alguns conceptes claus:
\begin{itemize}
\item {} 
\(T\) s’anomena \sphinxstylestrong{l’estadístic del test}, que calculem a partir de la mostra (en aquest exemple depèn de \(\hat{\rho}_X\))

\item {} 
El valor \(T_0\) contra el que comparem \(T\) per tal de prendre una decisió, s’anomena \sphinxstylestrong{valor crític}.

\item {} 
En el nostre exemple, el conjunt \(T \leq T_0\) és la \sphinxstylestrong{regió d’acceptació del test}, el complementari n’és la \sphinxstylestrong{regió crítica}.

\end{itemize}

Quan escollim \(T_0\), determinem implícitament
la quantitat de Falsos Positius i Falsos Negatius que tindrem. En \sphinxstylestrong{aquest exemple}, quan més gran \(T_0\), menys
“estricte” és el nostre criteri, i per tant més Falsos Positius i menys Falsos Negatius. Quan més petit, a l’inversa.


\subsection{El paradigma de Neyman\sphinxhyphen{}Pearson i Fisher}
\label{\detokenize{0_Intro/0_4_Tests:el-paradigma-de-neyman-pearson-i-fisher}}
La intuició que estem fent servir sembla raonable però ens falta una manera quantitativa
d’escollir quan \(T\) és “prou petit” o no (és a dir, escollir el valor crític \(T_0\).)
El paradigma que utilitzarem ens permetrà fer això. Però primer haurem de definir una sèrie de conceptes:
\begin{itemize}
\item {} 
\sphinxstylestrong{L’Hipòtesis Nula} (\(H_0\)): És l’hipòtesi sobre \sphinxstylestrong{el model estadístic} que volem contrastar/refutar

\item {} 
\sphinxstylestrong{L’Hipòtesis Alternativa} (\(H_1\)): És l’alternativa de l’hipòtesi nula (no necessàriament complementària!)

\item {} 
\sphinxstylestrong{L’Error de Tipus I} o Fals Positiu: Quan el nostre test refusa \(H_0\) quan aquesta és certa

\item {} 
\sphinxstylestrong{L’Error de Tipus II} o Fals Negatiu: Quan el nostre test accepta \(H_0\) quan \(H_1\) és certa

\item {} 
\(\alpha = P(\mbox{refusar } H_0 ; H_0)\): el \sphinxstylestrong{nivell de significació} o probabilitat d’Error de Tipus I

\item {} 
\(\beta = P(\mbox{acceptar } H_0 ; H_1)\): la probabilitat d’Error de Tipus II

\item {} 
\sphinxstylestrong{La potència del test} (\sphinxstyleemphasis{power}): \(1 - \beta\), la probabilitat \(P(\mbox{refusar } H_0 ; H_1)\).

\end{itemize}

Per tant, per especificar un test i caracteritzar\sphinxhyphen{}lo, necessitarem especificar \(H_0\),
\(H_1\), i la regió crítica…
\begin{itemize}
\item {} 
Fixeu\sphinxhyphen{}vos que, intuitivament, hi ha una tensió entre \(\alpha\) i \(1- \beta\).

\item {} 
En teoria, el test ideal és el que té \(\alpha=0\) i \(1 - \beta = 1\), és a dir no fa cap fals positiu ni cap fals negatiu.

\item {} 
A la pràctica el test ideal no existeix, i ens conformarem amb els tests que, donat \(\alpha\), tenen millor potència.

\end{itemize}
\begin{itemize}
\item {} 
\(H_0\): \(\rho_X - \rho_c \leq 0\), és a dir el fabricant ens diu la veritat (recordeu \(\rho_c:=0.01\))

\item {} 
\(H_1\): \(\rho_X - \rho_c > 0\), és a dir el fabricant ens ha mentit

\item {} 
\sphinxstylestrong{La regió crítica}: \(T > T_0\) on rebutjem \(H_0\)

\item {} 
\sphinxstylestrong{L’Error de Tipus I}: L’estadístic ens dona \(T > T_0\) però \(\rho_X \leq 0.01\).

\item {} 
\sphinxstylestrong{L’Error de Tipus II}: L’estadístic ens dona \(T \leq T_0\) però \(\rho_X > 0.01\).

\end{itemize}

Normalment primer escollirem un nivell de significació \(\alpha\), i a partir del mateix
escollirem \(T_0\) de manera que:
\begin{equation*}
\begin{split}P(T > T_0 ; H_0) = \alpha\end{split}
\end{equation*}
Això determinarà implícitament la potència del test \(1 - \beta = P(T > T_0 ; H_1)\).

Fixeu\sphinxhyphen{}vos que hi ha una “asimetria” entre \(H_0\) i \(H_1\), i que les escull el practicant…

Per trobar el valor crític amb una significació de \(\alpha=0.05\),
simularem \(Y \sim \mbox{Geomètrica}(1 - (1-\rho_X)^K)\) amb \(\rho_X=0.01\),
i buscarem \(T_0\) tal que \(P(T > T_0 ; H_0) = \alpha = 0.05\):

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=280\sphinxpxdimen]{{geom_at_alpha}.png}\hspace*{\fill}}

Fixeu\sphinxhyphen{}vos que per tenir una significació de \(\alpha=0.05\), només podrem
rebutjar l’hipòtesi Nula si \(\hat{\rho}_x > 0.022\)!
(això és en part perquè en el nostre exemple N=10, i degut això la variança del nostre estimador és gran).

Considereu els tests PCR que es fan per detectar la COVID\sphinxhyphen{}19.
\begin{itemize}
\item {} 
Quina seria \(H_0\)? I \(H_1\)?

\item {} 
Què és un fals positiu? I un fals negatiu?

\item {} 
Què és pitjor, un fals positiu? o un fals negatiu?

\item {} 
Quin és l’estadístic del test?

\item {} 
Quina creieu que és la regió crítica del test?

\item {} 
Què creieu que vol dir que els tests PCR tenen “alta sensibilitat”?

\end{itemize}


\subsection{Test de Raó de Versemblances (RV)}
\label{\detokenize{0_Intro/0_4_Tests:test-de-rao-de-versemblances-rv}}
El paradigma que hem explicat fins ara ens guia per escollir
el valor crític \(T_0\) quan ja tenim un estadístic de test \(T\) (
a l’exemple, \(T = \hat{\rho}_X - 0.01\))
sobre el que treballar, i una idea sobre quina hauria de ser
la regió crítica (a l’exemple \(T > T_0\)).

El següent Lemma estableix com construïr tests òptims per \sphinxstylestrong{hipòtesis simples}, és a dir
hipòtesis definides a partir del paràmetre \(\theta\) d’una població caracteritzada
per una f.d.p o f.m.p  \(f_X(x;\theta)\):
\begin{itemize}
\item {} 
\(H_0: \theta = \theta_0\)

\item {} 
\(H_1: \theta = \theta_1\)

\end{itemize}

\sphinxstylestrong{Lema 4.1 (de Neyman\sphinxhyphen{}Pearson, 1933)}: Per una mostra \(X_1, \cdots, X_N\) i \(H_0\) i \(H_1\)
hipòtesis simples, el test basat en l’estadístic de raó de versemblances \(T = \frac{f(X_1, \cdots, X_N; \theta_0)}{f(X_1, \cdots, X_N; \theta_1)}\)
amb regió crítica \(T \leq T_0\) i significació \(\alpha\), és el test amb més potència amb nivell de significació \(\alpha\).

Fixeu\sphinxhyphen{}vos que la versemblança d’una mostra:
\begin{equation*}
\begin{split}f(X_1, \cdots, X_N; \theta)\end{split}
\end{equation*}
és més gran quan més versemblant és que la mostra \(X_1, \cdots, X_N\) hagi estat generada
per el paràmetre \(\theta\). (Aquest és el mateix principi que vam fer servir per justificar
el Mètode de Màxima Versemblança per estimar \(\theta\) a partir de \(X_1, \cdots, X_N\)).

Per tant, si l’evaluem a dos valors diferents de \(\theta\), \(\theta_0\) i \(\theta_1\),
\(f_X(X_1, \cdots, X_N; \theta)\) hauria de ser més gran per aquell valor
que és més versemblant segons les dades. En conseqüència, l’estadístic
\begin{equation*}
\begin{split}T = \frac{f(X_1, \cdots, X_N; \theta_0)}{f(X_1, \cdots, X_N; \theta_1)}\end{split}
\end{equation*}
serà gran quan \(\theta_0\) és més versemblant que \(\theta_1\) i petita
en el cas contrari. Això justifica que refusem l’hipòtesi nula
\(\theta=\theta_0\) quan \(T \leq T_0\) per algun \(T_0\) a escollir.


\subsection{Exemple d’aplicació: detecció per infra\sphinxhyphen{}rojos}
\label{\detokenize{0_Intro/0_4_Tests:exemple-d-aplicacio-deteccio-per-infra-rojos}}
Considereu una mostra iid \(X_1, \cdots, X_N\) \sphinxstylestrong{normal} i de variança coneguda \(\sigma^2\),
obtinguda a partir de les mesures d’un sensor de detecció d’infra\sphinxhyphen{}rojos. Quan davant del sensor
hi ha un objecte, es mesura una senyal amb mitja \(\mu_1\), quan no, amb \(\mu_0\).
\begin{itemize}
\item {} 
\(H_0\): no hi ha cap objecte, \(\mu = \mu_0\)

\item {} 
\(H_1\): n’hi ha un, \(\mu = \mu_1\)

\end{itemize}

\noindent{\hspace*{\fill}\sphinxincludegraphics[height=250\sphinxpxdimen]{{exemple_ir}.png}\hspace*{\fill}}

Com que es tracta d’hipòtesis simples, segons el Lema 4.1 de Neyman\sphinxhyphen{}Pearson
és òptim fer servir el test de raó de versemblances. L’estadístic és:
\begin{equation*}
\begin{split}T &= \frac{f_X(X_1, \cdots, X_N; \mu_0)}{f_X(X_1, \cdots, X_N; \mu_1)} = \frac{\exp\left(-\frac{1}{2\sigma^2}\sum_i \left(X_i - \mu_0 \right)^2 \right)}{\exp\left(-\frac{1}{2\sigma^2}\sum_i \left(X_i - \mu_1 \right)^2 \right)} \\
  &= \exp\left(-\frac{1}{2\sigma^2}\left(\sum_i \left(X_i - \mu_0 \right)^2 - \sum_i \left(X_i - \mu_1 \right)^2 \right)\right) \\
  &= \exp\left(\frac{N}{2\sigma^2}\left(2\bar{X}(\mu_0 - \mu_1) + \mu_1^2 - \mu_0^2\right)\right)\end{split}
\end{equation*}
Fixeu\sphinxhyphen{}vos doncs que \(T\) depèn únicament de la mostra a través de \(\bar{X}\).
Per exemple, si \(\mu_0 > \mu_1\), \(T\) és petit si \(\bar{X}\) és petit,
per tant rebutjarem \(H_0\) si \(\bar{X}\) és petita. Per tant, per aquest test,
enlloc de fer servir \(T\) com estadístic, podem fer servir directament \(\bar{X}\)!

Ens queda només trobar el valor crític \(T_0\) tal que \(P(T \leq T_0; \mu_0) = \alpha\).

Com hem vist, \(T\) només depèn de la mostra a través d’una funció monotònica d’ \(\bar{X}\), per tant la regió crítica
es pot expressar en funció d’\(\bar{X}\):
\begin{equation*}
\begin{split} \left\{T \leq T_0 \right\} &= \left\{ \log T \leq \log T_0 \right\} \\
& = \left\{2\bar{X}(\mu_0 - \mu_1) + \mu_1^2 - \mu_0^2 \leq \frac{2 \sigma^2\log(T_0)}{N} \right\} \\
& = \left\{\bar{X} \leq X_0\right\} \mbox{ (si } \mu_0 > \mu_1 \mbox{ )}\end{split}
\end{equation*}
per \(X_0 = \frac{1}{2\left(\mu_0 - \mu_1\right)}\left(\frac{2 \sigma^2\log(T_0)}{N} + \mu_0^2 - \mu_1^2 \right)\).

Es a dir, enlloc de buscar \(T_0\) tal que \(P(T \leq T_0; \mu_0) = \alpha\),
buscarem directament \(X_0\) tal que \(P(\bar{X} \leq X_0; \mu_0) = \alpha\).

Per continuar, fixem\sphinxhyphen{}nos que sota
\(H_0\), \(X_1, \cdots, X_N \sim \mathcal{N}(\mu_0, \sigma^2)\) per tant
{[}\sphinxhref{https://atibaup.github.io/ModInfer\_2020/slides/0\_Intro/0\_2\_Intro\_stats.html\#29}{Diapo 29, Tema 2}{]}:
\begin{equation*}
\begin{split}\bar{X} \sim \mathcal{N}(\mu_0, \frac{\sigma^2}{N}).\end{split}
\end{equation*}
Podem doncs trobar \(X_0\) manipulant l’expressió de significació:
\begin{equation*}
\begin{split}P(\bar{X} \leq X_0; \mu_0) &= \alpha \\
P(\frac{\bar{X} - \mu_0}{\sqrt{\frac{\sigma^2}{N}}} \leq \frac{X_0 - \mu_0}{\sqrt{\frac{\sigma^2}{N}}}; \mu_0) &= \alpha\end{split}
\end{equation*}
on tenim que \(\frac{\bar{X} - \mu_0}{\sqrt{\frac{\sigma^2}{N}}} \sim \mathcal{N}(0, 1)\).

Així doncs, només caldrà trobar \(X_0\) tal que:
\begin{equation*}
\begin{split}\frac{X_0 - \mu_0}{\sqrt{\frac{\sigma^2}{N}}} = \phi\left(\alpha\right)\end{split}
\end{equation*}
on \(\phi(x)\) és la f.d.c. inversa d’una normal estàndard. Finalment:
\begin{equation*}
\begin{split}X_0 = \mu_0 + \sqrt{\frac{\sigma^2}{N}}\phi\left(\alpha\right)\end{split}
\end{equation*}
i per tant (en el cas on \(\mu_0 > \mu_1\)) rebutjarem
l’hipòtesi nula quan \(\bar{X} \leq \mu_0 + \sqrt{\frac{\sigma^2}{N}}\phi\left(\alpha\right)\)

\sphinxstylestrong{Exercici}: Quin valor crític \(X_0\) i regió crítica tindriem si
enlloc de \(\mu_0 > \mu_1\) tenim que \(\mu_0 \leq \mu_1\)?


\section{Test de Raó de Versemblances Generalitzada (RVG)}
\label{\detokenize{0_Intro/0_4_Tests:test-de-rao-de-versemblances-generalitzada-rvg}}

\subsection{Test de Raó de Versemblances Generalitzada (RVG)}
\label{\detokenize{0_Intro/0_4_Tests:id1}}
El test de Raó de Versemblances té propietats teòriques interessants (Lema 4.1.)
però és d’aplicació pràctica limitada, ja que sovint les nostres
hipòtesis seràn compostes, és a dir, del tipus:
\begin{itemize}
\item {} 
\(H_0: \theta \in \Theta_0\)

\item {} 
\(H_1: \theta \in \Theta_1\)

\end{itemize}

on \(\Theta_0\) i \(\Theta_1\) són subconjunts de \(\Theta\) i
per tant el test de Raó de Versemblances no és aplicable.

Per exemple, el nostre exemple inicial es tractava d’hipòtesis compostes:
\begin{itemize}
\item {} 
\(\Theta_0 = \left\{\rho_X \leq 0.01 \right\}\)

\item {} 
\(\Theta_1 = \left\{\rho_X > 0.01 \right\}\)

\end{itemize}

Donada una mostra \(X_1, \cdots, X_N\)
d’una població amb f.d.p. \(f_X\), i versemblança
\(f(X_1, \cdots, X_N; \theta)\), el Test de Raó de Versemblances Generalitzat, es basa en el següent estadístic:
\begin{equation*}
\begin{split}\Lambda = \frac{\max_{\theta \in \Theta_{0}} f(X_1, \cdots, X_N; \theta)}{\max_{\theta \in \Theta} f(X_1, \cdots, X_N; \theta)}\end{split}
\end{equation*}
(fixeu\sphinxhyphen{}vos que al denominador el max és respecte \(\Theta\) no \(\Theta_1\)
amb regió crítica:
\begin{equation*}
\begin{split}\Lambda \leq \lambda_0\end{split}
\end{equation*}
i \(\lambda_0\) tal que \(P(\Lambda \in \lambda_0; \theta_0) = \alpha, \forall \theta_0 \in \Theta_0\).

Per evaluar l’estadístic del test d’RVG, cal doncs trobar l’EMV sota cada una de les hipòtesis


\subsection{Exemple d’aplicació: test sobre la mitja d’una Gaussiana amb variança coneguda}
\label{\detokenize{0_Intro/0_4_Tests:exemple-d-aplicacio-test-sobre-la-mitja-d-una-gaussiana-amb-varianca-coneguda}}
Considereu una mostra iid \(X_1, \cdots, X_N\) \sphinxstylestrong{normal} i
de variança coneguda \(\sigma^2\).

Volem testejar si la mitja de la població és un valor donat \(\mu_0\) o no.
Les hipòtesis son doncs:
\begin{itemize}
\item {} 
\(H_0: \mu= \mu_0\) (simple)

\item {} 
\(H_1: \mu \neq \mu_0\) (composta)

\end{itemize}

Podeu pensar en una situació pràctica on voldriem testejar aquesta hipòtesi?

Com que l’hipòtesis nula és simple, en aquest exemple, l’estadístic del test RVG és:
\begin{equation*}
\begin{split}\Lambda = \frac{f(X_1, \cdots, X_N; \mu_0)}{\max_{\mu \in \Theta} f(X_1, \cdots, X_N; \mu)}\end{split}
\end{equation*}
Per tant haurem de trobar l’EMV (\(\max_{\theta \in \Theta} f(X_1, \cdots, X_N; \theta)\))
per calcular\sphinxhyphen{}lo. Però al tractar\sphinxhyphen{}se d’una mostra normal, ja sabem que l’EMV
de la mitja \(\mu\) és simplement la mitjana aritmètica de la mostra:

\(\hat{\mu} = \bar{X}\)

Per tant en aquest exemple l’estadístic es simplifica a:
\begin{equation*}
\begin{split}\Lambda = \frac{f(X_1, \cdots, X_N; \mu_0)}{f(X_1, \cdots, X_N; \bar{X})} = \frac{\Pi_i f_X(X_i; \mu_0)}{\Pi_i f_X(X_i; \bar{X})}\end{split}
\end{equation*}
on \(f_X\) és la f.d.p d’una Gaussiana.

Al numerador tindrem

\(\frac{1}{\left(\sqrt{2\pi\sigma^2}\right)^N}\exp\left(-\frac{1}{2\sigma^2}\sum_i(X_i - \mu_0)^2 \right)\)

i al denominador

\(\frac{1}{\left(\sqrt{2\pi\sigma^2}\right)^N}\exp\left(-\frac{1}{2\sigma^2}\sum_i(X_i - \bar{X})^2 \right)\)

Per tant, trobarem:
\begin{equation*}
\begin{split}\Lambda = \exp\left(-\frac{1}{2\sigma^2}\left[\sum_i(X_i - \mu_0)^2 - \sum_i(X_i - \bar{X})^2\right] \right)\end{split}
\end{equation*}
Amb una mica d’àlgebra, això es pot simplicar a
\begin{equation*}
\begin{split}\Lambda = \exp\left(-\frac{N}{2\sigma^2}\left(\bar{X} - \mu_0\right)^2  \right)\end{split}
\end{equation*}
per tant la regió crítica serà:
\begin{equation*}
\begin{split}\left\{\Lambda: \Lambda \leq \lambda_0\right\} &= \left\{\bar{X}: \exp\left(-\frac{N}{2\sigma^2}\left(\bar{X} - \mu_0\right)^2  \right) \leq \lambda_0 \right\} \\
& = \left\{\bar{X}: \left(\frac{\bar{X} - \mu_0}{\sqrt{\frac{\sigma^2}{N}}}\right)^2  \geq - 2 \log \lambda_0\right\} \\\end{split}
\end{equation*}
arribem a:
\begin{equation*}
\begin{split}\left\{\Lambda: \Lambda \leq \lambda_0\right\}  = \left\{\bar{X}: \left| \frac{\bar{X} - \mu_0}{\sqrt{\frac{\sigma^2}{N}}}\right| \geq X_0 \right\}\end{split}
\end{equation*}
Per \(X_0=\sqrt{- 2 \log \lambda_0 }\). És a dir, rebutjarem \(H_0\) quan la diferència entre \(\bar{X}\) i \(\mu_0\) sigui prou gran, relativa a la variança de \(\bar{X}\).

De nou, com que la mostra és Gaussiana, \(\frac{\bar{X} - \mu_0}{\sqrt{\frac{\sigma^2}{N}}} \sim \mathcal{N}(0, 1)\),
podem buscar el valor crític \(X_0\) com segueix:
\begin{equation*}
\begin{split}P(\Lambda \leq \lambda_0; \mu_0) = 1 - P(-X_0 \leq \frac{\bar{X} - \mu_0}{\sqrt{\frac{\sigma^2}{N}}} \leq X_0; \mu_0) = \alpha\end{split}
\end{equation*}
que és el mateix que:
\begin{equation*}
\begin{split}1 - (F_Z(X_0) - F_Z(-X_0)) = \alpha\end{split}
\end{equation*}
on \(F_Z\) és la f.d.c. d’una normal estàndard. Com que \(F_Z(x) = 1 - F_Z(-x)\),
concluïm que el valor crític serà tal que \(X_0\):
\begin{equation*}
\begin{split}F_Z(X_0) &= \frac{\alpha}{2} \\
X_0 &= \phi\left(\frac{\alpha}{2}\right)\end{split}
\end{equation*}
i per tant rebutjarem \(H_0\) quan:
\begin{equation*}
\begin{split}\bar{X} \not \in \left[\mu_0 - \phi\left(\frac{\alpha}{2}\right)\sqrt{\frac{\sigma^2}{N}}, \mu_0 + \phi\left(\frac{\alpha}{2}\right)\sqrt{\frac{\sigma^2}{N}}\right]\end{split}
\end{equation*}

\subsection{Exemple d’aplicació: test sobre la mitja d’una Gaussiana amb variança \sphinxstylestrong{desconeguda}}
\label{\detokenize{0_Intro/0_4_Tests:exemple-d-aplicacio-test-sobre-la-mitja-d-una-gaussiana-amb-varianca-desconeguda}}
En aquesta curs no ho desenvoluparem, però el que hem fet es pot generalitzar
a una situació on volem testejar si la mitja d’una població Gaussiana és igual a
un valor donat \(\mu_0\) o no:
\begin{itemize}
\item {} 
\(H_0: \mu= \mu_0\) (simple)

\item {} 
\(H_1: \mu \neq \mu_0\) (composta)

\end{itemize}

\sphinxstylestrong{però la variança de la mateixa és desconeguda}. En aquest cas,
el Test de RVG, dona lloc a  l’estadístic:
\begin{equation*}
\begin{split}T = \frac{\bar{X} - \mu_0}{\sqrt{\frac{S_X^2}{N}}}\end{split}
\end{equation*}
Sota l’hipòtesi nula, \(\bar{X} \sim \mathcal{N}(\mu_0, \sigma^2)\)
i \(\frac{N-1}{\sigma^2}S_X^2 \sim \chi^2_{N-1}\) (\sphinxhref{https://atibaup.github.io/ModInfer\_2020/slides/0\_Intro/0\_2\_Intro\_stats.html\#29}{Casella \& Berger 5.3.1}),
i com vam aprendre a \sphinxhref{https://e-aules.uab.cat/2020-21/pluginfile.php/609301/mod\_assign/introattachment/0/pra\%CC\%80ctica\_2.html}{la Pràctica 2 del Tema 2},
aleshores T segueix una distribució \sphinxstyleemphasis{t de Student} amb \(N-1\) graus de llibertat. La regió crítica
es pot doncs calcular a partir de la f.d.c. inversa de la distribució \sphinxstyleemphasis{t de Student}.


\subsection{Distribució asimptòtica de la log\sphinxhyphen{}raó de versemblances sota \protect\(H_0\protect\)}
\label{\detokenize{0_Intro/0_4_Tests:distribucio-asimptotica-de-la-log-rao-de-versemblances-sota-h-0}}
Als exemples anteriors, hem construït les regions crítiques a base de manipular
la regió crítica del Test de RVG:
\begin{equation*}
\begin{split}\left\{\Lambda: \Lambda \leq \lambda_0\right\}\end{split}
\end{equation*}
fins que trobàvem un conjunt equivalent, expressant en funció d’un estadístic diferent de
\(\Lambda\), que anomenem \(T\):
\begin{equation*}
\begin{split}\left\{T: T \leq T_0\right\}\end{split}
\end{equation*}
de manera que:
\begin{equation*}
\begin{split}P(\Lambda \leq \lambda_0; H_0) = P( T \leq T_0 ; H_0) = \alpha\end{split}
\end{equation*}
Això ho feiem perquè sovint podiem caracteritzar la f.d.p de \(T\) sota la hipòtesi nula,
ja que trobar la de \(\Lambda\) semblava massa difícil.

Resulta que hi ha un resultat teòric molt important que amplia l’aplicació
del test de RVG a moltes més situacions, sempre i quant la talla de la mostra sigui suficientment gran:

\sphinxstylestrong{Teorema 4.1.}: Sota certes condicions de regularitat de les f.d.p. involucrades,
la distribució de \(-2\log\Lambda\) sota l’hipòtesis nula tendeix a una distribució
de \(\chi^2_D\) amb \(D = \mbox{dim}\Theta - \mbox{dim}\Theta_0\) quan la talla
de la mostra tendeix a l’infinit.

En les pròximes seccions veurem una aplicació pràctica d’aquest resultat en el contexte
de tests per la Bondat d’Ajust.


\section{Aplicació del TRVG: Bondat d’Ajust}
\label{\detokenize{0_Intro/0_4_Tests:aplicacio-del-trvg-bondat-d-ajust}}

\subsection{Tests de Bondat d’Ajust}
\label{\detokenize{0_Intro/0_4_Tests:tests-de-bondat-d-ajust}}
Recordeu del Tema 3 que un dels problemes que vam “esquivar” a l’ajustar
lleis de probabilitat a les dades era el de determinar si la nostra hipòtesi sobre la família
que generava les dades era adequada o no:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{ajust}.png}
\end{figure}

De les tres famílies (Gaussiana, Poisson, Gamma), quina té millor ajust?

Intuitivament, si tenim dades discretes, sembla que el millor ajust seria el que
minimitzaria les diferències entre les comptes observades (histograma) i les
comptes que “esperaria” la f.d.p:

Necessitem un criteri més objectiu per determinar què és poca o molta diferència
entre el que espera el model i el que observem.

En el material que segueix, donarem dues alternatives per quantificar la \sphinxstyleemphasis{bondat d’ajust},
és a dir: donada una mostra i un ajust d’una f.d.p a les mateixes, com n’és de probable
que les dades fóssin generades pel model estocàstic que hem ajustat.
\begin{itemize}
\item {} 
Suposem que tenim una mostra de talla N d’una v.a. discreta no\sphinxhyphen{}negativa amb f.m.p. \(X \sim p_X(x;\theta)\)

\item {} 
Construïm l’histograma de la mostra, amb M “compartiments” \(\left\{1, 2, \cdots, M\right\}\)

\item {} 
El nombre d’elements en cada compartiment, \(Y_1, \cdots, Y_M\) seguirà una distribució multinomial amb paràmetres \([p_X(0;\theta), p_X(1;\theta), \cdots, p_X(M-1;\theta)]\)

\end{itemize}

Aleshores, la versemblança de \(Y_1, \cdots, Y_M\) sota el model \(p_X(x;\theta)\) és:
\begin{equation*}
\begin{split}p_{Y_1, \cdots, Y_M}(y_1, \cdots, y_M; \theta) = \left( \frac{N!}{y_1! \cdots y_M!} \right) p_X(0;\theta)^{y_1} \cdots p_X(M;\theta)^{y_M}\end{split}
\end{equation*}
En canvi, si no tenim cap hipòtesi sobre el model generador de \(X\), la versemblança de
\(Y_1, \cdots, Y_M\) és:
\begin{equation*}
\begin{split}p_{Y_1, \cdots, Y_M}(y_1, \cdots, y_M; p_1, \cdots, p_M) = \left( \frac{N!}{y_1! \cdots y_M!} \right) p_1^{y_1} \cdots p_M^{y_M}\end{split}
\end{equation*}
on fixeu\sphinxhyphen{}vos que \(p_i, i=1,\cdots,M\) són “lliures”, és a dir \(p_i \neq p_X(i;\theta)\).

Tenim per tant dues hipòtesis:
\begin{itemize}
\item {} 
\(H_0\): \(Y_1, \cdots, Y_M \sim \mbox{Multinomial}\left(p_X(0;\theta), p_X(1;\theta), \cdots, p_X(M-1;\theta), N\right)\)

\item {} 
\(H_1\): \(Y_1, \cdots, Y_M \sim \mbox{Multinomial}\left(p_1, \cdots, p_M, N\right)\) amb \(\sum_i p_i =1\)

\end{itemize}

Es tracta d’una hipòtesis composta, i podem fer servir el test de RVG!

Recordem que el test de RVG demana el càlcul de:
\begin{equation*}
\begin{split}\Lambda = \frac{\max_{\theta} p_{Y_1, \cdots, Y_M}(y_1, \cdots, y_M; \theta)}{\max_{p_1, \cdots, p_M \in \mathcal{P}} p_{Y_1, \cdots, Y_M}(y_1, \cdots, y_M; p_1, \cdots, p_M)}\end{split}
\end{equation*}
on \(\mathcal{P} := \left\{p_1, \cdots, p_M: \sum_i p_i =1 \right\}\).

El màxim del numerador es troba per \(\theta\) igual a l’EMV de \(\theta\) donat
\(X_1, \cdots, X_M\) (l’EMV “habitual”).

El màxim del denominador es troba com l’EMV de \(p_1, \cdots, p_M\) sense cap restricció
(vist al \sphinxhref{https://atibaup.github.io/ModInfer\_2020/slides/0\_Intro/0\_3\_Estimacio.html\#11}{Tema 3, Diapo 11}):
\begin{equation*}
\begin{split}\hat{p}_i = \frac{Y_i}{N}\end{split}
\end{equation*}
Per tant el RVG es simplifica a:
\begin{equation*}
\begin{split}\Lambda &= \frac{\left( \frac{N!}{y_1! \cdots y_M!} \right) p_X(0;\hat{\theta})^{y_1} \cdots p_X(M;\hat{\theta})^{y_M}}{ \left( \frac{N!}{y_1! \cdots y_M!} \right) \hat{p}_1^{y_1} \cdots \hat{p}_M^{y_M}} \\
&= \Pi_{i=1}^M \left(\frac{p_X(i;\hat{\theta})}{p_i} \right)^{y_i}\end{split}
\end{equation*}
Com que coneixem la distribució sota \(H_0\) de \(-2\log \Lambda\), aplicarem la transformació
\(-2\log(\cdot)\) a l’expressió anterior:
\begin{equation*}
\begin{split}-2 \log\Lambda = -2  \sum_i y_i \log \left(\frac{p_X(i;\hat{\theta})}{p_i} \right)\end{split}
\end{equation*}
Definint \(N p_X(i;\hat{\theta}) = \hat{y}_i\) i tenint en compte que \(N \hat{p}_i = y_i\),
obtindrem:
\begin{equation*}
\begin{split}-2 \log\Lambda & = -2 \sum_i y_i \log \left(\frac{\hat{y}_i}{y_i} \right) \\
&= 2 \sum_i y_i \log \left(\frac{y_i}{\hat{y}_i} \right)\end{split}
\end{equation*}
Suposant que \(N\) és gran, invoquem el Teorema 4.1. amb \(\mbox{dim}\Theta = M-1\) i \(\mbox{dim}\Theta_0 = 1\),
sabem que \(-2 \log\Lambda \sim \chi^2_{M-2}\) i podem refusar \(H_0\) quan
\begin{equation*}
\begin{split}2 \sum_i y_i \log \left(\frac{y_i}{\hat{y}_i} \right) \geq \phi_{\chi^2_{M-2}}(1-\alpha)\end{split}
\end{equation*}
on \(\phi_{\chi^2_{M-2}}\) és la f.d.c. inversa d’una llei \(\chi^2_{M-2}\).


\subsection{Test \protect\(\chi^2\protect\) de Pearson}
\label{\detokenize{0_Intro/0_4_Tests:test-chi-2-de-pearson}}
L’expressió anterior:
\begin{equation*}
\begin{split}2 \sum_i y_i \log \left(\frac{y_i}{\hat{y}_i} \right)\end{split}
\end{equation*}
mesura com de gran és la discrepància entre \(\hat{y}_i\) i \(y_i\) (i per tant
entre \(p_X(i;\hat{\theta})\) i \(\hat{p}_i\)), i està
justificada per la teoria del Test de RVG.

L’estadístic de Pearson, definit com:
\begin{equation*}
\begin{split}X^2 = \sum_i \frac{\left(y_i - N p_X(i;\hat{\theta}) \right)^2}{ N p_X(i;\hat{\theta})} = \sum_i \frac{\left(y_i - \hat{y}_i\right)^2}{ \hat{y}_i}\end{split}
\end{equation*}
és un altre test habitual (i potser més comú) per assolir el mateix objectiu.

Resulta que es pot demostrar (mitjantçant una expansió de Taylor), que sota l’hipòtesis
nula, l’estadístic \(X^2\) i l’estadístic \(-2\log\Lambda\) son asimptòticament
equivalents, i per tant, podem fer servir que \(X^2 \sim \chi^2_D\) per trobar\sphinxhyphen{}ne la
regió i valor crítics.


\subsection{Calendari fi de curs:}
\label{\detokenize{0_Intro/0_4_Tests:calendari-fi-de-curs}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
El dia 8/01 haureu d’entregar els problemes que us posaré aquest Dimecres del Tema 4. Podeu fer els problemes i pràctiques en parelles i amb qui volgueu.

\item {} 
El dia 11/01 farem una última pràctica: bondat d’ajust i tests de dues mostres (2\sphinxhyphen{}sample tests).

\item {} 
El dia 13/01 farem una classe de repàs i resolució de problemes de pràctiques. Podem fer\sphinxhyphen{}ne una altra el dia 15/01 si hi ha demanda.

\item {} 
El dia 18/01 farem l’exàmen final. Entra tot el que hem vist a teoria, pràctica i problemes excepte la secció 4.2.5. \sphinxhref{https://atibaup.github.io/ModInfer\_2020/html/index.html}{dels apunts}. Però no patiu, centreu\sphinxhyphen{}vos en practicar i entendre els conceptes i us sortirà bé.

\end{enumerate}

© Arnau Tibau Puig, 2020 \sphinxhref{https://github.com/atibaup/ModInfer\_2020/blob/master/LICENSE}{LICENSE}



\renewcommand{\indexname}{Index}
\printindex
\end{document}